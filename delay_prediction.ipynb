{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e969a8a-a169-4f44-88b0-debb09817866",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Flight Delay Predictive Analysis - Phase IV Notebook\n",
    "### 261 Final Project\n",
    "#### Team 4-1: Austin Chen: auschen@berkeley.edu, Casey Hsiung: hsiungc@berkeley.edu (Phase Leader), Cinthya Rosales: crosales@berkeley.edu, Saurabh Narain: snarain@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a4584d-5ce8-4112-86a2-6f5efe296608",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"https://i.imgur.com/z7ll7aY.jpeg\", width=400 />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297aa4e2-42a4-45e2-9a6b-7735a4c155b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Credit Assignment Plan\n",
    "\n",
    "![](https://i.imgur.com/McjrqZz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec59d8a5-3795-482b-aea1-8870aa9852a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Business Case\n",
    "Flight delays are a significant challenge for airlines and airports, causing inconvenience to passengers and economic losses. Most importantly, flight delays decrease customer satisfaction and affects customer retention. To address this issue, we aim to predict flight delays in the next two hours using data that contains flight information and weather data.  \n",
    "\n",
    "Since the primary customer is the consumer, we will focus on predicting departure delays two hours ahead. A delay will be defined as a 15 minute delay. By informing passengers of delays, airlines can better manage operations and also increase customer satisfaction at the same time. The project’s success will be measured by evaulating our final model’s metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca4aa4e-d0a9-4cd1-9b7c-48c1adc4b8a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data \n",
    "The data we have used so far for EDA and modeling are the following, The Flights Data contains 6 months of flights data during the first and second quarters of 2015. It is a subset of the passenger flight's on-time performance data. It includes columns such as the flight data and distance of the flight.\n",
    "\n",
    "The Weather Data contains 6 months of weather information during the first and second quarters of 2015. It is a subset of the weather information data from 2015-2021. It includes columns such as hourly precipitation and hourly wind speed\n",
    "\n",
    "The Weather Stations Data contains weather station information from 2015-2021. It includes columns such as latitude, longitude, and distance to its neighbor stations. \n",
    "\n",
    "The airport codes refer to the IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code. \n",
    "\n",
    "The OTPW dataset is a joined dataset of both the flights and weather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65592b1-9d41-42ff-8b99-e5faa052434e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Project Abstract\n",
    "\n",
    "This project addresses the critical issue of flight delays, which impose substantial costs and disruptions on airlines and passengers alike. The project's primary objective is to predict flight delays accurately, thereby enabling proactive planning and enhancing travel experiences. Initially, the project leveraged historical flight data and long-term weather information to develop a robust machine-learning model for delay prediction.\n",
    "\n",
    "In prior phases, various machine learning models, including Decision Tree, Random Forest, and K-Nearest Neighbors (KNN), were trained using the 60m OTPW dataset. These models were compared against the baseline Logistic Regression model. To enhance model performance, new features were incorporated into each model, accompanied by meticulous hyperparameter tuning. The phase also explored diverse model families, encompassing Decision Trees, Random Forests, and KNN, with the aim of identifying the most effective one. Augmenting the models with additional features further bolstered their predictive capabilities.\n",
    "\n",
    "Following extensive experimentation and evaluation, the Random Forest classifier emerged as the top-performing model, boasting a test accuracy of 0.75, test precision of 0.72, and test recall of 0.75. This underscored its proficiency in making accurate predictions and effectively recognizing true positive cases. Building upon these achievements, the subsequent phase introduced the Multi Layer Perceptron model, which outshined all others. It achieved remarkable results with a test accuracy of 0.91, test precision of 0.93, and test recall of 0.91, thereby establishing its superiority in prediction accuracy and overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "356c8775-d80b-4b3c-bcc1-545b01627f5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Project Description\n",
    "\n",
    "This notebook contains the Phase IV framework for Team 4-1's flight delay analysis. The focus of the following sections in the fourth phase is to conduct feature engineering, build more advanced models to predict flights delays, discuss experiments and results, and select our final model.\n",
    "\n",
    "After training the models, we analyze the results of the models in terms of accuracy, precision, and recall to determine which model is most well-rounded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b23c09f-21e5-4ca7-9e88-f74c88dce24c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Name of the image](https://i.imgur.com/OVhObhg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97188c46-2e6a-44a7-aba0-4725079f7fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cluster Information\n",
    "\n",
    "The below are the details of the team 4-1 cluster used for this project:\n",
    "\n",
    "- **Cluster Size:** 1 driver node and 1 to 8 worker nodes\n",
    "- **Driver Node:** Standard_DS3_V2, 14 GB Memory, 4 CPU Cores\n",
    "- **Worker Nodes:** Standard_DS3_V2, 14 GB to 112 GB Memory (per node), 4 to 32 CPU Cores (per node)\n",
    "- **Runtime:** Databricks Runtime 13.2.x with Scala 2.12 and CPU-optimized machine learning libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398b330b-0230-48ef-b974-05fed5ff58b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23421d61-82da-47c0-b358-3694cd956518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler, OneHotEncoder, PCA, MinMaxScaler\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT, Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col,max,isnan,when,count,regexp_replace,length,udf,lag,lead,round,split,rand,udf,sum,month,dayofmonth,collect_list,log,monotonically_increasing_id,row_number\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, StringType, IntegerType, DateType, ArrayType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler as skmm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
    "\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "import mlflow.pyspark.ml\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3648306c-fcf0-40ed-bfc9-bbb01a06f647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Init Script\n",
    "blob_container = \"261container\" # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"261project\" # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope = \"261-fp-project\" # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key = \"project-key\" # saskey The name of the secret key created in your local computer using the Databricks CLI \n",
    "blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\n",
    "mount_path = \"/mnt/mids-w261\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42b030d-1f1f-40b5-ba97-9e64d5867fe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up SAS token\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b234237c-dd45-4f8f-a07c-8f7ec220d086",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Processing\n",
    "\n",
    "The flight prediction datasets were each converted into Parquet format before being written into the blob storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d035179-7f25-4b86-ab0f-c38e6aa83607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights_6m = spark.read.parquet(f\"{blob_url}/df_flights_6m\")\n",
    "df_weather_6m = spark.read.parquet(f\"{blob_url}/df_weather_6m\")\n",
    "df_stations = spark.read.parquet(f\"{blob_url}/df_stations\")\n",
    "df_airport_codes = spark.read.parquet(f\"{blob_url}/df_airport_codes\")\n",
    "\n",
    "# df_otpw = spark.read.parquet(f\"{blob_url}/OTPW_3m\")\n",
    "# OTPW_12m = spark.read.parquet(f\"{blob_url}/OTPW_12m\")\n",
    "df_otpw = spark.read.parquet(f\"{blob_url}/OTPW_60m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548fc01e-ee7f-4681-b731-a69ed0bbf26c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Sanitization\n",
    "\n",
    "In our initial data cleaning phase for the combined dataset, we faced issues with duplicate entries and missing data. To ensure the uniqueness of each observation, we identified and eliminated duplicate flight records. Furthermore, we removed any rows entirely populated by null values, as they offer no substantial value to our dataset or model. Rows containing invalid characters, specifically within ID fields, were also discarded to maintain the integrity of our data. We recognized that several columns related to weather data were represented in a non-numerical fashion. Accordingly, we converted numerical data stored as strings to float types, enhancing our ability to perform subsequent data cleansing strategies.\n",
    "\n",
    "In an attempt to refine the dataset further, we addressed the missing weather values through interpolation, selecting a three-hour offset based on initial assessments. However, this approach may warrant a more nuanced investigation in the subsequent phase of our project. Particularly for regions with temperate weather, we do not anticipate severe weather shifts within a six-hour window—three hours before and three hours after the hour of interest. Yet, for states with volatile weather patterns, such as Florida, conditions can change drastically within a single hour.\n",
    "\n",
    "To ensure that the label was properly sanitized, imputation was also performed on certain arrival and departure delay features. This is a critical step in the machine learning process, as values such as NULLs are not accepted into modeling.\n",
    "\n",
    "Moving forward, our approach to handling missing values will likely evolve to consider both seasonal and geographical variations. By tailoring our data cleaning process to the unique characteristics of each region and time of year, we can more accurately fill missing values and enhance the predictive power of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05af2a88-e5ba-4494-b420-debd249d186c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### High-Level Cleaning\n",
    "\n",
    "High-level cleaning dropped NULLs, duplicates, unecessary columns, and invalid characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96002de0-ef99-4e96-9dec-447773eb7d89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTPW Rows: 31673119\n"
     ]
    }
   ],
   "source": [
    "print(f\"OTPW Rows: {df_otpw.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76810a3-d900-46d7-a7b8-5fc4563228bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_otpw Rows: 31673116\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate flights\n",
    "df_otpw_dedupe = df_otpw.dropDuplicates(subset=[\"FL_DATE\", \"OP_UNIQUE_CARRIER\", \"TAIL_NUM\",\"ORIGIN_AIRPORT_ID\", \"OP_CARRIER_FL_NUM\", \"CRS_DEP_TIME\"])\n",
    "print(f\"df_otpw Rows: {df_otpw_dedupe.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e427ad-f43f-4235-9400-898c7a3b2286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initial eyeball list of columns for features and target variable\n",
    "# Can cut down - this helps with data cleaning section by eliminating more unecessary rows\n",
    "df_otpw_feat = df_otpw_dedupe.select('QUARTER', 'YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'OP_UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_STATE_FIPS', 'DEST_AIRPORT_ID', 'DEST_CITY_MARKET_ID', 'DEST_STATE_FIPS', 'CRS_DEP_TIME', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_GROUP','ARR_DEL15', 'ARR_TIME_BLK', 'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'DISTANCE_GROUP', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'sched_depart_date_time', 'two_hours_prior_depart_UTC', 'dest_airport_lon', 'dest_airport_lat', 'STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'HourlyPrecipitation', 'HourlyPresentWeatherType', 'HourlySkyConditions', 'HourlyVisibility', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyStationPressure', 'HourlyWindDirection', 'HourlyWindGustSpeed', 'HourlyWindSpeed','DEP_TIME', 'DEP_DELAY', 'DEP_DEL15')\n",
    "\n",
    "# List of column names\n",
    "col_list = [col for col in df_otpw_feat.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7014ec4-05b3-4cff-af3f-4df64fb197ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_otpw Rows: 31673116\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with only NULL values\n",
    "df_otpw_dropna = df_otpw_feat.dropna(how=\"all\")\n",
    "print(f\"df_otpw Rows: {df_otpw_dropna.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df768489-8069-48d4-87c6-2909f57f84c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned OTPW Rows: 31673116\n"
     ]
    }
   ],
   "source": [
    "# Check rows w/ invalid characters\n",
    "pattern = \"[^a-zA-Z0-9\\\\s]\"\n",
    "invalid_list = [\"OP_UNIQUE_CARRIER\", \"ORIGIN_AIRPORT_ID\", \"ORIGIN_CITY_MARKET_ID\"]\n",
    "\n",
    "# Drop rows with invalid characters (IDs) and NULL values\n",
    "OP_UNIQUE_CARRIER_df = df_otpw_dropna.withColumn(f\"cleaned_{invalid_list[0]}\", regexp_replace(f\"{invalid_list[0]}\", f\"{pattern}\", \"\"))\n",
    "OTPW_stage = OP_UNIQUE_CARRIER_df.filter(length(f\"cleaned_{invalid_list[0]}\") == length(f\"{invalid_list[0]}\"))\n",
    "\n",
    "ORIGIN_AIRPORT_ID_df = OTPW_stage.withColumn(f\"cleaned_{invalid_list[1]}\", regexp_replace(f\"{invalid_list[1]}\", f\"{pattern}\", \"\"))\n",
    "OTPW_stage = ORIGIN_AIRPORT_ID_df.filter(length(f\"cleaned_{invalid_list[1]}\") == length(f\"{invalid_list[1]}\"))\n",
    "\n",
    "ORIGIN_CITY_MARKET_ID_df = OTPW_stage.withColumn(f\"cleaned_{invalid_list[2]}\", regexp_replace(f\"{invalid_list[2]}\", f\"{pattern}\", \"\"))\n",
    "df_otpw = ORIGIN_CITY_MARKET_ID_df.filter(length(f\"cleaned_{invalid_list[2]}\") == length(f\"{invalid_list[2]}\"))\n",
    "\n",
    "print(f\"Cleaned OTPW Rows: {df_otpw.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12b3d6b-35ad-4e3b-a30b-f21a54f4d7e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe clean up: drop staging columns and rename, round values\n",
    "df_otpw = df_otpw.drop(\"OP_UNIQUE_CARRIER\", \"ORIGIN_AIRPORT_ID\", \"ORIGIN_CITY_MARKET_ID\")\n",
    "\n",
    "df_otpw = df_otpw.withColumnRenamed(\"cleaned_OP_UNIQUE_CARRIER\", \"OP_UNIQUE_CARRIER\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"cleaned_ORIGIN_AIRPORT_ID\", \"ORIGIN_AIRPORT_ID\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"cleaned_ORIGIN_CITY_MARKET_ID\", \"ORIGIN_CITY_MARKET_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461235c3-5461-4e0a-9343-345cd5a62534",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>QUARTER</th><th>YEAR</th><th>MONTH</th><th>DAY_OF_MONTH</th><th>DAY_OF_WEEK</th><th>FL_DATE</th><th>ORIGIN_STATE_FIPS</th><th>DEST_AIRPORT_ID</th><th>DEST_CITY_MARKET_ID</th><th>DEST_STATE_FIPS</th><th>CRS_DEP_TIME</th><th>DEP_DELAY_GROUP</th><th>DEP_TIME_BLK</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>ARR_DELAY</th><th>ARR_DELAY_GROUP</th><th>ARR_DEL15</th><th>ARR_TIME_BLK</th><th>CANCELLED</th><th>CANCELLATION_CODE</th><th>DIVERTED</th><th>CRS_ELAPSED_TIME</th><th>ACTUAL_ELAPSED_TIME</th><th>AIR_TIME</th><th>DISTANCE</th><th>DISTANCE_GROUP</th><th>CARRIER_DELAY</th><th>WEATHER_DELAY</th><th>NAS_DELAY</th><th>SECURITY_DELAY</th><th>LATE_AIRCRAFT_DELAY</th><th>sched_depart_date_time</th><th>two_hours_prior_depart_UTC</th><th>dest_airport_lon</th><th>dest_airport_lat</th><th>STATION</th><th>DATE</th><th>LATITUDE</th><th>LONGITUDE</th><th>ELEVATION</th><th>HourlyPrecipitation</th><th>HourlyPresentWeatherType</th><th>HourlySkyConditions</th><th>HourlyVisibility</th><th>HourlyPressureChange</th><th>HourlyPressureTendency</th><th>HourlyStationPressure</th><th>HourlyWindDirection</th><th>HourlyWindGustSpeed</th><th>HourlyWindSpeed</th><th>DEP_TIME</th><th>DEP_DELAY</th><th>DEP_DEL15</th><th>OP_UNIQUE_CARRIER</th><th>ORIGIN_AIRPORT_ID</th><th>ORIGIN_CITY_MARKET_ID</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>475787</td><td>0</td><td>0</td><td>500367</td><td>568976</td><td>568976</td><td>568976</td><td>0</td><td>0</td><td>31184700</td><td>0</td><td>164</td><td>566378</td><td>566378</td><td>0</td><td>0</td><td>25889894</td><td>25889894</td><td>25889894</td><td>25889894</td><td>25889894</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>31673116</td><td>28321039</td><td>20814886</td><td>3047654</td><td>335865</td><td>20814886</td><td>82467</td><td>98260</td><td>27515586</td><td>90702</td><td>31615176</td><td>470813</td><td>475787</td><td>475787</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         475787,
         0,
         0,
         500367,
         568976,
         568976,
         568976,
         0,
         0,
         31184700,
         0,
         164,
         566378,
         566378,
         0,
         0,
         25889894,
         25889894,
         25889894,
         25889894,
         25889894,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         31673116,
         28321039,
         20814886,
         3047654,
         335865,
         20814886,
         82467,
         98260,
         27515586,
         90702,
         31615176,
         470813,
         475787,
         475787,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "QUARTER",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "YEAR",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "MONTH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DAY_OF_MONTH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DAY_OF_WEEK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_STATE_FIPS",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEST_AIRPORT_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_MARKET_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEST_STATE_FIPS",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY_GROUP",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME_BLK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY_GROUP",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DEL15",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME_BLK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLATION_CODE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DIVERTED",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ELAPSED_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ACTUAL_ELAPSED_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "AIR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE_GROUP",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CARRIER_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WEATHER_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "NAS_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "SECURITY_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LATE_AIRCRAFT_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sched_depart_date_time",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "two_hours_prior_depart_UTC",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dest_airport_lon",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dest_airport_lat",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "STATION",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DATE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LATITUDE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LONGITUDE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ELEVATION",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPrecipitation",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPresentWeatherType",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlySkyConditions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyVisibility",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPressureChange",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPressureTendency",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyStationPressure",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindDirection",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindGustSpeed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindSpeed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DEL15",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "OP_UNIQUE_CARRIER",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_AIRPORT_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_MARKET_ID",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of null values in each column\n",
    "display(df_otpw.select([count(when(df_otpw[c].isNull(), c)).alias(c) for c in df_otpw.columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de54baea-8097-4efb-b9b2-e4ff59eefb84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Imputation\n",
    "\n",
    "Missing data was addressed primarily for the weather, arrival/departure delay, and DEP_DEL15 variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef08b88-1d05-4e6b-95d0-decd39bea395",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd85b7de-b45b-4abe-93f0-399fd54a8db2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert columns to float\n",
    "cols_conv = ['HourlyPressureChange', 'HourlyPressureTendency', 'HourlyWindDirection', 'HourlyWindDirection']\n",
    "\n",
    "for cols in cols_conv:\n",
    "    df_otpw = df_otpw.withColumn(cols, col(cols).cast('float'))\n",
    "\n",
    "# columns dropped for now: 'HourlyPrecipitation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37162cc0-3d1f-44da-8196-dcccc4fb8fba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_cols = ['HourlyVisibility', 'HourlyStationPressure', 'HourlyWindGustSpeed', 'HourlyWindSpeed', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyWindDirection']\n",
    "\n",
    "for col_name in weather_cols:\n",
    "    window_spec = Window.orderBy('MONTH')\n",
    "    \n",
    "    lag_sum = 0\n",
    "    lead_sum = 0\n",
    "    \n",
    "    for i in range(1, 13):\n",
    "        lag_sum = lag_sum + F.lag(F.col(col_name), offset=i).over(window_spec)\n",
    "        lead_sum = lead_sum + F.lead(F.col(col_name), offset=i).over(window_spec)\n",
    "    \n",
    "    lag_avg = lag_sum / 12\n",
    "    lead_avg = lead_sum / 12\n",
    "\n",
    "    # Define the interpolation logic for general weather columns\n",
    "    interpolated_value = (F.when((lag_avg.isNotNull()) & (lead_avg.isNotNull()), (lag_avg + lead_avg) / 2.0)\n",
    "                          .when(lag_avg.isNotNull(), lag_avg)\n",
    "                          .when(lead_avg.isNotNull(), lead_avg)\n",
    "                          .otherwise(None))\n",
    "\n",
    "    # Custom logic for HourlyPrecipitation\n",
    "    if col_name == 'HourlyPrecipitation':\n",
    "        interpolated_value = (F.when((lag_avg.isNotNull()) & (lead_avg.isNotNull()), (lag_avg + lead_avg) / 2.0)\n",
    "                              .otherwise(0.0))\n",
    "\n",
    "\n",
    "    # Apply the interpolation logic to the DataFrame\n",
    "    df_otpw = df_otpw.withColumn(col_name, F.when(F.col(col_name).isNull(), interpolated_value).otherwise(F.col(col_name)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84bf1e77-56c6-48f3-a306-0c20f16c16b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Impute missing labels and departure & arrivals delay information\n",
    "df_otpw.createOrReplaceTempView(\"df_otpw\")\n",
    "\n",
    "df_otpw_weath = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                o.*,\n",
    "                                CASE WHEN HourlyPrecipitation IS NULL THEN AVG(HourlyPrecipitation) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                    DEP_TIME_BLK,\n",
    "                                                    STATION,\n",
    "                                                    LATITUDE,\n",
    "                                                    LONGITUDE,\n",
    "                                                    ELEVATION\n",
    "                                                    ) ELSE \n",
    "                                                        HourlyPrecipitation  END AS HourlyPrecipitation_IMP,\n",
    "                                CASE WHEN HourlyVisibility IS NULL THEN AVG(HourlyVisibility) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                    DEP_TIME_BLK,\n",
    "                                                    STATION,\n",
    "                                                    LATITUDE,\n",
    "                                                    LONGITUDE,\n",
    "                                                    ELEVATION\n",
    "                                                    ) ELSE \n",
    "                                                        HourlyVisibility  END AS HourlyVisibility_IMP,\n",
    "                                CASE WHEN HourlyPresentWeatherType IS NULL THEN AVG(HourlyPresentWeatherType) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                    DEP_TIME_BLK,\n",
    "                                                    STATION,\n",
    "                                                    LATITUDE,\n",
    "                                                    LONGITUDE,\n",
    "                                                    ELEVATION\n",
    "                                                    ) ELSE \n",
    "                                                        HourlyPresentWeatherType END AS HourlyPresentWeatherType_IMP,\n",
    "                                CASE WHEN HourlySkyConditions IS NULL THEN AVG(HourlySkyConditions) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                    DEP_TIME_BLK,\n",
    "                                                    STATION,\n",
    "                                                    LATITUDE,\n",
    "                                                    LONGITUDE,\n",
    "                                                    ELEVATION\n",
    "                                                    ) ELSE \n",
    "                                                        HourlySkyConditions END AS HourlySkyConditions_IMP,\n",
    "                                CASE WHEN HourlyPressureChange IS NULL THEN AVG(HourlyPressureChange) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                    DEP_TIME_BLK,\n",
    "                                                    STATION,\n",
    "                                                    LATITUDE,\n",
    "                                                    LONGITUDE,\n",
    "                                                    ELEVATION\n",
    "                                                    ) ELSE \n",
    "                                                        HourlyPressureChange END AS HourlyPressureChange_IMP,\n",
    "                                CASE WHEN HourlyPressureTendency IS NULL THEN AVG(HourlyPressureTendency) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                DEP_TIME_BLK,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyPressureTendency END AS HourlyPressureTendency_IMP, \n",
    "                                CASE WHEN HourlyStationPressure IS NULL THEN AVG(HourlyStationPressure) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                    DEP_TIME_BLK,\n",
    "                                                    STATION,\n",
    "                                                    LATITUDE,\n",
    "                                                    LONGITUDE,\n",
    "                                                    ELEVATION\n",
    "                                                    ) ELSE \n",
    "                                                        HourlyStationPressure END AS HourlyStationPressure_IMP,\n",
    "                                CASE WHEN HourlyWindDirection IS NULL THEN AVG(HourlyWindDirection) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                    DEP_TIME_BLK,\n",
    "                                                    STATION,\n",
    "                                                    LATITUDE,\n",
    "                                                    LONGITUDE,\n",
    "                                                    ELEVATION\n",
    "                                                    ) ELSE \n",
    "                                                        HourlyWindDirection END AS HourlyWindDirection_IMP,\n",
    "                                CASE WHEN HourlyWindGustSpeed IS NULL THEN AVG(HourlyWindGustSpeed) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                    DEP_TIME_BLK,\n",
    "                                                    STATION,\n",
    "                                                    LATITUDE,\n",
    "                                                    LONGITUDE,\n",
    "                                                    ELEVATION\n",
    "                                                    ) ELSE \n",
    "                                                        HourlyWindGustSpeed END AS HourlyWindGustSpeed_IMP,\n",
    "                            CASE WHEN HourlyWindSpeed IS NULL THEN AVG(HourlyWindSpeed) OVER (\n",
    "                                PARTITION BY FL_DATE,\n",
    "                                                DEP_TIME_BLK,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyWindSpeed END AS HourlyWindSpeed_IMP \n",
    "                            FROM df_otpw o\n",
    "                          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79056238-c686-4496-a3d9-e9dd73be0ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Impute missing labels and departure & arrivals delay information\n",
    "df_otpw_weath.createOrReplaceTempView(\"df_otpw_weath\")\n",
    "\n",
    "df_otpw_weath = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                w.*,\n",
    "                                CASE WHEN HourlyPrecipitation_IMP IS NULL THEN AVG(HourlyPrecipitation_IMP) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyPrecipitation_IMP  END AS HourlyPrecipitation_FINAL,\n",
    "                                CASE WHEN HourlyVisibility_IMP IS NULL THEN AVG(HourlyVisibility_IMP) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyVisibility_IMP  END AS HourlyVisibility_FINAL,\n",
    "                                CASE WHEN HourlyPresentWeatherType_IMP IS NULL THEN AVG(HourlyPresentWeatherType_IMP) OVER (    \n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyPresentWeatherType_IMP END AS HourlyPresentWeatherType_FINAL,\n",
    "                                CASE WHEN HourlySkyConditions_IMP IS NULL THEN AVG(HourlySkyConditions_IMP) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlySkyConditions_IMP END AS HourlySkyConditions_FINAL,\n",
    "                                CASE WHEN HourlyPressureChange_IMP IS NULL THEN AVG(HourlyPressureChange_IMP) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyPressureChange_IMP END AS HourlyPressureChange_FINAL,\n",
    "                                CASE WHEN HourlyPressureTendency_IMP IS NULL THEN AVG(HourlyPressureTendency_IMP) OVER (        \n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyPressureTendency_IMP END AS HourlyPressureTendency_FINAL, \n",
    "                                CASE WHEN HourlyStationPressure_IMP IS NULL THEN AVG(HourlyStationPressure_IMP) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyStationPressure_IMP END AS HourlyStationPressure_FINAL,\n",
    "                                CASE WHEN HourlyWindDirection_IMP IS NULL THEN AVG(HourlyWindDirection_IMP) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyWindDirection_IMP END AS HourlyWindDirection_FINAL,\n",
    "                                CASE WHEN HourlyWindGustSpeed_IMP IS NULL THEN AVG(HourlyWindGustSpeed_IMP) OVER (\n",
    "                                    PARTITION BY FL_DATE,\n",
    "                                                STATION,\n",
    "                                                LATITUDE,\n",
    "                                                LONGITUDE,\n",
    "                                                ELEVATION\n",
    "                                                ) ELSE \n",
    "                                                    HourlyWindGustSpeed_IMP END AS HourlyWindGustSpeed_FINAL,\n",
    "                            CASE WHEN HourlyWindSpeed_IMP IS NULL THEN AVG(HourlyWindSpeed_IMP) OVER (\n",
    "                                PARTITION BY FL_DATE,\n",
    "                                            STATION,\n",
    "                                            LATITUDE,\n",
    "                                            LONGITUDE,\n",
    "                                            ELEVATION\n",
    "                                            ) ELSE \n",
    "                                                HourlyWindSpeed_IMP END AS HourlyWindSpeed_FINAL \n",
    "                            FROM df_otpw_weath w\n",
    "                          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5112983f-62a0-4280-b867-18c5329f3dbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe clean up: drop staging columns and rename, round values\n",
    "df_otpw_weath = df_otpw_weath.drop(\"HourlyPrecipitation\", \"HourlyVisibility\", \"HourlyStationPressure\", \"HourlyPresentWeatherType\", \"HourlySkyConditions\", \"HourlyPressureChange\", \"HourlyPressureTendency\",\n",
    "                                         \"HourlyWindDirection\", \"HourlyWindGustSpeed\", \"HourlyWindSpeed\", \n",
    "                                         \"HourlyPrecipitation_IMP\", \"HourlyVisibility_IMP\", \"HourlyVisibility_IMP\", \"HourlyStationPressure_IMP\", \"HourlyPresentWeatherType_IMP\", \"HourlySkyConditions_IMP\", \"HourlyPressureChange_IMP\", \"HourlyPressureTendency_IMP\",\n",
    "                                         \"HourlyWindDirection_IMP\", \"HourlyWindGustSpeed_IMP\", \"HourlyWindSpeed_IMP\")\n",
    "\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlyPrecipitation_FINAL\",\"HourlyPrecipitation\")\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlyVisibility_FINAL\",\"HourlyVisibility\")\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlyStationPressure_FINAL\",\"HourlyStationPressure\")\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlyPresentWeatherType_FINAL\",\"HourlyPresentWeatherType\")\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlySkyConditions_FINAL\",\"HourlySkyConditions\")\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlyPressureChange_FINAL\",\"HourlyPressureChange\")\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlyPressureTendency_FINAL\",\"HourlyPressureTendency\")\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlyWindDirection_FINAL\",\"HourlyWindDirection\")\n",
    "df_otpw_weath = df_otpw_weath.withColumnRenamed(\"HourlyWindGustSpeed_FINAL\",\"HourlyWindGustSpeed\")\n",
    "df_otpw = df_otpw_weath.withColumnRenamed(\"HourlyWindSpeed_FINAL\",\"HourlyWindSpeed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a3b2029-1bfb-46f0-a560-2cc21ceddd85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop additional weather rows with null values\n",
    "df_otpw = df_otpw.filter(df_otpw.HourlyVisibility.isNotNull())\n",
    "df_otpw = df_otpw.filter(df_otpw.HourlyPressureTendency.isNotNull())\n",
    "df_otpw = df_otpw.filter(df_otpw.HourlyStationPressure.isNotNull())\n",
    "df_otpw = df_otpw.filter(df_otpw.HourlyWindGustSpeed.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f802b609-d791-45c8-8315-f97c05971995",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Departures, Arrivals & Labels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ba8364-9e08-46f7-b152-8b1521c8a7ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184252\n"
     ]
    }
   ],
   "source": [
    "# Impute missing labels and departure & arrivals delay information\n",
    "df_otpw.createOrReplaceTempView(\"df_otpw\")\n",
    "\n",
    "df_otpw_label = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                o.*,\n",
    "                                CASE WHEN ARR_DELAY IS NULL THEN AVG(ARR_DELAY) OVER (PARTITION BY FL_DATE,\n",
    "                                                                                                    OP_UNIQUE_CARRIER,\n",
    "                                                                                                    DEST_AIRPORT_ID,\n",
    "                                                                                                    ARR_TIME_BLK) ELSE \n",
    "                                                                                                                    ARR_DELAY END AS ARR_DELAY_IMP,\n",
    "                                CASE WHEN ARR_DEL15 IS NULL THEN AVG(ARR_DEL15) OVER (PARTITION BY FL_DATE,\n",
    "                                                                                                    OP_UNIQUE_CARRIER,\n",
    "                                                                                                    DEST_AIRPORT_ID,\n",
    "                                                                                                    ARR_TIME_BLK) ELSE \n",
    "                                                                                                                    ARR_DEL15 END AS ARR_DEL15_IMP,\n",
    "                                CASE WHEN DEP_DELAY IS NULL THEN AVG(DEP_DELAY) OVER (PARTITION BY FL_DATE,\n",
    "                                                                                                    OP_UNIQUE_CARRIER,\n",
    "                                                                                                    ORIGIN_AIRPORT_ID,\n",
    "                                                                                                    DEP_TIME_BLK) ELSE \n",
    "                                                                                                                    DEP_DELAY END AS DEP_DELAY_IMP,\n",
    "                                CASE WHEN DEP_DEL15 IS NULL THEN AVG(DEP_DEL15) OVER (PARTITION BY FL_DATE,\n",
    "                                                                                                    OP_UNIQUE_CARRIER,\n",
    "                                                                                                    ORIGIN_AIRPORT_ID,\n",
    "                                                                                                    DEP_TIME_BLK) ELSE \n",
    "                                                                                                                    DEP_DEL15 END AS DEP_DEL15_IMP\n",
    "                            FROM df_otpw o\n",
    "                          \"\"\")\n",
    "\n",
    "print(df_otpw_label.filter(df_otpw_label.DEP_DEL15_IMP.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a1e10e-48d0-417a-abf0-bec48ae62de5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_label.createOrReplaceTempView(\"df_otpw_label\")\n",
    "\n",
    "df_otpw_label = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                l.*,\n",
    "                                CASE WHEN ARR_DELAY_IMP IS NULL THEN AVG(ARR_DELAY) OVER (PARTITION BY FL_DATE,\n",
    "                                                                                                        OP_UNIQUE_CARRIER,\n",
    "                                                                                                        DEST_AIRPORT_ID,\n",
    "                                                                                                        ARR_TIME_BLK) ELSE \n",
    "                                                                                                                ARR_DELAY_IMP END AS ARR_DELAY_FINAL, \n",
    "                                CASE WHEN ARR_DEL15_IMP IS NULL THEN AVG(ARR_DEL15) OVER (PARTITION BY FL_DATE,\n",
    "                                                                                                        OP_UNIQUE_CARRIER,\n",
    "                                                                                                        DEST_AIRPORT_ID,\n",
    "                                                                                                        ARR_TIME_BLK) ELSE \n",
    "                                                                                                                ARR_DEL15_IMP END AS ARR_DEL15_FINAL, \n",
    "                                CASE WHEN DEP_DELAY_IMP IS NULL THEN AVG(DEP_DELAY) OVER (PARTITION BY DAY_OF_WEEK,\n",
    "                                                                                                        OP_UNIQUE_CARRIER,\n",
    "                                                                                                        ORIGIN_AIRPORT_ID,\n",
    "                                                                                                        DEP_TIME_BLK) ELSE \n",
    "                                                                                                                DEP_DELAY_IMP END AS DEP_DELAY_FINAL,\n",
    "                                CASE WHEN DEP_DEL15_IMP IS NULL THEN AVG(DEP_DEL15) OVER (PARTITION BY DAY_OF_WEEK,\n",
    "                                                                                                        OP_UNIQUE_CARRIER,\n",
    "                                                                                                        ORIGIN_AIRPORT_ID,\n",
    "                                                                                                        DEP_TIME_BLK) ELSE \n",
    "                                                                                                                DEP_DEL15_IMP END AS DEP_DEL15_FINAL\n",
    "                            FROM df_otpw_label l\n",
    "                          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a4b423-0af3-4f69-948c-88cf5b6b4d9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining null values: 225\n"
     ]
    }
   ],
   "source": [
    "# Check and eliminate last remaining null values\n",
    "print(f\"Remaining null values: {df_otpw_label.filter(df_otpw_label.DEP_DEL15_FINAL.isNull()).count()}\")\n",
    "df_otpw_label = df_otpw_label.filter(df_otpw_label.DEP_DEL15_FINAL.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01218e8f-47e1-4eb1-85ad-35c5421c5b67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe clean up: drop staging columns and rename, round values\n",
    "df_otpw_label = df_otpw_label.drop(\"ARR_DELAY\", \"ARR_DEL15\", \"ARR_DELAY_IMP\", \"ARR_DEL15_IMP\", \n",
    "                                         \"DEP_DELAY\", \"DEP_DEL15\", \"DEP_DELAY_IMP\", \"DEP_DEL15_IMP\")\n",
    "\n",
    "df_otpw_label = df_otpw_label.withColumnRenamed(\"ARR_DEL15_FINAL\",\"ARR_DEL15\")\n",
    "df_otpw_label = df_otpw_label.withColumnRenamed(\"ARR_DELAY_FINAL\",\"ARR_DELAY\")\n",
    "df_otpw_label = df_otpw_label.withColumnRenamed(\"DEP_DEL15_FINAL\",\"DEP_DEL15\")\n",
    "df_otpw_label = df_otpw_label.withColumnRenamed(\"DEP_DELAY_FINAL\",\"DEP_DELAY\")\n",
    "\n",
    "df_otpw_label = df_otpw_label.withColumn(\"ARR_DEL15\", round(df_otpw_label.ARR_DEL15,0))\n",
    "df_otpw_label = df_otpw_label.withColumn(\"ARR_DELAY\", round(df_otpw_label.ARR_DELAY,0))\n",
    "df_otpw_label = df_otpw_label.withColumn(\"DEP_DEL15\", round(df_otpw_label.DEP_DEL15,0))\n",
    "df_otpw_label = df_otpw_label.withColumn(\"DEP_DELAY\", round(df_otpw_label.DEP_DELAY,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c852996-f174-484b-8970-287a50ccbc6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Dataset Checkpoint (Data Sanitization/Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2dc9d5-28eb-4208-9854-dc51c19e1c35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write data frame into blob storage\n",
    "df_otpw_label.write.mode(\"overwrite\").parquet(f\"{blob_url}/OTPW_60m_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76995080-e735-45e3-8c11-6acc3e636131",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load checkpointed dataframe\n",
    "df_otpw = spark.read.parquet(f\"{blob_url}/OTPW_60m_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee78bb80-86a8-40a7-8e06-d710dec521bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Target Variable Creation\n",
    "\n",
    "The purpose of the flight delay prediction was to predict whether a flight will be delayed two hours ahead of time. The OTPW dataset contained labels reflecting delay information at the existing timeframe with no futurization.\n",
    "\n",
    "To properly capture the objective, a new target variable `TWO_HR_DELAY` was created, which was a binary variable based on the probability of a flight being delayed in two hours. The probability was calculated using a window function average of the `DEP_DEL15` labels, partitioned by date, carrier, airport, and 2-hour future time block. Future time blocks took the current time block with two hours added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e5ba0a-6ff3-4301-b839-f7f7287ae4df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Breakdown timeblocks to have more granularity\n",
    "df_otpw.createOrReplaceTempView(\"df_otpw\")\n",
    "\n",
    "df_otpw_blk_det = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                a.*, \n",
    "                                (\n",
    "                                    CASE WHEN DEP_TIME_BLK == \"0001-0559\" AND DEP_TIME BETWEEN 1 AND 59 THEN \"0000-0059\" \n",
    "                                        WHEN DEP_TIME_BLK == \"0001-0559\" AND DEP_TIME BETWEEN 100 AND 159 THEN \"0100-0159\"\n",
    "                                        WHEN DEP_TIME_BLK == \"0001-0559\" AND DEP_TIME BETWEEN 200 AND 259 THEN \"0200-0259\"\n",
    "                                        WHEN DEP_TIME_BLK == \"0001-0559\" AND DEP_TIME BETWEEN 300 AND 359 THEN \"0300-0359\"\n",
    "                                        WHEN DEP_TIME_BLK == \"0001-0559\" AND DEP_TIME BETWEEN 300 AND 359 THEN \"0400-0459\"\n",
    "                                        WHEN DEP_TIME_BLK == \"0001-0559\" AND DEP_TIME BETWEEN 300 AND 359 THEN \"0500-0559\"\n",
    "                                        ELSE DEP_TIME_BLK\n",
    "                                    END\n",
    "                                ) AS DEP_TIME_BLK_DET\n",
    "                            FROM df_otpw a\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbb8dba-575d-4bef-b4ba-11c03b3c0432",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert timeblocks to blocks at the top of each hour\n",
    "df_otpw_blk_det.createOrReplaceTempView(\"df_otpw_blk_det\")\n",
    "\n",
    "df_otpw_delay = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                b.*, \n",
    "                                (\n",
    "                                    CASE WHEN DEP_TIME_BLK_DET == \"0000-0059\" THEN 0 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0100-0159\" THEN 100 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0200-0259\" THEN 200\n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0300-0359\" THEN 300 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0400-0459\" THEN 400 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0500-0559\" THEN 500 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0600-0659\" THEN 600 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0700-0759\" THEN 700 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0800-0859\" THEN 800 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"0900-0959\" THEN 900 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1000-1059\" THEN 1000 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1100-1159\" THEN 1100 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1200-1259\" THEN 1200 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1300-1359\" THEN 1300 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1400-1459\" THEN 1400 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1500-1559\" THEN 1500 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1600-1659\" THEN 1600 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1700-1759\" THEN 1700 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1800-1859\" THEN 1800 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"1900-1959\" THEN 1900 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"2000-2059\" THEN 2000 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"2100-2159\" THEN 2100 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"2200-2259\" THEN 2200 \n",
    "                                        WHEN DEP_TIME_BLK_DET == \"2300-2359\" THEN 2300 \n",
    "                                    END\n",
    "                                ) AS HOUR_BLK\n",
    "                            FROM df_otpw_blk_det b\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d234297c-7ce5-4b06-bf6a-f3b6f1d71fd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Take hour blocks and add two hours for futurization\n",
    "df_otpw_delay.createOrReplaceTempView(\"df_otpw_delay\")\n",
    "\n",
    "df_otpw_blk = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                d.*,\n",
    "                                (\n",
    "                                    CASE WHEN HOUR_BLK = 2200 THEN 0\n",
    "                                        WHEN HOUR_BLK = 2300 THEN 100 \n",
    "                                        ELSE HOUR_BLK + 200 \n",
    "                                    END\n",
    "                                ) AS HOUR_BLK_2\n",
    "                            FROM df_otpw_delay d\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8868a00-e119-4edb-b6a6-5752b0196eef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate delay probability using window function\n",
    "df_otpw_blk.createOrReplaceTempView(\"df_otpw_blk\")\n",
    "\n",
    "df_otpw_prob = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                l.*,\n",
    "                                AVG(DEP_DEL15) OVER (PARTITION BY FL_DATE, \n",
    "                                                                  OP_UNIQUE_CARRIER,\n",
    "                                                                  DEST_AIRPORT_ID,\n",
    "                                                                  HOUR_BLK_2) AS TWO_HR_PROB\n",
    "                            FROM df_otpw_blk l\n",
    "                          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e4fcab-ae14-4106-95be-5025e2cc5270",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create 2-hour delay new target variable based on threshold\n",
    "df_otpw_prob.createOrReplaceTempView(\"df_otpw_prob\")\n",
    "\n",
    "df_otpw = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                p.*,\n",
    "                                (\n",
    "                                    CASE WHEN TWO_HR_PROB >= 0.7 THEN 1\n",
    "                                        ELSE 0\n",
    "                                    END\n",
    "                                ) AS TWO_HR_DELAY\n",
    "                            FROM df_otpw_prob p\n",
    "                          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fa6197-3aa6-4b9e-883b-f9e661e782df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Dataset Checkpoint (New Label Creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555d03c3-3457-4273-9f8e-220cef8aca01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write data frame into blob storage\n",
    "df_otpw.write.mode(\"overwrite\").parquet(f\"{blob_url}/OTPW_60m_new_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458d5a51-2cd0-4009-b73f-d5de700af7d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load checkpointed dataframe\n",
    "df_otpw = spark.read.parquet(f\"{blob_url}/OTPW_60m_new_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44811d82-16e1-40bc-a689-f4b515eaa5a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "In phase IV, we did not conduct expansive EDA, since it has been included in the previous phases. In the previous phases we made exploratory visualizations using the flights and weather datasets. The visualizations helped us determine which features we should be including in the model and variables that are correlated with the target variable. The necessary EDA in the previous phases helped us in the model building process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "839e0ec9-a437-44e0-9b19-ea9a1f0d8ff3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**(The below charts and tables were not displayed to reduce the notebook download size)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9856e07-b090-46f6-b76a-279bc6f54f6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Flights Data 6m\n",
    "\n",
    "The Flights Data contains 6 months of flights data during the firsta and second quarters of 2015. It is a subset of the passenger flight's on-time performance data. The data dictionary can be found here https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ . \n",
    "\n",
    "There are 5779024 rows, 60 numerical columns, and 49 categorical columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c808042d-122c-40e0-83b9-6ed8e86ad39c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights_6m_pd = df_flights_6m.select(['FL_DATE', 'DEP_DEL15']).toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6097c1f-904f-43a1-ab0f-ab975d744c9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flights_6m_pd.groupby(by=\"FL_DATE\")['DEP_DEL15'].sum().plot()\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Delayed Frequency\")\n",
    "plt.title(\"Delayed Frequency by Data\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24cb2bb8-53f6-4eee-9705-4ec400594a19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The time series plot above contains the number of delayed flights for a given day. It can be observed that the lowest delayed flights for a given day occurs in the end of January, and the highest number of delayed flights occurs in the beginning of January. This makes sense, since many people travel after New Year's Day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41afbe14-e5d3-4381-96b3-ae0e31d05a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_flights_6m_pd_corr = df_flights_6m.limit(1000000).toPandas() \n",
    "# correlation_matrix = df_flights_6m_pd_corr.corr()\n",
    "# correlation_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c3d561-652c-48ca-89d2-a5c46d6b0e75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since there are many columns in the dataset, we will only talk about the highly correlated features with DEP_DEL15, which indicates if the flight had a delay in departure. There seems to only be highly correlated features with columns related to delay such as DEP_DELAY. DEP_DELAY is the difference in minutes between scheduled and actual departure time, so this will cause a multicollinearity issue as the columns are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9a66c8-3495-4cda-a9e8-fa628294521e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Weather Data 6m\n",
    "\n",
    "The Weather Data contains 6 months of weather information during the first and second quarters of 2015. It is a subset of the weather information data from 2015-2021. The data dictionary can be found here https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf on pages 8-12. \n",
    "\n",
    "There are 61204666 rows, 1 numerical column, and 123 categorical columns in the dataset. It appears that we will need to convert some columns to float type since some numerical columns are string types. We will do the cleaning in the OTPW dataset since that is where we will build our model. We will also build visualizations and correlation matrices with that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff837ae-212c-4ab2-94e6-a55c420124d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61204666"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather_6m.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8e5211-600c-4552-9e6d-5d0f22ecc10a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_year = df_weather_6m.groupBy(\"HourlyDewPointTemperature\").count()\n",
    "df_year_pd = df_year.toPandas()\n",
    "plt.figure(figsize=(35, 6))\n",
    "sns.barplot(x=\"HourlyDewPointTemperature\", y=\"count\", data = df_year_pd )\n",
    "plt.xticks(rotation = 90, fontsize = 5)\n",
    "plt.xlabel(\"HourlyDewPointTemperature\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of HourlyDewPointTemperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebfa0d4-70bc-4762-8995-2b7ec47b8bff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The most common hourly dewpoint is 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b863a6-7a49-4faa-8bcf-fc9bb8cbd14b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Stations Data\n",
    "\n",
    "The Weather Stations Data contains weather station information from 2015-2021. \n",
    "\n",
    "**Data Dictionary:**\n",
    "\n",
    "- **lat:** Latitude (Num)\n",
    "- **lon:** Longitude (Num)\n",
    "- **neighbor_lat:** neighbor latitude (Num)\n",
    "- **neighbor_lon:** neighbor longitude (Num)\n",
    "- **distance_to_neighbor:** Distance away from neighbor (Num)\n",
    "- **usaf:** United States Airforce (Str)\n",
    "- **wban:** Weather Bureau Army Navy (Str)\n",
    "- **station_id:** Station ID (Str)\n",
    "- **neighbor_id:** Neighbor ID (Str)\n",
    "- **neighbor_name:** Neighbor Name (Str)\n",
    "- **neighbor_state:** Neighbor State (Str)\n",
    "- **neighbor_call:** Neighbor Call (Str)\n",
    "\n",
    "There are 5004169 rows, 5 numerical columns, and 7 categorical columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39df913-994d-494f-8c61-ffd800903bcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5004169"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53b36c3-071f-4f5b-92c9-e0cef68d211e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stations_df = df_stations.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.histplot(data=stations_df, x=\"lon\", y=\"lat\", bins=100, cmap=\"Blues\")\n",
    "plt.title(\"Density Map of Stations\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdef86ce-e1eb-466f-aada-c242d8c527d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It looks like weather stations are located mostly located close to each other, and it seems that there are two regions where there are many weather stations,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4cb8d6-486c-4582-a2c6-cd07912fd13b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### OTPW 3m Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadb0e91-ae9c-483b-88af-24e51388516c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state_counts_df = df_otpw.groupBy(\"ORIGIN_STATE_NM\").count().orderBy(\"ORIGIN_STATE_NM\")\n",
    "state_counts_df = state_counts_df.withColumn(\"ORIGIN_STATE_NM\", col(\"ORIGIN_STATE_NM\").cast(\"string\"))\n",
    "state_counts_pandas = state_counts_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa08cc5-8421-47ed-8dd3-a090398f4083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"ORIGIN_STATE_NM\", y=\"count\", data=state_counts_pandas)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"States\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Origin States\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936b0acd-f77e-4047-be5c-2b6562a63e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state_counts_df = df_otpw.groupBy(\"DEST_STATE_NM\").count().orderBy(\"DEST_STATE_NM\")\n",
    "state_counts_df = state_counts_df.withColumn(\"DEST_STATE_NM\", col(\"DEST_STATE_NM\").cast(\"string\"))\n",
    "state_counts_pandas = state_counts_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa2f3bd-a973-4d72-9704-87df031e6a98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"DEST_STATE_NM\", y=\"count\", data=state_counts_pandas)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"States\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Destination States\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1811ed1-face-49cf-874e-25361bcb6618",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Flight Codes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1734b3f0-a43d-4bfd-94fd-9c09f01d755a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_airport_codes.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d964c5-2858-4aea-8688-b434c28db0fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th></tr></thead><tbody><tr><td>ident</td><td>string</td></tr><tr><td>type</td><td>string</td></tr><tr><td>name</td><td>string</td></tr><tr><td>elevation_ft</td><td>int</td></tr><tr><td>continent</td><td>string</td></tr><tr><td>iso_country</td><td>string</td></tr><tr><td>iso_region</td><td>string</td></tr><tr><td>municipality</td><td>string</td></tr><tr><td>gps_code</td><td>string</td></tr><tr><td>iata_code</td><td>string</td></tr><tr><td>local_code</td><td>string</td></tr><tr><td>coordinates</td><td>string</td></tr><tr><td>longitude</td><td>double</td></tr><tr><td>latitude</td><td>double</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ident",
         "string"
        ],
        [
         "type",
         "string"
        ],
        [
         "name",
         "string"
        ],
        [
         "elevation_ft",
         "int"
        ],
        [
         "continent",
         "string"
        ],
        [
         "iso_country",
         "string"
        ],
        [
         "iso_region",
         "string"
        ],
        [
         "municipality",
         "string"
        ],
        [
         "gps_code",
         "string"
        ],
        [
         "iata_code",
         "string"
        ],
        [
         "local_code",
         "string"
        ],
        [
         "coordinates",
         "string"
        ],
        [
         "longitude",
         "double"
        ],
        [
         "latitude",
         "double"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_airport_codes.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c49cd0a-6dbe-4558-ba3c-490dac741845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_airport_codes = df_airport_codes.withColumn(\"longitude\", split(col(\"coordinates\"), \",\").getItem(0).cast(\"double\"))\n",
    "df_airport_codes = df_airport_codes.withColumn(\"latitude\", split(col(\"coordinates\"), \",\").getItem(1).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492d18d5-692b-4358-badf-b4603befd1c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_airport_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3092f42d-2782-448a-a244-96b6b2c7fd7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = df_airport_codes.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.histplot(data=pandas_df, x=\"longitude\", y=\"latitude\", bins=100, cmap=\"Blues\")\n",
    "plt.title(\"Density Map of Airports\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b6c17d-220b-4642-aeaa-53d88ea6668f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print summaries for all columns\n",
    "for i in range(0, len(col_list), 5):\n",
    "    df_otpw.select(col_list[i:i+5]).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba50c66-701a-4cbc-9861-315142afcfe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca53dd3f-007b-449e-979f-6f4e689cf80b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Name of the image](https://i.imgur.com/OVhObhg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c57b7f-d0cd-4a89-9dcf-b65ba2095bde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Analysis\n",
    "\n",
    "The feature analysis determined which features to implement in modeling and experiments.\n",
    " \n",
    "The below lists the families of the features that were being considered:\n",
    "| Family  | Features | Count Per Family |\n",
    "|----------|----------|:----------:|\n",
    "| Time | QUARTER, YEAR, MONTH, DAY_OF_MONTH, DAY_OF_WEEK, FL_DATE, sched_depart_date_time, two_hours_prior_depart_UTC | 8 |\n",
    "| Flight Info | OP_UNIQUE_CARRIER, OP_CARRIER_AIRLINE_ID, CRS_ELAPSED_TIME, ACTUAL_ELAPSED_TIME, AIR_TIME, FLIGHTS, DISTANCE, DISTANCE_GROUP, TAXI_OUT, WHEELS_OFF, WHEELS_ON, TAXI_IN | 12 |\n",
    "| Origin Location | ORIGIN_AIRPORT_ID, origin_airport_name, ORIGIN_AIRPORT_SEQ_ID, ORIGIN_CITY_MARKET_ID, ORIGIN, ORIGIN_CITY_NAME, ORIGIN_STATE_ABR, ORIGIN_STATE_FIPS, ORIGIN_STATE_NM | 9 |\n",
    "| Destination Location | DEST_AIRPORT_ID, DEST_AIRPORT_SEQ_ID, dest_airport_lon, dest_airport_lat, DEST_CITY_MARKET_ID, DEST, DEST_CITY_NAME, DEST_STATE_ABR, DEST_STATE_FIPS, DEST_STATE_NM | 10 |\n",
    "| Departure Time | CRS_DEP_TIME, DEP_TIME, DEP_DELAY, DEP_DELAY_GROUP, DEP_TIME_BLK, HOUR_BLK, TWO_HOUR_BLK | 7 |\n",
    "| Arrival Time | CRS_ARR_TIME, ARR_TIME, ARR_DELAY, ARR_DEL15, ARR_DELAY_GROUP, ARR_TIME_BLK | 6 |\n",
    "| Cancelled/Diverted | CANCELLED, CANCELLATION_CODE, DIVERTED | 3 |\n",
    "| Delay Details | CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY | 6 |\n",
    "| Station | STATION, DATE, LATITUDE, LONGITUDE, ELEVATION | 12 |\n",
    "| Hourly Weather | HourlyPrecipitation, HourlyPresentWeatherType, HourlySkyConditions, HourlyVisibility, HourlyPressureChange, HourlyPressureTendency, HourlyStationPressure, HourlyWindDirection, HourlyWindGustSpeed, HourlyWindSpeed | 10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3a6cb3-3d93-436e-a0f8-b4e09115b084",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pearson Correlation\n",
    "The below Pearson Correlation analysis was performed on continuous and other numerical features, and the assumption was made that all features were I.I.D. The results indicate that features from the same family are more likely to have higher positive and negative correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10887efd-dc03-43d3-b279-aecb7f4ce2de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform testing on certain features\n",
    "correlation_col = df_otpw.select('CRS_DEP_TIME', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'dest_airport_lon', 'dest_airport_lat', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'HourlyPrecipitation', 'HourlyVisibility', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyStationPressure', 'HourlyWindDirection', 'HourlyWindGustSpeed', 'HourlyWindSpeed', 'DEP_TIME', 'DEP_DELAY', 'TWO_HR_DELAY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c59ef56-4f5c-48b5-a542-1dd10dda7b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARR_DELAY', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'dest_airport_lon', 'dest_airport_lat', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'HourlyVisibility', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyStationPressure', 'HourlyWindDirection', 'HourlyWindGustSpeed', 'HourlyWindSpeed', 'DEP_DELAY', 'TWO_HR_DELAY']\n"
     ]
    }
   ],
   "source": [
    "correl_col = [col for col, dtype in correlation_col.dtypes if dtype == \"float\" or dtype == \"integer\" or dtype == \"double\" or col == \"TWO_HR_DELAY\"]\n",
    "print(correl_col)\n",
    "\n",
    "df_otpw_corr = df_otpw[correl_col]\n",
    "df_otpw_corr = df_otpw_corr.withColumn(\"TWO_HOUR_DELAY\", df_otpw_corr[\"TWO_HR_DELAY\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c0eef2a-5cee-4d2d-90de-f3b2dc9f9e11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**(The below correlation matrix has been commented to reduce the notebook download size)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2894c30-6c31-4efd-ab6e-0a538288d1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_corr = df_otpw_corr.toPandas() \n",
    "# correlation_matrix = df_corr.corr()\n",
    "# correlation_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048b1805-6f34-4fde-97a4-99875270956c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The following features had 1) the largest positive/negative correlations with the target variable as compared with other features belonging to the same feature family, and 2) were selected over similar features that shared high positive/negative correlation.\n",
    "\n",
    "\n",
    "- **Flight Info:** Actual_Elapsed_Time\n",
    "- **Destination Location:** dest_airport_lon & dest_airport_lat\n",
    "- **Departure Time:** DEP_DELAY\n",
    "- **Arrival Time:** ARR_DELAY\n",
    "\n",
    "Correlations between the target variable and features from the Hourly Weather and Delay Details families were extremely low. However, there are indications that these features may require further feature engineering (such as scaling, log transformations, or dimension reduction) for a more thorough analysis and determination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b67cefe-3e4b-4f81-873e-d47b50c19904",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Chi-Squared Test\n",
    "The following Chi-Squared testing was done on non-ordinal categorical features against the target variable, and the assumption that features are I.I.D. was made. With more time we would use additional tests such as ANOVA or time-based cross correlation to assess relationships. Only features with no NULL values were tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8181646-4cfc-456b-803d-6db46a11e456",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate time and continous features\n",
    "cat_col = ['OP_UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_STATE_FIPS', 'DEST_AIRPORT_ID',\n",
    "           'DEST_CITY_MARKET_ID', 'DEST_STATE_FIPS', 'DEP_TIME_BLK', 'ARR_TIME_BLK', 'CANCELLED', 'DIVERTED', 'DISTANCE_GROUP', 'HOUR_BLK']\n",
    "\n",
    "# Create indexed columns based on categorical features\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f'i_{col}', handleInvalid='keep')\n",
    "            for col in cat_col]\n",
    "\n",
    "for indexer in indexers:\n",
    "    indexer_model = indexer.fit(df_otpw)\n",
    "    df_otpw = indexer_model.transform(df_otpw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5becc7-c92c-41bf-a5fd-88210333958f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi_OP_UNIQUE_CARRIER\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|            [18]|[341853.5223240425]|\n",
      "+-------+----------------+-------------------+\n",
      "\n",
      "vi_ORIGIN_AIRPORT_ID\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|           [368]|[453312.0082218895]|\n",
      "+-------+----------------+-------------------+\n",
      "\n",
      "vi_ORIGIN_CITY_MARKET_ID\n",
      "+-------+----------------+--------------------+\n",
      "|pValues|degreesOfFreedom|          statistics|\n",
      "+-------+----------------+--------------------+\n",
      "|  [0.0]|           [341]|[410607.31130195945]|\n",
      "+-------+----------------+--------------------+\n",
      "\n",
      "vi_ORIGIN_STATE_FIPS\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|            [52]|[263208.9869696186]|\n",
      "+-------+----------------+-------------------+\n",
      "\n",
      "vi_DEST_AIRPORT_ID\n",
      "+-------+----------------+------------------+\n",
      "|pValues|degreesOfFreedom|        statistics|\n",
      "+-------+----------------+------------------+\n",
      "|  [0.0]|           [367]|[994211.958756259]|\n",
      "+-------+----------------+------------------+\n",
      "\n",
      "vi_DEST_CITY_MARKET_ID\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|           [341]|[938555.6444500914]|\n",
      "+-------+----------------+-------------------+\n",
      "\n",
      "vi_DEST_STATE_FIPS\n",
      "+-------+----------------+--------------------+\n",
      "|pValues|degreesOfFreedom|          statistics|\n",
      "+-------+----------------+--------------------+\n",
      "|  [0.0]|            [52]|[514672.63497701584]|\n",
      "+-------+----------------+--------------------+\n",
      "\n",
      "vi_DEP_TIME_BLK\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|            [18]|[726273.2281526036]|\n",
      "+-------+----------------+-------------------+\n",
      "\n",
      "vi_ARR_TIME_BLK\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|            [18]|[691049.9713518913]|\n",
      "+-------+----------------+-------------------+\n",
      "\n",
      "vi_CANCELLED\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|             [1]|[74191.84459635013]|\n",
      "+-------+----------------+-------------------+\n",
      "\n",
      "vi_DIVERTED\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|             [1]|[6891.194548673746]|\n",
      "+-------+----------------+-------------------+\n",
      "\n",
      "vi_DISTANCE_GROUP\n",
      "+-------+----------------+--------------------+\n",
      "|pValues|degreesOfFreedom|          statistics|\n",
      "+-------+----------------+--------------------+\n",
      "|  [0.0]|            [10]|[22436.975595369706]|\n",
      "+-------+----------------+--------------------+\n",
      "\n",
      "vi_HOUR_BLK\n",
      "+-------+----------------+-------------------+\n",
      "|pValues|degreesOfFreedom|         statistics|\n",
      "+-------+----------------+-------------------+\n",
      "|  [0.0]|            [22]|[779029.4978766271]|\n",
      "+-------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert indexed columns to vectors, perform Chi Squared Test\n",
    "indexed_columns = [f'i_{col}' for col in cat_col]\n",
    "\n",
    "assemblers = [(VectorAssembler(inputCols=[f'{col}'], outputCol=f'v{col}'), f'v{col}') for col in \n",
    "              indexed_columns]\n",
    "\n",
    "for assembler in assemblers:\n",
    "    df_VA = assembler[0].transform(df_otpw)\n",
    "    df_VA.createOrReplaceTempView(f'va_{assembler[1]}')\n",
    "    print(assembler[1])\n",
    "    result = ChiSquareTest.test(df_VA, f'{assembler[1]}', 'TWO_HR_DELAY')\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3a0bba-b6c4-4a8c-8a87-56f739934c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up indexed columns and replace original column\n",
    "df_otpw = df_otpw.drop('OP_UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_STATE_FIPS', \n",
    "                       'DEST_AIRPORT_ID', 'DEST_CITY_MARKET_ID','DEST_STATE_FIPS', 'DEP_TIME_BLK', 'ARR_TIME_BLK', 'CANCELLED', 'DIVERTED', 'DISTANCE_GROUP')\n",
    "\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_OP_UNIQUE_CARRIER\",\"OP_UNIQUE_CARRIER\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_ORIGIN_AIRPORT_ID\",\"ORIGIN_AIRPORT_ID\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_ORIGIN_CITY_MARKET_ID\",\"ORIGIN_CITY_MARKET_ID\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_ORIGIN_STATE_FIPS\",\"ORIGIN_STATE_FIPS\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_DEST_AIRPORT_ID\",\"DEST_AIRPORT_ID\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_DEST_CITY_MARKET_ID\",\"DEST_CITY_MARKET_ID\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_DEST_STATE_FIPS\",\"DEST_STATE_FIPS\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_DEP_TIME_BLK\",\"DEP_TIME_BLK\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_ARR_TIME_BLK\",\"ARR_TIME_BLK\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_CANCELLED\",\"CANCELLED\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_DIVERTED\",\"DIVERTED\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"i_DISTANCE_GROUP\",\"DISTANCE_GROUP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d7ee1e-bb97-4020-a5ce-ebeb1b30654a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Chi-Squared Results\n",
    "\n",
    "Based on the test results, the Chi-Squared Test suggests a significant association between each tested feature and the target variable (with p-value = 0.05). For example, the DISTANCE_GROUP feature had a Chi-Squared score of 22,436.97 and a p-value of 0.0, which suggests a significant association between the flight carrier and having a departure delay two hours ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc015e5-f90b-4902-815c-7fccc4321f39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Dataset Checkpoint (Feature Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0529c7-f85f-4eb6-b9db-7053a96f288e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data frame into blob storage\n",
    "df_otpw.write.mode(\"overwrite\").parquet(f\"{blob_url}/OTPW_60m_FA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b768d00f-c170-4661-942b-3c369f16adf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load checkpointed dataframe\n",
    "df_otpw = spark.read.parquet(f\"{blob_url}/OTPW_60m_FA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "092080fe-64bb-4cd2-b325-e0d404787500",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Concluding the feature analysis, the following features underwent feature engineering:\n",
    "\n",
    "- **Hourly Weather Family:** HourlyPrecipitation, HourlyVisibility, HourlyPressureChange, HourlyPressureTendency, HourlyStationPressure, HourlyWindDirection, HourlyWindSpeed\n",
    "\n",
    "- dest_airport_lon, dest_airport_lat\n",
    "\n",
    "- **Delay Details Family:** CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY, ARR_DELAY, DEP_DELAY\n",
    "\n",
    "- CANCELLED\n",
    "\n",
    "- DIVERTED\n",
    "\n",
    "- **Flight Info Family:** OP_UNIQUE_CARRIER, ORIGIN_AIRPORT_ID, ORIGIN_CITY_MARKET_ID, ORIGIN_STATE_FIPS, DEST_AIRPORT_ID, DEST_CITY_MARKET_ID, DEST_STATE_FIPS\n",
    "\n",
    "- FL_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50d86c8-910f-4e8a-a643-f09da38d4569",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Feature engineering is a critical step in the development of robust machine learning models. In our effort to optimize the dataset, which contains both flight information and hourly weather data, we utilized a range of techniques tailored to the unique characteristics of our data. We applied Log Transformations and Min/Max Scaling to address distribution and scale issues, and turned to Principal Component Analysis (PCA) for dimensionality reduction. Interaction Effects were incorporated to capture the combined influence of features, while Graph-Based Features tapped into structured data relationships. Additionally, One-Hot Encoding ensured we effectively processed categorical data. Each method was carefully chosen to mold our dataset into an optimized form for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7856e485-e5f8-4dfc-8f90-d65a6dc1aeb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below is a table containing the feature strategy by feature.\n",
    "\n",
    "| Feature(s)  | Strategy | Reason |\n",
    "|----------|----------|----------|\n",
    "| Origin & Destination Families | One-Hot Encoding, Graph-Based Engineering | There was reason to indicate that the Origin & Destination families could have a relationship with the target variable, which will be confirmed with further engineering. |\n",
    "| Cancelled/Diverted Family | One-Hot Encoding | There was reason to indicate that the Cancelled/Diverted family could have a relationship with the target variable, which will be confirmed with further engineering. |\n",
    "| Hourly Weather Family | Log Transformation, Min/Max Scaling | There was reason to indicate that the Hourly Weather family could have a relationship with the target variable, which will be confirmed with further engineering. |\n",
    "| Delay Details Family | PCA, Scaling | There was similarity and high correlation between these features and the target. Since the minutes vary greatly, scaling will also be applied. |\n",
    "| dest_airport_lon, dest_airport_lat | Interaction Terms |  There was similarity and high correlation between these features and the target. |\n",
    "| FL_DAT | Event-Based Feature Engineering |  A separate feature was created to take into consideration both Christmas and New Year's holidays. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2184de4e-42f0-47f8-9e4c-2e327dbc85de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Log Transformation\n",
    "\n",
    "In the course of our data preprocessing, we applied a logarithmic transformation to the weather variables within our dataset. This transformation was implemented to address the non-linearity and potential skewness present in some of the meteorological measurements. By applying the logarithm, we aimed to normalize the distribution of these features, ensuring they align better with the requirements of our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de047a35-aaa0-4f3c-b5bd-832413b55aa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weather data to transform\n",
    "columns_to_transform = ['HourlyVisibility', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyStationPressure', 'HourlyWindDirection', 'HourlyWindGustSpeed', 'HourlyWindSpeed']\n",
    "\n",
    "# Log transformation\n",
    "for col_name in columns_to_transform:\n",
    "    # Appends new columns with log transformation data\n",
    "    #df_otpw = df_otpw.withColumn(f\"{col_name}_log\", log(df_otpw[col_name] + 1))\n",
    "    # to replace the values in the original dataset with the log transformation uncomment below\n",
    "    df_otpw = df_otpw.withColumn(col_name, log(df_otpw[col_name] + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "553ef40f-1fcf-47c9-9b93-3df80454c362",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Principal Component Analysis (PCA)\n",
    "\n",
    "To reduce the number of dimensions of the Delay Details Family, PCA was used to create a separate PCA feature. Each Delay Details Family feature allocated the number of minutes the flight was delayed to the category of delay. For example, a flight that was delayed by 20 minutes due to weather and the carrier could split 15 minutes to `WEATHER_DELAY` and 5 minutes to `CARRIER_DELAY`.\n",
    "\n",
    "The six features were condensed into three principal components to eliminate noise and retain only the most important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a68caab-88f5-4414-9a57-5ced338395ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply PCA to Delay Details Family\n",
    "delay_list = [\"CARRIER_DELAY\", \"WEATHER_DELAY\", \"NAS_DELAY\", \"SECURITY_DELAY\", \"LATE_AIRCRAFT_DELAY\", \"DEP_DELAY\"]\n",
    "\n",
    "df_delay_pca = df_otpw.fillna(0, subset=delay_list)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=delay_list, outputCol=\"delay_features\", handleInvalid=\"skip\")\n",
    "scaler = StandardScaler(inputCol=\"delay_features\", outputCol=\"scaled_delay_features\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ec5c7b-33a7-44f2-aa4f-1ae3a43186ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PCA w/ 3 principal components\n",
    "k = 3\n",
    "pca = PCA(k=k, inputCol=\"scaled_delay_features\", outputCol=\"pca_delay_features\")\n",
    "\n",
    "pca_pipeline = Pipeline(stages=[assembler, scaler, pca])\n",
    "\n",
    "pipeline_model = pca_pipeline.fit(df_delay_pca)\n",
    "df_otpw = pipeline_model.transform(df_delay_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f891ae-31fa-45f0-97d8-e4afc7bd8750",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up scaled columns and replace original column\n",
    "df_otpw = df_otpw.drop('scaled_delay_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b812b068-3ceb-4e2f-b1bb-879dbae999e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Scaling\n",
    "\n",
    "When analyzing the features within the Delay Details Family, it became evident that they exhibited significant ranges and long-tail distributions. These values spanned from below zero to thousands of minutes. In order to standardize the range and distribution, each feature underwent individual scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82fa7c52-0fff-4de1-ab5b-c4a1c8dc541e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Individual Scaling of each feature in Delay Details Family\n",
    "stages = []\n",
    "\n",
    "for col in delay_list:\n",
    "    assembler = VectorAssembler(inputCols=[col], outputCol=f\"vect_{col}\", handleInvalid=\"keep\")\n",
    "    scaler = StandardScaler(inputCol=f\"vect_{col}\", outputCol=f\"scaled_{col}\")\n",
    "    \n",
    "    stages.extend([assembler, scaler])\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(df_otpw)\n",
    "\n",
    "df_scaled = model.transform(df_otpw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c17e35-6391-47a3-b220-029be260f00b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up scaled columns and replace original column\n",
    "df_otpw = df_scaled.drop('CARRIER_DELAY', 'vect_CARRIER_DELAY', 'WEATHER_DELAY', 'vect_WEATHER_DELAY', 'NAS_DELAY', 'vect_NAS_DELAY', 'SECURITY_DELAY', 'vect_SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'vect_LATE_AIRCRAFT_DELAY', 'DEP_DELAY', 'vect_DEP_DELAY')\n",
    "\n",
    "df_otpw = df_otpw.withColumnRenamed(\"scaled_CARRIER_DELAY\",\"CARRIER_DELAY\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"scaled_WEATHER_DELAY\",\"WEATHER_DELAY\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"scaled_NAS_DELAY\",\"NAS_DELAY\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"scaled_SECURITY_DELAY\",\"SECURITY_DELAY\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"scaled_LATE_AIRCRAFT_DELAY\",\"LATE_AIRCRAFT_DELAY\")\n",
    "df_otpw = df_otpw.withColumnRenamed(\"scaled_DEP_DELAY\",\"DEP_DELAY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc32064-79b0-4e5f-9032-0ac1dceb147a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Interaction Effects\n",
    "\n",
    "Since latitude and longitude were interacted with each other, we added both latitude and longitude to serve as one feature in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f9a9a2-0d69-4a9c-8b95-a57ff8a1fef7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sum Latitude and Longitude\n",
    "df_otpw = df_otpw.withColumn(\"sum_lat_lon\", col(\"dest_airport_lat\") + col(\"dest_airport_lon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02acca2-54f1-4527-9b20-b2e3081feee8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Min/Max Scaling \n",
    "\n",
    "To harmonize the scale of key weather attributes in our dataset, we utilized the Min/Max scaling technique. This approach linearly adjusts each feature to reside specifically within the [0, 1] range, ensuring that no single attribute exerts undue influence on the predictive model due to scale disparities. An integral part of our methodology involved managing null values by temporarily substituting them with a default value for smooth scaling operations. Once the scaling was completed, these substituted values were reverted to their original null status, preserving the integrity of our dataset. Post-processing also entailed the removal of auxiliary columns, maintaining a streamlined data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eced4104-01fb-487d-b6f8-36eb4f2894b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define UDF \n",
    "first_element_udf = udf(lambda v: float(v[0]), DoubleType())\n",
    "\n",
    "# List of columns to be scaled\n",
    "weather_feat_col = ['HourlyVisibility', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyStationPressure', 'HourlyWindDirection', 'HourlyWindSpeed']\n",
    "\n",
    "# Define a default value for nulls\n",
    "default_value = 0.0\n",
    "\n",
    "for col_name in weather_feat_col:\n",
    "    # Create a new column indicating whether the original value was null\n",
    "    df_otpw = df_otpw.withColumn(col_name + \"_was_null\", col(col_name).isNull())\n",
    "\n",
    "    # Fill null values with the default value\n",
    "    df_otpw = df_otpw.fillna({col_name: default_value})\n",
    "\n",
    "    # VectorAssembler Transformation\n",
    "    assembler = VectorAssembler(inputCols=[col_name], outputCol=col_name + \"_vec\")\n",
    "\n",
    "    # MinMaxScaler Transformation\n",
    "    scaler = MinMaxScaler(inputCol=col_name + \"_vec\", outputCol=col_name + \"_scaled_vec\")\n",
    "\n",
    "    # Construct pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    # Fit the pipeline model\n",
    "    pipeline_model = pipeline.fit(df_otpw)\n",
    "\n",
    "    # Use the pipeline model to transform the data\n",
    "    df_otpw = pipeline_model.transform(df_otpw)\n",
    "\n",
    "    # Where the \"_was_null\" column is true, replace the value with null; otherwise, use the scaled value\n",
    "    df_otpw = df_otpw.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name + \"_was_null\"), None).otherwise(first_element_udf(col(col_name + \"_scaled_vec\")))\n",
    "    )\n",
    "\n",
    "    # Drop auxiliary columns: original, vectorized, scaled vector, and \"_was_null\"\n",
    "    df_otpw = df_otpw.drop(col_name + \"_vec\").drop(col_name + \"_scaled_vec\").drop(col_name + \"_was_null\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e727ba-9af2-4d17-a6c0-db4f4f8c548e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display( df_otpw.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a216289e-1573-45a6-a7df-054311e81940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Graph-Based Feature\n",
    "\n",
    "Graph-based approaches involve representing data as a network of interconnected nodes and edges, where nodes represent entities (in this case, airports, flights, etc.), and edges represent relationships between those entities (such as flight routes, connections, etc.). In the case of this feature, we count the number of unique outgoing flights from an origin airport. The theory is that an airport with higher counts of these flights will be more likely to have delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007ee238-c233-44ef-8c69-3a249725ee44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_otpw.select(\"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\").limit(100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a811abba-6603-4c33-a970-5347681c17f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw.groupBy(\"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\") \\\n",
    "                               .agg(F.count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "043e83b4-8534-4780-8c40-f385f4131351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airport_counts = df_otpw.groupBy(\"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\") \\\n",
    "                               .agg(F.count(\"*\").alias(\"count\"))\n",
    "\n",
    "airport_counts = airport_counts.withColumnRenamed(\"count\", \"ORIGIN_DESTINATION_AIRPORT_COUNT\")\n",
    "\n",
    "# CHANGE HERE IF NEEDED\n",
    "df_otpw = df_otpw.join(airport_counts, \n",
    "                       df_otpw.ORIGIN_AIRPORT_ID == airport_counts.ORIGIN_AIRPORT_ID, \n",
    "                       \"left\").drop(df_otpw.ORIGIN_AIRPORT_ID, df_otpw.DEST_AIRPORT_ID)\n",
    "\n",
    "# display(df_otpw.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ddc1c2e-3845-4a32-8845-5802c5794e84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exploratory_df = df_otpw.select(\"ORIGIN_DESTINATION_AIRPORT_COUNT\", \"TWO_HR_DELAY\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7a27bb-f1ad-4689-864b-c516cd6c943e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(exploratory_df[exploratory_df[\"TWO_HR_DELAY\"] == 0][\"ORIGIN_DESTINATION_AIRPORT_COUNT\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041e7766-4ba1-45e7-90ba-d631191403b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(exploratory_df[exploratory_df[\"TWO_HR_DELAY\"] == 1][\"ORIGIN_DESTINATION_AIRPORT_COUNT\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053fadbb-4485-458c-839d-a156482e52a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ztest(exploratory_df[exploratory_df[\"TWO_HR_DELAY\"] == 0][\"ORIGIN_DESTINATION_AIRPORT_COUNT\"], exploratory_df[exploratory_df[\"TWO_HR_DELAY\"] == 1][\"ORIGIN_DESTINATION_AIRPORT_COUNT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27227cc3-54e0-4bce-bc5e-65469b590be8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### One-Hot Encoding\n",
    "\n",
    "Since location-based features and features in the Cancelled/Diverted Family were both categorial and non-ordinal in nature, one-hot encoding was employed. Values were assigned to a binary matrix. One-hot encoding preserved the information of features without assuming any ordering to the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276350cc-f64d-4864-b756-05aa50e53ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "categorical_cols = ['OP_UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_STATE_FIPS', \n",
    "                    'DEST_AIRPORT_ID', 'DEST_CITY_MARKET_ID', 'DEST_STATE_FIPS', 'CANCELLED', 'DIVERTED', 'DISTANCE_GROUP']\n",
    "\n",
    "# Create encoder for each categorical column\n",
    "encoders = [OneHotEncoder(inputCol=col, outputCol=f'onehot_{col}') for col in categorical_cols]\n",
    "\n",
    "# Apply the encoders to the DataFrame\n",
    "for encoder in encoders:\n",
    "    model = encoder.fit(df_otpw)\n",
    "    df_otpw = model.transform(df_otpw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0026160-9e58-4de0-937a-0c54c9b424b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Event-Based Feature Engineering\n",
    "Since Christmas and New Years are major holidays that produce many flights, we want to create a column that indicates whether the flight is on Christmas or New Year's to add as a feature in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e0a166-be52-400b-81f5-cb34dc854175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column 'is_christmas_or_newyear' to the DataFrame\n",
    "df_otpw = df_otpw.withColumn(\"is_christmas_or_newyear\",\n",
    "                                  when((month(col(\"FL_DATE\")) == 12) & (dayofmonth(col(\"FL_DATE\")) == 25) |\n",
    "                                       (month(col(\"FL_DATE\")) == 1) & (dayofmonth(col(\"FL_DATE\")) == 1), True)\n",
    "                                  .otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7404b3ea-9b83-490c-8210-22f3ad53b4bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>QUARTER</th><th>YEAR</th><th>MONTH</th><th>DAY_OF_MONTH</th><th>DAY_OF_WEEK</th><th>FL_DATE</th><th>CRS_DEP_TIME</th><th>DEP_DELAY_GROUP</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>ARR_DELAY_GROUP</th><th>CANCELLATION_CODE</th><th>CRS_ELAPSED_TIME</th><th>ACTUAL_ELAPSED_TIME</th><th>AIR_TIME</th><th>DISTANCE</th><th>sched_depart_date_time</th><th>two_hours_prior_depart_UTC</th><th>dest_airport_lon</th><th>dest_airport_lat</th><th>STATION</th><th>DATE</th><th>LATITUDE</th><th>LONGITUDE</th><th>ELEVATION</th><th>DEP_TIME</th><th>HourlyPrecipitation</th><th>HourlyVisibility</th><th>HourlyPresentWeatherType</th><th>HourlySkyConditions</th><th>HourlyPressureChange</th><th>HourlyPressureTendency</th><th>HourlyStationPressure</th><th>HourlyWindDirection</th><th>HourlyWindGustSpeed</th><th>HourlyWindSpeed</th><th>ARR_DELAY</th><th>ARR_DEL15</th><th>DEP_DEL15</th><th>DEP_TIME_BLK_DET</th><th>HOUR_BLK</th><th>HOUR_BLK_2</th><th>TWO_HR_PROB</th><th>TWO_HR_DELAY</th><th>i_HOUR_BLK</th><th>delay_features</th><th>pca_delay_features</th><th>CARRIER_DELAY</th><th>WEATHER_DELAY</th><th>NAS_DELAY</th><th>SECURITY_DELAY</th><th>LATE_AIRCRAFT_DELAY</th><th>DEP_DELAY</th><th>sum_lat_lon</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>471812</td><td>0</td><td>496311</td><td>564809</td><td>31138080</td><td>163</td><td>562217</td><td>562217</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>31622477</td><td>466869</td><td>28275831</td><td>0</td><td>20780501</td><td>3041246</td><td>20773284</td><td>0</td><td>0</td><td>27471354</td><td>0</td><td>31565291</td><td>202424</td><td>202424</td><td>0</td><td>0</td><td>716854</td><td>716854</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         471812,
         0,
         496311,
         564809,
         31138080,
         163,
         562217,
         562217,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         31622477,
         466869,
         28275831,
         0,
         20780501,
         3041246,
         20773284,
         0,
         0,
         27471354,
         0,
         31565291,
         202424,
         202424,
         0,
         0,
         716854,
         716854,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "QUARTER",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "YEAR",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "MONTH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DAY_OF_MONTH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DAY_OF_WEEK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY_GROUP",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY_GROUP",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLATION_CODE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ELAPSED_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ACTUAL_ELAPSED_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "AIR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sched_depart_date_time",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "two_hours_prior_depart_UTC",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dest_airport_lon",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dest_airport_lat",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "STATION",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DATE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LATITUDE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LONGITUDE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ELEVATION",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPrecipitation",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyVisibility",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPresentWeatherType",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlySkyConditions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPressureChange",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPressureTendency",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyStationPressure",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindDirection",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindGustSpeed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindSpeed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DEL15",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DEL15",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME_BLK_DET",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HOUR_BLK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HOUR_BLK_2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "TWO_HR_PROB",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "TWO_HR_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "i_HOUR_BLK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "delay_features",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "pca_delay_features",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CARRIER_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WEATHER_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "NAS_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "SECURITY_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LATE_AIRCRAFT_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sum_lat_lon",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBDb3VudCB0aGUgbnVtYmVyIG9mIG51bGwgdmFsdWVzIGluIGVhY2ggY29sdW1uCmRpc3BsYXkoZGZfb3RwdzIuc2VsZWN0KFtjb3VudCh3aGVuKGRmX290cHcyW2NdLmlzTnVsbCgpLCBjKSkuYWxpYXMoYykgZm9yIGMgaW4gZGZfb3RwdzIuY29sdW1uc10pKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "finishTime": 1691687584246,
       "globalVars": {},
       "guid": "a252aaf2-16e8-4759-8710-f8762f19d244",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "50e04600-98e1-451a-9d96-a50864d062cd",
       "origId": 4444148632306314,
       "parentHierarchy": [
        "c19559e7-8856-40e8-a376-c00fb878d8cb"
       ],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.99850641934771,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1691687576511,
       "state": "error",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1691687576511,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of null values in each column\n",
    "display(df_otpw2.select([count(when(df_otpw2[c].isNull(), c)).alias(c) for c in df_otpw2.columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3a04f7-9d06-48f2-8786-0739446103c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Dataset Checkpoint (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f77cfe-7741-4edd-8482-69df353559b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data frame into blob storage\n",
    "df_otpw.write.mode(\"overwrite\").parquet(f\"{blob_url}/OTPW_60m_feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cefb258-0ff9-4fc3-88de-d73c68d1f38c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load checkpointed dataframe\n",
    "df_otpw = spark.read.parquet(f\"{blob_url}/OTPW_60m_feat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131f7b26-b8e9-4c5b-8e6c-6c7c69bebf05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Splitting & Cross Validation\n",
    "\n",
    "For data splitting, the time-based aspect of the dataset for both regular train/test splits and cross validation splits was taken into consideration. In regular train/test splitting, data from 2019 was used as the test set. For time-based cross validation, the number of folds was based on the number of years in the dataset, with each fold having a single year's data for the test set.\n",
    "\n",
    "In the future as time permits, we would expand the dataset to include data from 2020-2022, although these years are typically considered anomalous due to the COVID-19 pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eec1b6a-e7e7-4559-8b89-fe7febabd9a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Traditional Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f49dfb-c38e-4067-9f64-737c2274982a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select features & label\n",
    "feat_col = [col for col in df_otpw.columns if col != 'TWO_HR_DELAY']\n",
    "label_col = 'TWO_HR_DELAY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9028bdf3-3092-4f83-a284-76472e49121b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split data by year for cross validation and create artificial folds\n",
    "split_df = df_otpw.select('YEAR').distinct().collect()\n",
    "split_list = [row.YEAR for row in split_df]\n",
    "\n",
    "modeling_df = df_otpw.select(*feat_col, label_col)\n",
    "\n",
    "train_data = modeling_df.filter(modeling_df.YEAR != 2019).orderBy(rand())\n",
    "test_data = modeling_df.filter(modeling_df.YEAR == 2019).orderBy(rand())\n",
    "split_data = (train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2bea96b-baa1-417d-9e21-ffd9aef4ebfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cross Validation\n",
    "\n",
    "Cross-validation was implemented with the number of folds set equal to the number of years present in the dataset. The dataset was split into equal parts based on the number of years. Each part was used as a validation set while the remaining parts were used for training. This approach helps to assess the models' performances across different years and ensures that each year's data contributes to both training and validation, leading to more robust evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc55f451-dfc6-4abc-8c28-1c05e32541bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split data by year for cross validation and create artificial folds\n",
    "cv_split_df = df_otpw.select('YEAR').distinct().collect()\n",
    "cv_split_list = [row.YEAR for row in cv_split_df]\n",
    "\n",
    "cv_modeling_df = df_otpw.select(*feat_col, label_col)\n",
    "\n",
    "fold_list = []\n",
    "for i in cv_split_list:\n",
    "    cv_train_data = cv_modeling_df.filter(cv_modeling_df.YEAR != i).orderBy(rand())\n",
    "    cv_val_data = cv_modeling_df.filter(cv_modeling_df.YEAR == i).orderBy(rand())\n",
    "    fold_list.append((cv_train_data, cv_val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56bb0e71-2479-4d7b-8949-5dd7cf411358",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Class Imbalance & Resampling\n",
    "\n",
    "The flight prediction training set was analyzed for class imbalance and corrected using resampling. Both undersampling and oversampling techniques were considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7121950-0949-4933-8b90-b39296d75588",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Undersampling\n",
    "\n",
    "We explored undersampling as a robust option to correct for class imbalance. In undersampling, the majority class is reduced to balance the distribution and decrease modeling bias. Since it was important to maintain the integrity of the data, the strategy applied a 9:1 ratio of non-delayed flights to delayed flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5957a5-2bd7-4783-b02e-3333f43c0b47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TWO_HR_DELAY</th><th>Count</th></tr></thead><tbody><tr><td>1</td><td>1757016</td></tr><tr><td>0</td><td>22487652</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1757016
        ],
        [
         0,
         22487652
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TWO_HR_DELAY",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBMb29rIGF0IGxhYmVsIGRpc3RyaWJ1dGlvbgpkaXNwbGF5KHRyYWluX2RhdGEuZ3JvdXBCeSgiREVQX0RFTDE1IikuYWdnKGNvdW50KCJERVBfREVMMTUiKS5hbGlhcygiQ291bnQiKSkp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView5c4b59f\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView5c4b59f\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView5c4b59f\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView5c4b59f) SELECT `DEP_DEL15`,SUM(`Count`) `column_86c5c7ba4` FROM q GROUP BY `DEP_DEL15`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView5c4b59f\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Before Undersampling",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "DEP_DEL15",
             "id": "column_86c5c7ba1"
            },
            "y": [
             {
              "column": "Count",
              "id": "column_86c5c7ba4",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0[.]00000",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_86c5c7ba2": {
             "name": "Count",
             "type": "column",
             "yAxis": 0
            },
            "column_86c5c7ba4": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "Count of Records"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "6cda512b-2a81-4a43-ab61-c3e479a34893",
       "height": "185",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b91bf31d-05ec-4e91-95c6-b1c0a4bf9cb8",
       "origId": 3984215836264399,
       "parentHierarchy": [
        "e78a4944-6421-4725-9cbf-a138f15e8497"
       ],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.998973414301872,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "DEP_DEL15",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "DEP_DEL15",
           "type": "column"
          },
          {
           "alias": "column_86c5c7ba4",
           "args": [
            {
             "column": "Count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at label distribution\n",
    "display(train_data.groupBy(\"TWO_HR_DELAY\").agg(count(\"TWO_HR_DELAY\").alias(\"Count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfc0dbe-f0e3-4471-8118-5104d48afc63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling Label Ratio: 12.782856000921983\n",
      "After Undersampling Label Ratio: 8.998379149561632\n"
     ]
    }
   ],
   "source": [
    "# Perform undersampling\n",
    "major_class = train_data.filter(train_data.TWO_HR_DELAY == 0)\n",
    "minor_class = train_data.filter(train_data.TWO_HR_DELAY == 1)\n",
    "\n",
    "# Check current ratio\n",
    "major_count = major_class.count()\n",
    "minor_count = minor_class.count()\n",
    "print(f\"Before Undersampling Label Ratio: {major_count / minor_count}\")\n",
    "\n",
    "# Undersample to 9:1 ratio for moderate adjustment\n",
    "fraction = 9 * (minor_count / major_count)\n",
    "\n",
    "# Perform undersampling\n",
    "sample_maj = major_class.sample(withReplacement=False, fraction=fraction)\n",
    "print(f\"After Undersampling Label Ratio: {sample_maj.count() / minor_count}\")\n",
    "\n",
    "train_data_undersample = minor_class.union(sample_maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296a732c-36dc-4dd6-9e87-108d3e0ee384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TWO_HR_DELAY</th><th>Count</th></tr></thead><tbody><tr><td>1</td><td>1761421</td></tr><tr><td>0</td><td>15850105</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1761421
        ],
        [
         0,
         15850105
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TWO_HR_DELAY",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBVbmRlcnNhbXBsaW5nIHJlc3VsdHMKZGlzcGxheSh0cmFpbl9kYXRhX3VuZGVyc2FtcGxlLmdyb3VwQnkoIkRFUF9ERUwxNSIpLmFnZyhjb3VudCgiREVQX0RFTDE1IikuYWxpYXMoIkNvdW50IikpKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewd68fa60\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewd68fa60\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewd68fa60\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd68fa60) SELECT `DEP_DEL15`,SUM(`Count`) `column_86c5c7ba6` FROM q GROUP BY `DEP_DEL15`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd68fa60\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "After Undersampling",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "DEP_DEL15",
             "id": "column_86c5c7ba5"
            },
            "y": [
             {
              "column": "Count",
              "id": "column_86c5c7ba6",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0[.]00000",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_86c5c7ba6": {
             "name": "Count",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "6ae24ca7-cc6d-45ff-b91d-e9b47ee51b73",
       "height": "146",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "2d6b4fe1-a76f-417c-9a9d-4dc50db93592",
       "origId": 3984215836264400,
       "parentHierarchy": [
        "897bc565-53c5-4300-b826-4c9d2398e445"
       ],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.999533370137215,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "DEP_DEL15",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "DEP_DEL15",
           "type": "column"
          },
          {
           "alias": "column_86c5c7ba6",
           "args": [
            {
             "column": "Count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "1213",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Undersampling results\n",
    "display(train_data_undersample.groupBy(\"TWO_HR_DELAY\").agg(count(\"TWO_HR_DELAY\").alias(\"Count\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f009171-4e1f-4081-a70c-bc2a73521ef4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Adaptive Synthetic Sampling (ADASYN)\n",
    "\n",
    "The second resampling strategy explored was the oversampling method of Adaptive Synthetic Sampling. In this approach, ADASYN takes instances in the minority class that are considered more difficult to classify (interpreted as instances close to the decision boundary), and creates synthetic samples to balance the dataset. This helps with eliminating blind spots in data and improving model performance. ADASYN was selected over Synthetic Minority Oversampling Technique (SMOTE), as SMOTE considers all instances of the minority class equally when creating synthetic samples.\n",
    "\n",
    "The ADASYN sampling method was based on selecting the most relevant features with no nulls, and turning them into vectors to create synthetic samples. ADASYN then uses the 'minority' sampling strategy to select instances from the minority class for resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c2950e-45b2-489b-bfca-92c6eb3fe646",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "adasyn_feat = ['QUARTER', 'YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'sum_lat_lon', 'OP_UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'HOUR_BLK', 'DEP_DELAY', 'DISTANCE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760eec0a-1e92-4765-b6a1-0ff3eaf39a1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for ADASYN\n",
    "assembler = VectorAssembler(inputCols=adasyn_feat, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "train_assembled = assembler.transform(train_data)\n",
    "train_oversampled = train_assembled.select(\"features\", label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c183984f-d8ae-47ac-82f2-e10badccaf88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply ADASYN to training data\n",
    "adasyn = ADASYN(sampling_strategy=\"minority\", random_state=42)\n",
    "\n",
    "x_train = train_oversampled.select(\"features\").toPandas()\n",
    "y_train = train_oversampled.select(label_col).toPandas()\n",
    "\n",
    "x_resampled, y_resampled = adasyn.fit_resample(x_train, y_train)\n",
    "\n",
    "# Convert resampled data back to dataframe\n",
    "train_data_adasyn = spark.createDataFrame(np.column_stack((x_resampled, y_resampled)), [\"features\", label_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb20527-13d4-494f-83b2-ea20a006a13a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Selected Class Imbalance Strategy\n",
    "\n",
    "Ultimately, the resampling strategy we selected was undersampling, as it was simpler to implement and did not require major/direct modifications to the data that would alter individual feature distributions. ADASYN does not handle NULL values, which means complete removal or imputation. Additionally, this strategy does not allow vectorized datatypes (i.e.; one-hot encodings).\n",
    "\n",
    "Like SMOTE, ADASYN is more complicated to parallelize and requires fitting the dataset into memory. As a result, resampling using ADASYN required a lot of time. Given more time to either hone the group of features used or create a thorough imputation strategy, we would explore ADASYN more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b07831-2b45-4592-a77e-dc52e0e65fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "train_data = train_data_undersample\n",
    "split_data = (train_data_undersample, test_data)\n",
    "\n",
    "# ADASYN\n",
    "# train_data = train_data_adasyn\n",
    "# split_data = (train_data_adasyn, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e39a5b-564b-4a59-9955-f44c9b226197",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If data needs to be checkpointed again, run this to get no nulls:\n",
    "df_otpw = df_otpw.filter(df_otpw.HourlyVisibility.isNotNull())\n",
    "train_data = train_data.filter(train_data.HourlyVisibility.isNotNull())\n",
    "test_data = test_data.filter(test_data.HourlyVisibility.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e90dcf-8b1a-4cbb-8123-a904fed5d00b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data frame into blob storage\n",
    "df_otpw.write.mode(\"overwrite\").parquet(f\"{blob_url}/OTPW_60m_under\")\n",
    "\n",
    "train_data.write.mode(\"overwrite\").parquet(f\"{blob_url}/OTPW_60m_train\")\n",
    "test_data.write.mode(\"overwrite\").parquet(f\"{blob_url}/OTPW_60m_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad35505b-7174-4999-a8fb-a5202ccb44fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Hyperopt was chosen for distributed hyperparameter tuning across all models due to its scalability, automated and efficient searching, and parallelization. Instead of manually updating hyperparameters, Hyperopt iterates through a predefined search space for each model, efficiently using the Tree-Structured Parzen Estimator (TPE) algorithm. TPE is a Bayesian optimization algorithm that traverses the search space and selects hyperparameters based on past performance, making it more sophisticated than methods such as random search.\n",
    "\n",
    "To perform hyperparameter tuning, the tuning function takes in a set of model variables and a pipeline that handles vectorized data and the model. The tuning function then utilizes the variables for the objective function and producing evaluation metrics. Hyperopt's `Trials` function then logs and identifies the best set of parameters by minimizing the objective function.\n",
    "\n",
    "By using Hyperopt, finding the best set of parameters for each model becomes more efficient. It streamlines the tuning process and contributes to improving model performance across all models. The below cells contain a hypertuning function for a normal train/validation split, and a second cell for hypertuning with time-based cross validation.\n",
    "\n",
    "**The below section sets the hypertuning function for use in the Experiments & Modeling phase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a176c74-cc5f-4b2c-8f5e-416756abee41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pipeline function\n",
    "def pipelineFunc(model, feat_col, label_col):\n",
    "    assembler = VectorAssembler(inputCols=feat_col, outputCol=\"vect_feat\", handleInvalid=\"keep\")\n",
    "    model.setFeaturesCol(\"vect_feat\")\n",
    "    model.setLabelCol(label_col)\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, model])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7cf029c-6e4f-48c7-9fad-8e15e48a1421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main tuning function w/ objective function\n",
    "def hyperTuneFunc(data, pipeline, evaluator, modelStrTitle, experimentNum, searchSpace, maxEvals=10):\n",
    "    with mlflow.start_run(run_name=f\"{modelStrTitle}_{experimentNum}\"):\n",
    "\n",
    "        def objectiveFunc(params):\n",
    "            with mlflow.start_run(nested=True):\n",
    "                pipeline.getStages()[-1].setParams(**params)\n",
    "\n",
    "                train, val = data\n",
    "                model = pipeline.fit(train)\n",
    "                pred = model.transform(val)\n",
    "                metrics = evaluator.evaluate(pred)\n",
    "\n",
    "                mlflow.log_metric(f\"AUC\", metrics)\n",
    "                mlflow.log_params(params)\n",
    "                                \n",
    "            return -metrics\n",
    "                \n",
    "        trials=Trials()\n",
    "        best_params = fmin(fn=objectiveFunc, \n",
    "                        space=searchSpace, \n",
    "                        algo=tpe.suggest, \n",
    "                        trials=trials, \n",
    "                        max_evals=maxEvals)\n",
    "\n",
    "        best_auc = -trials.best_trial[\"result\"][\"loss\"]\n",
    "        return best_params, best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6db32b-cc1b-43b9-a869-d40d32826ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Hypertuning with Time-Based Cross Validation\n",
    "\n",
    "The below function requires cross validation splits using time-based folds and the pipeline function from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45258972-0e3a-4bf8-bea2-8ca793930cc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cv_hyperTuneFunc(foldList, pipeline, evaluator, paramGrid, modelStrTitle, experimentNum, searchSpace, maxEvals=10):\n",
    "    with mlflow.start_run(run_name=f\"{modelStrTitle}_hypertuning_{experimentNum}\"):\n",
    "        cross_validator = CrosvsValidator(estimator=pipeline,\n",
    "                                        estimatorParamMaps=paramGrid,\n",
    "                                        evaluator=evaluator)\n",
    "\n",
    "        def objectiveFunc(params):\n",
    "            with mlflow.start_run(nested=True):\n",
    "                pipeline.getStages()[-1].setParams(**params)\n",
    "\n",
    "                avg_metric = []\n",
    "                for fold, (train, val) in enumerate(foldList):\n",
    "                    cvmodel = cross_validator.fit(train)\n",
    "                    cv_pred = cvmodel.transform(val)\n",
    "                    fold_metrics = evaluator.evaluate(cv_pred)\n",
    "\n",
    "                    avg_metric.append(fold_metrics)\n",
    "\n",
    "                    mlflow.log_metric(f\"{fold}_metric_auc\", fold_metrics)\n",
    "                    mlflow.log_params(params)\n",
    "\n",
    "                avg_metric = sum(avg_metric) / len(foldList)\n",
    "\n",
    "                mlflow.log_metric(\"overall_auc\", avg_metric)\n",
    "                                \n",
    "            return -avg_metric\n",
    "                \n",
    "        trials=Trials()\n",
    "        best_params = fmin(fn=objectiveFunc, \n",
    "                        space=searchSpace, \n",
    "                        algo=tpe.suggest, \n",
    "                        trials=trials, \n",
    "                        max_evals=maxEvals)\n",
    "\n",
    "        best_auc = -trials.best_trial[\"result\"][\"loss\"]\n",
    "        return best_params, best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c048f7f-dc9a-45dd-bcc8-ed8cf8e88aa0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Checkpointed Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c30654a-fa29-437d-8aec-17a538fe4efd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load checkpointed dataframe\n",
    "df_otpw = spark.read.parquet(f\"{blob_url}/OTPW_60m_under\")\n",
    "\n",
    "train_data = spark.read.parquet(f\"{blob_url}/OTPW_60m_train\")\n",
    "test_data = spark.read.parquet(f\"{blob_url}/OTPW_60m_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d453d7-4e24-4d84-9d00-d72e08de04bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Experiments & Modeling\n",
    "\n",
    "**Models**\n",
    "- **Baseline:** Logistic Regression\n",
    "- **Model 2:** Decision Tree\n",
    "- **Model 3:** Random Forest\n",
    "- **Model 4:** K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Sophisticated Models**\n",
    "- **Model 5:** XGBoost\n",
    "- **Model 6:** Multilayer Perceptron (MLP)\n",
    "- **Model 7:** Long Short-Term Memory (LSTM)\n",
    "- **Model 8:** Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204cb4f1-2b9e-46cf-8d70-e3970e51578b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the first two phases of modeling, we explored several machine learning models: logistic regression, decision trees, random forests, and K-Nearest Neighbors (KNN). \n",
    "\n",
    "Logistic regression was employed for its simplicity and interpretability, providing insights into the significance of features. Decision trees and random forests were chosen due to their ability to handle non-linear relationships and handle feature interactions effectively. Meanwhile, KNN was tested for its suitability in capturing local patterns and considering the neighbors' labels. Given the immense size of our dataset, for KNN, we opted to work with a representative subset to efficiently identify patterns and optimize model performance.\n",
    "\n",
    "To ensure robust evaluation, we employed cross-validation techniques, dividing the data into multiple folds, and measuring each model's performance on unseen data. Additionally, we used appropriate evaluation metrics such as accuracy, precision and recall\n",
    "This phase of experiments and modeling phase will allowed us to gain valuable insights into the strengths and weaknesses of different algorithms in predicting flight delays. The results will guide our selection of the most suitable model for further optimization and deployment in the final phase of the algorithm development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f67f274-30bf-485e-ae10-bd44e1db3426",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Baseline Model: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf4361a-e1f4-49b5-9edf-b492dc7cad28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Picking Features for our Baseline\n",
    "\n",
    "We wanted to include some basic feature selection in phases II and III. We took the cleaned data and created box plots comparing the various features against the two labels (DEP_DEL15). Box plots that appeared visually different were selected as features because they would serve as better diferentiators compared to other features.\n",
    "\n",
    "**(The below box plot grid was not run to reduce the notebook download size)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98212457-0880-48ec-8879-9749f58e213e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = df_otpw.toPandas()\n",
    "\n",
    "# Filter columns with data type int, float32, or float64\n",
    "numerical_columns = pandas_df.select_dtypes(include=['int', 'float32', 'float64']).columns\n",
    "\n",
    "# Create box plots for each numerical feature based on the target variable\n",
    "target_col = \"DEP_DEL15\"\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "num_plots = len(numerical_columns) - 1  # Exclude the target column from the plots\n",
    "num_cols = 3  # Number of columns in the subplot grid\n",
    "num_rows = math.ceil(num_plots / num_cols)\n",
    "\n",
    "# Create the subplot grid\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "\n",
    "# Flatten the axis array in case we have only one row in the subplot grid\n",
    "if num_rows == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "# Iterate through each numerical feature column\n",
    "for i, column in enumerate(numerical_columns):\n",
    "    if column != target_col:  # Skip the target column\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        \n",
    "        axs[row][col].set_title(f\"{column} vs. {target_col}\")\n",
    "        axs[row][col].set_xlabel(target_col)\n",
    "        axs[row][col].set_ylabel(column)\n",
    "\n",
    "        # Plot box plots for target values 0 and 1\n",
    "        boxplot_data = [pandas_df[column][pandas_df[target_col] == 0],\n",
    "                        pandas_df[column][pandas_df[target_col] != 0]]\n",
    "        boxplot_labels = [f\"{target_col}=0\", f\"{target_col}=1\"]\n",
    "        \n",
    "        axs[row][col].boxplot(boxplot_data, labels=boxplot_labels)\n",
    "\n",
    "# Remove any empty subplots\n",
    "for i in range(len(numerical_columns), num_rows * num_cols):\n",
    "    fig.delaxes(axs.flatten()[i])\n",
    "\n",
    "# Adjust layout and spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e50203-84b0-4bcf-8e52-d97307093aa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The features used for the baseline logistic model in phases II and III to predict the target variable DEP_DEL15 are DAY_OF_MONTH, DISTANCE, and sum_lat_lon.\n",
    "Below is a data dictionary for variables used. \n",
    "\n",
    "##### Data Dictionary:\n",
    "\n",
    "DAY_OF_MONTH: Day of the month\n",
    "\n",
    "DISTANCE: Distance between airports measured in miles\n",
    "\n",
    "sum_lat_lon: An interaction feature combining Latitude of departure airport and Longitude of departure airport\n",
    "\n",
    "DEP_DEL15: Departure Delay Indicator, 15 Minutes or More (1=Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3d1ae3-b460-4466-9135-340a51a44bc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_train = train_data.select(['DAY_OF_MONTH', 'DISTANCE', 'sum_lat_lon',  'DEP_DEL15'])\n",
    "df_otpw_test = test_data.select(['DAY_OF_MONTH', 'DISTANCE', 'sum_lat_lon',  'DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b3addb-e505-47ca-b232-0916e22a55ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_columns = ['DAY_OF_MONTH', 'DISTANCE', 'sum_lat_lon']\n",
    "target_label = 'DEP_DEL15'\n",
    "\n",
    "# Prepare data\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_otpw_train)\n",
    "otpw_test_transform = assembler.transform(df_otpw_test)\n",
    "\n",
    "# Split the data into training,test,validation sets\n",
    "# train_data, test_data, validation_data = otpw_model_transform.randomSplit([0.7, 0.15, 0.15], seed=100)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=target_label)\n",
    "\n",
    "# Train the model on the training data\n",
    "lr_model = lr.fit(otpw_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85c1312-987e-4b6c-b650-c5300058a131",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary = lr_model.summary\n",
    "coefficients = lr_model.coefficients\n",
    "coefficients_by_feature = [(feature_columns[i], coefficients[i]) for i in range(len(feature_columns))]\n",
    "\n",
    "print(\"Coefficients:\")\n",
    "for feature, coeff in coefficients_by_feature:\n",
    "    print(f\"{feature}: {coeff}\")\n",
    "print(\"Area under ROC:\", summary.areaUnderROC)\n",
    "print(\"Accuracy:\", summary.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655b8516-3355-4eca-93ad-af59909b5464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Make predictions on the test_data\n",
    "predictions = lr_model.transform(otpw_test_transform)\n",
    "\n",
    "# 2. Calculate accuracy\n",
    "correct_predictions = predictions.filter(predictions['prediction'] == predictions[target_label])\n",
    "accuracy = correct_predictions.count() / predictions.count()\n",
    "print(\"Accuracy on test_data: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# 3. Calculate ROC and AUC\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=target_label)\n",
    "roc_auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(\"ROC AUC on test_data: {:.2f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a909cc15-f0e0-4436-9b92-bd12b1b22a8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Logistic Regression Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f411467-198c-4635-9a8e-2c207546386b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set model\n",
    "lr = LogisticRegression(labelCol=label_col,\n",
    "                        featuresCol=\"vect_feat\",\n",
    "                        )\n",
    "\n",
    "# Set evaluator and metrics\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=label_col, \n",
    "                                          metricName=\"areaUnderROC\")\n",
    "\n",
    "# SET SEARCH SPACE FOR HYPERTUNING\n",
    "search_space = {\"regParam\": hp.uniform(\"regParam\", 0, 1),\n",
    "                \"elasticNetParam\": hp.uniform(\"elasticNetParam\", 0, 1)\n",
    "               }\n",
    "\n",
    "# Set max evaluations\n",
    "max_evals = 1\n",
    "\n",
    "# Set model type, experiment number for logging\n",
    "model_type = \"lr\"\n",
    "experiment_num = 1\n",
    "\n",
    "# Set pipeline\n",
    "pipeline = pipelineFunc(lr, feat_col, label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fa4a29-2ae9-4f26-899f-c02e0f6bf8bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params, best_auc = hyperTuneFunc(split_data, pipeline, evaluator, model_type, experiment_num, lr_search_space, max_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f38d671-0873-437a-8f73-3b8cc0a644ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model 2: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941841e7-77d1-40e6-b158-c4badd65536f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_dt = train_data.withColumn(\"DISTANCE_GROUP\", col(\"DISTANCE_GROUP\").cast(FloatType()))\n",
    "df_test_dt = test_data.withColumn(\"DISTANCE_GROUP\", col(\"DISTANCE_GROUP\").cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508356ae-4197-408e-ab3e-a8e0c800a2cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_dt_model = df_train_dt.select(['DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'HourlyWindSpeed', 'ELEVATION', 'DISTANCE_GROUP', 'DEP_DEL15'])\n",
    "df_test_dt_model = df_test_dt.select(['DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'HourlyWindSpeed', 'ELEVATION', 'DISTANCE_GROUP', 'DEP_DEL15'])\n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'HourlyWindSpeed', 'ELEVATION', 'DISTANCE_GROUP']\n",
    "target_label = 'DEP_DEL15'\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_dt_model)\n",
    "otpw_test_transform = assembler.transform(df_test_dt_model)\n",
    "\n",
    "# train_data, test_data, validation_data = otpw_model_transform.randomSplit([0.7, 0.15, 0.15], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc50c666-34c5-4732-9ca0-1766fb6055ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=target_label, maxBins=4, maxDepth=14)\n",
    "dt_model = dt_classifier.fit(otpw_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfcbcb84-a6cf-4dd7-b5df-1c30862db81b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = dt_model.transform(otpw_test_transform)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_label)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=target_label, metricName=\"accuracy\")\n",
    "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "# auc = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"areaUnderROC\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "# print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99478793-be76-40de-8090-7ae6b8dc4e79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = dt_model.transform(otpw_test_transform)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_label)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=target_label, metricName=\"accuracy\")\n",
    "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "# auc = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"areaUnderROC\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "# print(f\"AUC: {auc}\")\n",
    "\n",
    "tree_model = dt_model.toDebugString\n",
    "print(tree_model.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6196f317-6d49-4f1d-9c22-e384c34686f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Decision Tree Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5129a5a-95e0-40a4-a080-d36ee51051ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set model\n",
    "dt = DecisionTreeClassifier(featuresCol=\"vect_feat\", labelCol=label_col)\n",
    "\n",
    "# Set evaluator and metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_label, metricName=\"accuracy\")\n",
    "\n",
    "# SET SEARCH SPACE FOR HYPERTUNING\n",
    "search_space = {\n",
    "    \"maxDepth\": hp.choice(\"maxDepth\", range(1, 30)),  # You can customize the range as needed\n",
    "    \"maxBins\": hp.choice(\"maxBins\", range(1, 30))\n",
    "}\n",
    "\n",
    "# Set max evaluations\n",
    "max_evals = 1\n",
    "\n",
    "# Set model type, experiment number for logging\n",
    "model_type = \"dt\"\n",
    "experiment_num = 1\n",
    "\n",
    "# Set pipeline\n",
    "pipeline = pipelineFunc(dt, feat_col, label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd267199-b2cd-45ef-87bb-3c40e6e91416",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params, best_acc = hyperTuneFunc(split_data, pipeline, evaluator, model_type, experiment_num, search_space, max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f150fc-9eed-4021-a81e-3fe21d6a53b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c030c0-2ea5-481f-8717-41381270f644",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75978107-819e-400d-8f14-b8aeb59baef7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Random forest is an ensemble learning technique that combines multiple decision trees for prediction. Random forest works well on large datasets and when given multiple features. It understands feature importance, which makes it easy to identify which features are relevant for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da8e352-baae-4ab3-a95c-d1005f27fe4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c38e47b-2dc6-439b-bbbf-195cfb33b1d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_rf_model = train_data.select(['DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyWindSpeed', 'ELEVATION', 'DEP_DEL15'])\n",
    "df_test_rf_model = test_data.select(['DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyWindSpeed', 'ELEVATION', 'DEP_DEL15'])\n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyWindSpeed', 'ELEVATION' ]\n",
    "target_label = 'DEP_DEL15'\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_rf_model)\n",
    "otpw_test_transform = assembler.transform(df_test_rf_model)\n",
    "\n",
    "# train_data, test_data, validation_data = otpw_model_transform.randomSplit([0.7, 0.15, 0.15], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446d0b39-1c4b-4cdd-ba0e-7e16802dc3b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_trees = 50\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=target_label,  numTrees=num_trees, maxDepth = 5)\n",
    "rf_model = rf.fit(otpw_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574cd48a-9f48-4b23-aebc-d3c59233ae72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = rf_model.transform(otpw_test_transform)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_label, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c590f9-425d-4737-b894-26dc3e952de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b178b71b-9a5b-4bef-b25b-5cbe1d095e83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa67056-1b00-447e-9b6a-53735c9c7fdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_rf_model = train_data.select(['DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility', 'ELEVATION', 'DEP_DEL15'])\n",
    "df_test_rf_model = test_data.select(['DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility', 'ELEVATION', 'DEP_DEL15'])\n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility',  'ELEVATION' ]\n",
    "target_label = 'DEP_DEL15'\n",
    "\n",
    "# Prepare data\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_rf_model)\n",
    "otpw_test_transform = assembler.transform(df_test_rf_model)\n",
    "\n",
    "# Split the data into training,test,validation sets\n",
    "# train_data, test_data, validation_data = otpw_model_transform.randomSplit([0.7, 0.15, 0.15], seed=100)\n",
    "num_trees = 50\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=target_label,  numTrees=num_trees, maxDepth = 5)\n",
    "rf_model = rf.fit(otpw_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2757c7f-a00d-498b-ba58-62b0ea4605af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = rf_model.transform(otpw_test_transform)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_label, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df05084a-377b-432f-aafd-64a92189af6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b9e179-a631-4aa5-b862-f734a74c99a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02aca28c-784c-4de6-ae23-291ba4cd0d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_rf_model = train_data.select(['DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility', 'ELEVATION', 'DEP_DEL15'])\n",
    "df_test_rf_model = test_data.select(['DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility', 'ELEVATION', 'DEP_DEL15'])\n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility', 'ELEVATION' ]\n",
    "target_label = 'DEP_DEL15'\n",
    "\n",
    "# Prepare data\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_rf_model)\n",
    "otpw_test_transform = assembler.transform(df_test_rf_model)\n",
    "\n",
    "# Split the data into training,test,validation sets\n",
    "# train_data, test_data, validation_data = otpw_model_transform.randomSplit([0.7, 0.15, 0.15], seed=100)\n",
    "num_trees = 2\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=target_label,  numTrees=num_trees, maxDepth = 5)\n",
    "rf_model = rf.fit(otpw_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c42198-8618-4ec6-a069-c9a7a3cf2763",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = rf_model.transform(otpw_test_transform)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_label, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=target_label, predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=target_label, predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77bb863e-342b-4cca-8076-0417d49f25ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdfd491b-53c0-4642-b854-f33dfe3b18a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Random Forest Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08301d7e-3547-4fa6-ab8f-1785562afc2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set model\n",
    "rf = RandomForestClassifier(featuresCol=\"vect_feat\", labelCol=label_col)\n",
    "\n",
    "# Set evaluator and metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_label, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# SET SEARCH SPACE FOR HYPERTUNING\n",
    "search_space = {\n",
    "    \"maxDepth\": hp.choice(\"maxDepth\", range(1, 21)),  # You can customize the range as needed\n",
    "    \"numTrees\": hp.choice(\"numTrees\", range(10, 101, 10)),  # You can customize the range as needed\n",
    "}\n",
    "\n",
    "# Set max evaluations\n",
    "max_evals = 1\n",
    "\n",
    "# Set model type, experiment number for logging\n",
    "model_type = \"rf\"\n",
    "experiment_num = 1\n",
    "\n",
    "# Set pipeline\n",
    "pipeline = pipelineFunc(rf, feat_col, label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f433830b-b8fc-4495-a48e-9fb32a15efd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the function with the parameters to run experiment and produce best hyperparameters\n",
    "best_params, best_auc = hyperTuneFunc(split_data, pipeline, evaluator, model_type, experiment_num, search_space, max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43bace6-d5e8-4d19-a292-51cf0b3679c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392a3331-daeb-4149-a526-d7401a412f27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model 4: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860b2890-c02c-4241-9b1d-34f473e8ad35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "k-Nearest Neighbors (KNN) is often unsuitable for large datasets due to its computational and memory intensiveness, as it requires calculating and storing distances between each pair of data points, which scales quadratically with dataset size. Applying KNN on a smaller sample of the data is a more feasible approach. By applying a KNN to a small sample of our data, we hope to gain valuable insights into local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e8dc61-760f-4890-8abd-cf6dc7f72892",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d862d3-6875-45b3-940d-1bd47f84f508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pick Values\n",
    "feature_col = ['DAY_OF_MONTH', 'MONTH', 'HourlyVisibility', 'HourlyStationPressure', 'HourlyWindGustSpeed', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID', 'TWO_HR_DELAY']\n",
    "\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "  \n",
    "# Filter and select necessary columns\n",
    "train_data_df = train_data.select(*feature_col).filter(F.col('HourlyVisibility').isNotNull())\n",
    "test_data_df = test_data.select(*feature_col).filter(F.col('HourlyVisibility').isNotNull())\n",
    "\n",
    "# Shuffle the data and limit to 10,000 rows\n",
    "train_data_sample = train_data_df.orderBy(F.rand(seed=42)).limit(10000)\n",
    "test_data_sample = test_data_df.orderBy(F.rand(seed=42)).limit(10000)\n",
    "\n",
    "# Extracting features and target\n",
    "X_train = train_data_sample.drop(target_label)\n",
    "y_train = train_data_sample.select(target_label).toPandas()[target_label]\n",
    "\n",
    "X_test = test_data_sample.drop(target_label)\n",
    "y_test = test_data_sample.select(target_label).toPandas()[target_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b03fe7-e2d0-4c26-bb32-78e0f0aea47f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KNN for feature selection\n",
    "# Specify your feature columns and target label\n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH', 'HourlyVisibility', 'HourlyStationPressure', 'HourlyWindGustSpeed', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']\n",
    "\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Feature selection using KNN\n",
    "def feature_selection_knn(X_train, y_train, X_val, y_val, model, feature_columns):\n",
    "    remaining_features = list(feature_columns)\n",
    "    best_score = 0\n",
    "    \n",
    "    while len(remaining_features) > 0:\n",
    "        scores_with_feature_removed = []\n",
    "        \n",
    "        for feature in remaining_features:\n",
    "            model_clone = clone(model)\n",
    "            \n",
    "            X_train_reduced = X_train.drop(feature).toPandas()\n",
    "            X_val_reduced = X_val.drop(feature).toPandas()\n",
    "            \n",
    "            model_clone.fit(X_train_reduced, y_train.ravel())\n",
    "            y_pred_val = model_clone.predict(X_val_reduced)\n",
    "            score = accuracy_score(y_val.ravel(), y_pred_val)\n",
    "            \n",
    "            scores_with_feature_removed.append((feature, score))\n",
    "        \n",
    "        scores_with_feature_removed.sort(key=lambda x: x[1], reverse=True)\n",
    "        best_feature_removed, best_new_score = scores_with_feature_removed[0]\n",
    "        \n",
    "        if best_new_score > best_score:\n",
    "            print(\"Best Score:\", best_score)\n",
    "            best_score = best_new_score\n",
    "            remaining_features.remove(best_feature_removed)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return remaining_features\n",
    "\n",
    "# Run the feature selection\n",
    "selected_features = feature_selection_knn(X_train, y_train, X_test, y_test, knn, feature_columns)\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509a2a8d-dd58-498c-bb3e-f2ba7110466e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/databricks/python/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9022\n",
      "Test Accuracy: 0.9162\n",
      "Test Precision: 0.0780\n",
      "Test Recall: 0.0089\n",
      "Cross-validation scores: [0.89370588 0.89305882 0.89297059 0.893      0.89391176]\n",
      "Mean cross-validation score: 0.8933\n"
     ]
    }
   ],
   "source": [
    "# Define your feature columns and target label\n",
    "feature_col = ['DAY_OF_MONTH', 'HourlyVisibility', 'HourlyStationPressure', 'HourlyWindGustSpeed', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID', 'TWO_HR_DELAY']\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "  \n",
    "# Filter and select necessary columns\n",
    "train_data_df = train_data.select(*feature_col).filter(F.col('HourlyVisibility').isNotNull())\n",
    "test_data_df = test_data.select(*feature_col).filter(F.col('HourlyVisibility').isNotNull())\n",
    "\n",
    "# Shuffle the data and limit to 170,000 rows\n",
    "train_data_sample = train_data_df.orderBy(F.rand(seed=42)).limit(170000)\n",
    "test_data_sample = test_data_df.orderBy(F.rand(seed=42)).limit(170000)\n",
    "\n",
    "# Extracting features and target\n",
    "X_train = train_data_sample.drop(target_label).toPandas()\n",
    "y_train = train_data_sample.select(target_label).toPandas()[target_label]\n",
    "\n",
    "X_test = test_data_sample.drop(target_label).toPandas()\n",
    "y_test = test_data_sample.select(target_label).toPandas()[target_label]\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "accuracy_train = accuracy_score(y_train, knn.predict(X_train))\n",
    "\n",
    "# Calculate precision and recall of the model\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "\n",
    "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean cross-validation score: {scores.mean():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024a2eed-1917-4e33-832c-ee5e09a15e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Change to look at different features\n",
    "feature_1 = 'ORIGIN_AIRPORT_ID'\n",
    "feature_2 = 'HourlyVisibility'\n",
    "\n",
    "# Get indices of features\n",
    "index_1 = feature_columns.index(feature_1)\n",
    "index_2 = feature_columns.index(feature_2)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_train[:, index_1], X_train[:, index_2], c=y_train)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(feature_1)\n",
    "plt.ylabel(feature_2)\n",
    "plt.title('KNN: Distance vs Hourly Visibility')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20876240-b7d5-4310-9c74-af4a46c967c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Sophisticated Models\n",
    "\n",
    "Predicting flight delays using flight and weather data requires understanding and discovery of underlying behaviors and complex relationships. Many times these insights are too intricate to unearth with more sophisticated models. To detect these patterns and accurately make predictions, models like XGBoost, Multilayer Perceptrons (MLP), Long Short-Term Memory (LSTM), and 1D Convolutional Neural Networks (CNN) were explored to expand the search landscape.\n",
    "\n",
    "XGBoost was an enhancement to the decision tree/random forest modeling, effectively boosting to capture interactions between flight delays and other patterns within the dataset.\n",
    "\n",
    "The MLP, on the other hand, was a generalizable and versatile neural network that also learned patterns in the data while establishing both linear and nonlinear relationships.\n",
    "\n",
    "The LSTM, a type of recurrent neural network, was also used to capture features considered time-dependent, such as historical delays and seasonal trends. It is able to recognize recurrent patterns through the use of memory cells that retain information over time.\n",
    "\n",
    "The 1D CNN was implemented to focus on local pattern recognition and extracting features from the sequential data. The goal was to have the CNN identify distinctive patterns in flight delay data that may contribute to predictive accuracy.\n",
    "\n",
    "By exploring these advanced models, we enhance our predictive capabilities in constructing a comprehensive framework for forecasting future flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a56db1e-6da9-4392-9f23-a5a3c0bdd4f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model 5: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2674c056-6c7b-472b-86a1-9ab437c9372d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "XGBoost is specifically designed for classification and regression problems and designed for large datasets. The model is a good choice for the data, since we are trying to classify whether a flight is delayed or not delayed. We are also dealing with millions of rows in the data. \n",
    "\n",
    "Additionally, XGBoost utilizes decision trees as base learners. The model also provides an attribute to see which features are important for better understanding of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b90f26f4-63f0-4c19-88d6-47b4dfde02f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    }
   ],
   "source": [
    "df_train_xgb_model = train_data.select(['DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed', 'TWO_HR_DELAY'])\n",
    "df_test_xgb_model = test_data.select(['DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed', 'TWO_HR_DELAY'])\n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed']\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "\n",
    "# Prepare data\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_xgb_model)\n",
    "otpw_test_transform = assembler.transform(df_test_xgb_model)\n",
    "\n",
    "# Define XGBoost Classifier\n",
    "xgb_classifier = SparkXGBRegressor(features_col=\"features\", label_col=target_label,num_workers=2,)\n",
    "xgb_model = xgb_classifier.fit(otpw_train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535d6fc9-1ad8-4215-85c5-de970cd5c4bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9225314248327361\n",
      "Precision: 0.3172043010752688\n",
      "Recall: 0.00020608257973474728\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = xgb_model.transform(otpw_test_transform)\n",
    "predictions = predictions.select(\"prediction\", target_label)\n",
    "\n",
    "# Set the threshold for binary predictions\n",
    "threshold = 0.5\n",
    "\n",
    "# Add a new column for binary predictions\n",
    "predictions = predictions.withColumn(\"binary_prediction\", F.when(F.col(\"prediction\") > threshold, 1.0).otherwise(0.0))\n",
    "\n",
    "# Calculate true positives, false positives, true negatives, false negatives\n",
    "tp = predictions.filter((F.col(\"binary_prediction\") == 1.0) & (F.col(target_label) == 1.0)).count()\n",
    "fp = predictions.filter((F.col(\"binary_prediction\") == 1.0) & (F.col(target_label) == 0.0)).count()\n",
    "tn = predictions.filter((F.col(\"binary_prediction\") == 0.0) & (F.col(target_label) == 0.0)).count()\n",
    "fn = predictions.filter((F.col(\"binary_prediction\") == 0.0) & (F.col(target_label) == 1.0)).count()\n",
    "\n",
    "# Calculate accuracy, precision, and recall\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9e7f38-7f87-44ef-9fba-d464223ec307",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model 6: Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ca8055-61da-40f3-8c8b-b57282e133a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To introduce deep learning and neural network-based models, we began with the versatile multilayer perceptron model containing a single hidden layer as a base comparison.\n",
    "\n",
    "MLPs are known for their flexibility and adaptability to a wide variety of datasets by learning which features are important. They scale well with large datasets, and can identify non-linear patterns and complexities amongst different features. MLPs are also able to generalize well over unseen data, hence having robust prediction abilities. Flight delay data changes over time (i.e.; through scheduling, demand, seasons, etc.), and MLPs can adapt accordingly to its dynamic nature.\n",
    "\n",
    "From here we continue our exploration into deep learning with more advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5da7ed-9dc1-4fa8-b5b9-a8a5ce851a9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95457050-25f0-42f2-92c5-1956b7ca9507",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_mlp = train_data.select(['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', \n",
    "                                  'HourlyStationPressure', 'HourlyWindGustSpeed', 'TWO_HR_DELAY'])\n",
    "df_test_mlp = test_data.select(['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', \n",
    "                                 'HourlyStationPressure', 'HourlyWindGustSpeed', 'TWO_HR_DELAY'])\n",
    "                                 \n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', \n",
    "                   'HourlyStationPressure', 'HourlyWindGustSpeed']\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_mlp)\n",
    "otpw_test_transform = assembler.transform(df_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e044ef03-7f32-431f-8b56-82fdd9034cfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9225987010506778\n",
      "weightedPrecision: 0.8662997477094988\n",
      "weightedRecall: 0.9225987010506778\n",
      "f1: 0.8855039605891918\n"
     ]
    }
   ],
   "source": [
    "layers = [len(feature_columns), 4, 2]\n",
    "mlp = MultilayerPerceptronClassifier(featuresCol=\"features\", \n",
    "                                     labelCol=target_label, \n",
    "                                     layers=layers, seed=42)\n",
    "\n",
    "model = mlp.fit(otpw_train_transform)\n",
    "\n",
    "pred = model.transform(otpw_test_transform)\n",
    "\n",
    "metric_list = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
    "for metric in metric_list:\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                labelCol=target_label, \n",
    "                                                metricName=metric\n",
    "                                                )\n",
    "    result = evaluator.evaluate(pred)\n",
    "\n",
    "    print(f\"{metric}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064467b3-63a0-4088-8253-e066a5daaa14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='MultilayerPerceptronClassifier_bd71658a548e', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlayers = model.layers\n",
    "wlayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48a1431-1fa7-435e-95fe-9c97c5013f35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8343729f-c4ae-4071-a238-0dd6bfb44faf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_mlp = train_data.select(['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', \n",
    "                                  'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY', 'TWO_HR_DELAY'])\n",
    "df_test_mlp = test_data.select(['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', \n",
    "                                 'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY', 'TWO_HR_DELAY'])\n",
    "                                 \n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', \n",
    "                   'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', \n",
    "                   'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY']\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_mlp)\n",
    "otpw_test_transform = assembler.transform(df_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4983aaa3-5483-473a-857b-147fcdf84f50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9075594122862221\n",
      "weightedPrecision: 0.9265980176160474\n",
      "weightedRecall: 0.9075594122862221\n",
      "f1: 0.9152963687591603\n"
     ]
    }
   ],
   "source": [
    "layers = [len(feature_columns), 4, 2]\n",
    "mlp = MultilayerPerceptronClassifier(featuresCol=\"features\", \n",
    "                                     labelCol=target_label, \n",
    "                                     layers=layers, seed=42)\n",
    "\n",
    "model = mlp.fit(otpw_train_transform)\n",
    "\n",
    "pred = model.transform(otpw_test_transform)\n",
    "\n",
    "metric_list = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
    "for metric in metric_list:\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                                labelCol=target_label, \n",
    "                                                metricName=metric\n",
    "                                                )\n",
    "    result = evaluator.evaluate(pred)\n",
    "\n",
    "    print(f\"{metric}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f9a905-85cf-44c0-9762-4f281da796f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='MultilayerPerceptronClassifier_4a6c7cf7e509', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlayers = model.layers\n",
    "wlayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "032b3023-a160-4a96-a2e6-8e4ec011eb13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Experiment 3\n",
    "\n",
    "This experiment used the features from Experiment 2, but with hypertuned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208ebc8a-a0ee-4f9a-8dc9-cef4fa8f1ebb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.922628926826379\n",
      "weightedPrecision: 0.8512441366167958\n",
      "weightedRecall: 0.922628926826379\n",
      "f1: 0.8855001864784349\n"
     ]
    }
   ],
   "source": [
    "blockSize = 2\n",
    "hidden_size_1 = 2\n",
    "layers = [13, hidden_size_1, 2]\n",
    "maxIter = 2\n",
    "stepSize = 0.04908798897714254\n",
    "\n",
    "layers = [len(feature_columns), 4, 2]\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "                                     featuresCol=\"features\", \n",
    "                                     labelCol=target_label, \n",
    "                                     layers=layers, \n",
    "                                     blockSize=blockSize, \n",
    "                                     maxIter=maxIter, \n",
    "                                     stepSize=stepSize, \n",
    "                                     seed=42\n",
    "                                    )\n",
    "\n",
    "\n",
    "model = mlp.fit(otpw_train_transform)\n",
    "\n",
    "pred = model.transform(otpw_test_transform)\n",
    "\n",
    "metric_list = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
    "for metric in metric_list:\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "                                                  predictionCol=\"prediction\", \n",
    "                                                  labelCol=target_label, \n",
    "                                                  metricName=metric\n",
    "                                                  )\n",
    "    result = evaluator.evaluate(pred)\n",
    "\n",
    "    print(f\"{metric}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc91468-9d2c-4b98-b047-f664f8389a45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### MLP Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfdb08e-5f11-4141-9643-f8f188f2b400",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set dataset\n",
    "mlp_feat_col = [col for col in df_train_mlp.columns if col != \"TWO_HR_DELAY\"]\n",
    "mlp_label_col = \"TWO_HR_DELAY\"\n",
    "mlp_data = (df_train_mlp, df_test_mlp)\n",
    "\n",
    "# Set model\n",
    "mlp = MultilayerPerceptronClassifier(featuresCol=\"vect_feat\", \n",
    "                                     labelCol=mlp_label_col, \n",
    "                                     seed=42)\n",
    "\n",
    "# Set evaluator and metrics\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \n",
    "                                              labelCol=mlp_label_col, \n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "# Set layer sizes\n",
    "input_size = len(mlp_feat_col)\n",
    "output_size = 2\n",
    "hs1 = hp.choice(\"hidden_size_1\", [4, 5, 10])\n",
    "hs2 = hp.choice(\"hidden_size_2\", [3, 4, 6])\n",
    "hs3 = hp.choice(\"hidden_size_3\", [4, 6, 8])\n",
    "\n",
    "# Set search space for hypertuning\n",
    "mlp_search_space = {\n",
    "                \"layers\": hp.choice(\"layers\", [\n",
    "                        [input_size, hs1, output_size],\n",
    "                        [input_size, hs1, hs2, output_size],\n",
    "                        [input_size, hs1, hs2, hs3, output_size],\n",
    "                        ]),\n",
    "                \"blockSize\": hp.choice(\"blockSize\", [32, 64, 128]),\n",
    "                \"maxIter\": hp.choice(\"maxIter\", [10, 15, 20]),\n",
    "                \"stepSize\": hp.uniform(\"stepSize\", 0.01, 0.1),\n",
    "                }\n",
    "\n",
    "\n",
    "# Set max evaluations\n",
    "max_evals = 1\n",
    "\n",
    "# Set model type, experiment number for logging\n",
    "model_type = \"mlp\"\n",
    "experiment_num = 1\n",
    "\n",
    "# Set pipeline\n",
    "pipeline = pipelineFunc(mlp, mlp_feat_col, mlp_label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f4540b-ffb8-41c2-9d1a-a1ee3b5253b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|██████████| 1/1 [05:34<00:00, 334.17s/trial, best loss: -0.9226655230570485]\r100%|██████████| 1/1 [05:34<00:00, 334.17s/trial, best loss: -0.9226655230570485]\n"
     ]
    }
   ],
   "source": [
    "best_params, best_acc = hyperTuneFunc(mlp_data, pipeline, evaluator, model_type, experiment_num, mlp_search_space, max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb94e735-5e49-48c5-ac7c-6749655c372d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'blockSize': 2, 'hidden_size_1': 2, 'layers': 0, 'maxIter': 2, 'stepSize': 0.04908798897714254}\n",
      "Best Accuracy: 0.9226655230570485\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Accuracy: {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da32acb4-9344-4c58-a85e-938a90ac5e08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model 7: Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTM networks offer a promising avenue for predicting flight delays due to their adeptness in grasping intricate temporal connections within sequential data. These models, when applied to flight delay prediction, harness a blend of weather and flight data for precise forecasts.\n",
    "\n",
    "These models can process varying sequence lengths adeptly, accommodating irregular intervals in flight data due to unforeseen events. However, in our endeavor to apply LSTMs within PySpark for flight delay prediction, we encountered a limitation. The scale of our Spark DataFrame was too extensive to fit into a single numpy array for training the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7641fc5-72c0-43ed-9c42-af403d6c19ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>QUARTER</th><th>YEAR</th><th>MONTH</th><th>DAY_OF_MONTH</th><th>DAY_OF_WEEK</th><th>FL_DATE</th><th>CRS_DEP_TIME</th><th>DEP_DELAY_GROUP</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>ARR_DELAY_GROUP</th><th>CANCELLATION_CODE</th><th>CRS_ELAPSED_TIME</th><th>ACTUAL_ELAPSED_TIME</th><th>AIR_TIME</th><th>DISTANCE</th><th>sched_depart_date_time</th><th>two_hours_prior_depart_UTC</th><th>dest_airport_lon</th><th>dest_airport_lat</th><th>STATION</th><th>DATE</th><th>LATITUDE</th><th>LONGITUDE</th><th>ELEVATION</th><th>DEP_TIME</th><th>HourlyPrecipitation</th><th>HourlyVisibility</th><th>HourlyPresentWeatherType</th><th>HourlySkyConditions</th><th>HourlyPressureChange</th><th>HourlyPressureTendency</th><th>HourlyStationPressure</th><th>HourlyWindDirection</th><th>HourlyWindGustSpeed</th><th>HourlyWindSpeed</th><th>ARR_DELAY</th><th>ARR_DEL15</th><th>DEP_DEL15</th><th>DEP_TIME_BLK_DET</th><th>HOUR_BLK</th><th>HOUR_BLK_2</th><th>TWO_HR_PROB</th><th>TWO_HR_DELAY</th><th>OP_UNIQUE_CARRIER</th><th>ORIGIN_AIRPORT_ID</th><th>ORIGIN_CITY_MARKET_ID</th><th>ORIGIN_STATE_FIPS</th><th>DEST_AIRPORT_ID</th><th>DEST_CITY_MARKET_ID</th><th>DEST_STATE_FIPS</th><th>DEP_TIME_BLK</th><th>ARR_TIME_BLK</th><th>CANCELLED</th><th>DIVERTED</th><th>DISTANCE_GROUP</th><th>i_HOUR_BLK</th><th>delay_features</th><th>pca_delay_features</th><th>CARRIER_DELAY</th><th>WEATHER_DELAY</th><th>NAS_DELAY</th><th>SECURITY_DELAY</th><th>LATE_AIRCRAFT_DELAY</th><th>DEP_DELAY</th><th>sum_lat_lon</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>475497</td><td>0</td><td>500080</td><td>568684</td><td>31182239</td><td>163</td><td>566086</td><td>566086</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>31670371</td><td>470530</td><td>28318649</td><td>47894</td><td>20812402</td><td>3045741</td><td>20805185</td><td>0</td><td>0</td><td>27513512</td><td>0</td><td>31613185</td><td>203939</td><td>203939</td><td>0</td><td>0</td><td>720964</td><td>720964</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         475497,
         0,
         500080,
         568684,
         31182239,
         163,
         566086,
         566086,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         31670371,
         470530,
         28318649,
         47894,
         20812402,
         3045741,
         20805185,
         0,
         0,
         27513512,
         0,
         31613185,
         203939,
         203939,
         0,
         0,
         720964,
         720964,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "QUARTER",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "YEAR",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "MONTH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DAY_OF_MONTH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DAY_OF_WEEK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY_GROUP",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY_GROUP",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLATION_CODE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ELAPSED_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ACTUAL_ELAPSED_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "AIR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sched_depart_date_time",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "two_hours_prior_depart_UTC",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dest_airport_lon",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dest_airport_lat",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "STATION",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DATE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LATITUDE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LONGITUDE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ELEVATION",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPrecipitation",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyVisibility",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPresentWeatherType",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlySkyConditions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPressureChange",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyPressureTendency",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyStationPressure",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindDirection",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindGustSpeed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HourlyWindSpeed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_DEL15",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DEL15",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME_BLK_DET",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HOUR_BLK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "HOUR_BLK_2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "TWO_HR_PROB",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "TWO_HR_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "OP_UNIQUE_CARRIER",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_AIRPORT_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_MARKET_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_STATE_FIPS",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEST_AIRPORT_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_MARKET_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEST_STATE_FIPS",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME_BLK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME_BLK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DIVERTED",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE_GROUP",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "i_HOUR_BLK",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "delay_features",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "pca_delay_features",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CARRIER_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WEATHER_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "NAS_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "SECURITY_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "LATE_AIRCRAFT_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_DELAY",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sum_lat_lon",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of null values in each column\n",
    "display(df_otpw.select([count(when(df_otpw[c].isNull(), c)).alias(c) for c in df_otpw.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73f3d03-0f20-4ad4-a0f4-aa19dec07403",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_data.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51968231-7414-4509-a384-aa21824cf897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_data.limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f01e0f-7cef-4c66-8c86-5395747d4d32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_lstm_model = train_data.select(['DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed', 'TWO_HR_DELAY'])\n",
    "df_test_lstm_model = test_data.select(['DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed', 'TWO_HR_DELAY'])\n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed']\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "\n",
    "# Prepare data\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_xgb_model)\n",
    "otpw_test_transform = assembler.transform(df_test_xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4e3edd-35fc-46b3-88d2-ab1fb99ee43d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_lstm_model = train_data.select(['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility',  'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY', 'TWO_HR_DELAY'])\n",
    "df_test_lstm_model = test_data.select(['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility',  'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY', 'TWO_HR_DELAY'])\n",
    "feature_columns = ['DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility',  'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY']\n",
    "\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "otpw_train_transform = assembler.transform(df_train_lstm_model)\n",
    "otpw_test_transform = assembler.transform(df_test_lstm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c9ee60-3e93-4469-b63e-e3be6b1e3c34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001b[0m\n",
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mLast messages on stderr:\u001b[0m\n",
       "\u001b[0;31m3396)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3396)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1467)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1467)\u001b[0m\n",
       "\u001b[0;31m\tat scala.Option.foreach(Option.scala:407)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1467)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3709)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3633)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3621)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1213)\u001b[0m\n",
       "\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n",
       "\u001b[0;31m\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1201)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2841)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2824)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2936)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPythonInternal$4(Dataset.scala:4543)\u001b[0m\n",
       "\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1683)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPythonInternal$1(Dataset.scala:4547)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPythonInternal$1$adapted(Dataset.scala:4512)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4637)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:926)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4635)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:242)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:453)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:177)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:127)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:403)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4635)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.collectAsArrowToPythonInternal(Dataset.scala:4512)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4574)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4573)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:154)\u001b[0m\n",
       "\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1683)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:156)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:151)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:125)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:118)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:71)\u001b[0m\n",
       "\u001b[0;31m\tat scala.util.Try$.apply(Try.scala:213)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:71)\u001b[0m\n",
       "\u001b[0;31mCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$9246/1122250678: (struct<DAY_OF_MONTH_double_VectorAssembler_d7887a9ef877:double,MONTH_double_VectorAssembler_d7887a9ef877:double,ORIGIN_AIRPORT_ID:double,sum_lat_lon_double_VectorAssembler_d7887a9ef877:double,HourlyVisibility:double,HourlyStationPressure:double,HourlyWindGustSpeed:double,CARRIER_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,WEATHER_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,NAS_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SECURITY_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,LATE_AIRCRAFT_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:248)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.$anonfun$next$1(ArrowConverters.scala:141)\u001b[0m\n",
       "\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1683)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.next(ArrowConverters.scala:154)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.next(ArrowConverters.scala:87)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.Iterator.foreach(Iterator.scala:943)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPythonInternal$5(Dataset.scala:4545)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2935)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\u001b[0m\n",
       "\u001b[0;31m\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\u001b[0m\n",
       "\u001b[0;31m\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:191)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\u001b[0m\n",
       "\u001b[0;31m\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\u001b[0m\n",
       "\u001b[0;31m\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:909)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1683)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:912)\u001b[0m\n",
       "\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n",
       "\u001b[0;31m\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\u001b[0m\n",
       "\u001b[0;31m\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001b[0m\n",
       "\u001b[0;31m\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001b[0m\n",
       "\u001b[0;31m\tat java.lang.Thread.run(Thread.java:750)\u001b[0m\n",
       "\u001b[0;31mCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\u001b[0m\n",
       "\u001b[0;31mremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:293)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:262)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\u001b[0m\n",
       "\u001b[0;31m\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:262)\u001b[0m\n",
       "\u001b[0;31m\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:145)\u001b[0m\n",
       "\u001b[0;31m\t... 48 more\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31m  warn(msg)\u001b[0m\n",
       "\u001b[0;31m/databricks/python/lib/python3.10/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\u001b[0m\n",
       "\u001b[0;31m  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\u001b[0m\n",
       "\u001b[0;31m/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\u001b[0m\n",
       "\u001b[0;31m  warnings.warn(\u001b[0m\n",
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mLast messages on stdout:\u001b[0m\n",
       "\u001b[0;31mNOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31mTo exit, you will have to explicitly quit this process, by either sending\u001b[0m\n",
       "\u001b[0;31m\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31mTo read more about this, see https://github.com/ipython/ipython/issues/2049\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31mTo connect another client to this kernel, use:\u001b[0m\n",
       "\u001b[0;31m    --existing /databricks/kernel-connections/151c109e508beb94ab297631d523fd4cff01a46ba2e944e0b2caf4be3c718ef0.json\u001b[0m\n",
       "\u001b[0;31mBefore Undersampling Label Ratio: 12.782856000921983\u001b[0m\n",
       "\u001b[0;31mAfter Undersampling Label Ratio: 8.99837971728508\u001b[0m\n",
       "\u001b[0;31mroot\u001b[0m\n",
       "\u001b[0;31m-- QUARTER: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- YEAR: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- MONTH: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DAY_OF_MONTH: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DAY_OF_WEEK: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- FL_DATE: date (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- CRS_DEP_TIME: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEP_DELAY_GROUP: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- CRS_ARR_TIME: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ARR_TIME: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ARR_DELAY_GROUP: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- CANCELLATION_CODE: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- CRS_ELAPSED_TIME: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ACTUAL_ELAPSED_TIME: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- AIR_TIME: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DISTANCE: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- sched_depart_date_time: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- two_hours_prior_depart_UTC: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- dest_airport_lon: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- dest_airport_lat: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- STATION: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DATE: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- LATITUDE: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- LONGITUDE: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ELEVATION: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEP_TIME: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyPrecipitation: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyVisibility: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyPresentWeatherType: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlySkyConditions: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyPressureChange: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyPressureTendency: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyStationPressure: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyWindDirection: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyWindGustSpeed: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HourlyWindSpeed: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ARR_DELAY: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ARR_DEL15: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEP_DEL15: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEP_TIME_BLK_DET: string (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HOUR_BLK: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- HOUR_BLK_2: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- TWO_HR_PROB: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- OP_UNIQUE_CARRIER: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ORIGIN_AIRPORT_ID: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ORIGIN_CITY_MARKET_ID: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ORIGIN_STATE_FIPS: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEST_AIRPORT_ID: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEST_CITY_MARKET_ID: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEST_STATE_FIPS: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEP_TIME_BLK: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- ARR_TIME_BLK: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- CANCELLED: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DIVERTED: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DISTANCE_GROUP: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- i_HOUR_BLK: double (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- delay_features: vector (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- pca_delay_features: vector (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- CARRIER_DELAY: vector (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- WEATHER_DELAY: vector (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- NAS_DELAY: vector (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- SECURITY_DELAY: vector (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- LATE_AIRCRAFT_DELAY: vector (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- DEP_DELAY: vector (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- sum_lat_lon: float (nullable = true)\u001b[0m\n",
       "\u001b[0;31m-- TWO_HR_DELAY: integer (nullable = true)\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31mColumn DAY_OF_MONTH does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn MONTH does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn HourlyVisibility does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn HourlyPressureTendency does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn HourlyStationPressure does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn HourlyWindDirection does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn HourlyWindSpeed does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn ORIGIN_AIRPORT_ID does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn DEST_AIRPORT_ID does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mColumn sum_lat_lon does not contain date-like values.\u001b[0m\n",
       "\u001b[0;31mQUARTER                  int32\u001b[0m\n",
       "\u001b[0;31mYEAR                     int32\u001b[0m\n",
       "\u001b[0;31mMONTH                    int32\u001b[0m\n",
       "\u001b[0;31mDAY_OF_WEEK              int32\u001b[0m\n",
       "\u001b[0;31mFL_DATE                 object\u001b[0m\n",
       "\u001b[0;31m                        ...   \u001b[0m\n",
       "\u001b[0;31mNAS_DELAY               object\u001b[0m\n",
       "\u001b[0;31mSECURITY_DELAY          object\u001b[0m\n",
       "\u001b[0;31mLATE_AIRCRAFT_DELAY     object\u001b[0m\n",
       "\u001b[0;31mDEP_DELAY               object\u001b[0m\n",
       "\u001b[0;31msum_lat_lon            float32\u001b[0m\n",
       "\u001b[0;31mLength: 64, dtype: object\u001b[0m\n",
       "\u001b[0;31m       QUARTER  YEAR  ...                DEP_DELAY  sum_lat_lon\u001b[0m\n",
       "\u001b[0;31m0            2  2019  ...    [0.45589903403465726]   -79.099998\u001b[0m\n",
       "\u001b[0;31m1            2  2019  ...    [-0.0683848551051986]   -54.500000\u001b[0m\n",
       "\u001b[0;31m2            2  2019  ...   [-0.09117980680693145]   -37.499996\u001b[0m\n",
       "\u001b[0;31m3            1  2019  ...   [-0.04558990340346573]   -54.500004\u001b[0m\n",
       "\u001b[0;31m4            3  2019  ...     [0.7294384544554516]   -54.900002\u001b[0m\n",
       "\u001b[0;31m...        ...   ...  ...                      ...          ...\u001b[0m\n",
       "\u001b[0;31m74183        3  2019  ...    [-0.0683848551051986]   -62.199997\u001b[0m\n",
       "\u001b[0;31m74184        1  2019  ...    [-0.2735394204207944]   -65.400002\u001b[0m\n",
       "\u001b[0;31m74185        1  2019  ...    [0.22794951701732863]   -65.400002\u001b[0m\n",
       "\u001b[0;31m74186        4  2019  ...  [-0.022794951701732864]   -64.899994\u001b[0m\n",
       "\u001b[0;31m74187        3  2019  ...  [-0.022794951701732864]   -32.299999\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31m[74188 rows x 64 columns]\u001b[0m\n",
       "\u001b[0;31mQUARTER                  int32\u001b[0m\n",
       "\u001b[0;31mYEAR                     int32\u001b[0m\n",
       "\u001b[0;31mMONTH                    int32\u001b[0m\n",
       "\u001b[0;31mDAY_OF_WEEK              int32\u001b[0m\n",
       "\u001b[0;31mFL_DATE                 object\u001b[0m\n",
       "\u001b[0;31m                        ...   \u001b[0m\n",
       "\u001b[0;31mNAS_DELAY               object\u001b[0m\n",
       "\u001b[0;31mSECURITY_DELAY          object\u001b[0m\n",
       "\u001b[0;31mLATE_AIRCRAFT_DELAY     object\u001b[0m\n",
       "\u001b[0;31mDEP_DELAY               object\u001b[0m\n",
       "\u001b[0;31msum_lat_lon            float32\u001b[0m\n",
       "\u001b[0;31mLength: 64, dtype: object\u001b[0m\n",
       "\u001b[0;31m       QUARTER  YEAR  ...               DEP_DELAY  sum_lat_lon\u001b[0m\n",
       "\u001b[0;31m0            2  2019  ...   [0.45589903403465726]   -79.099998\u001b[0m\n",
       "\u001b[0;31m1            2  2019  ...   [-0.0683848551051986]   -54.500000\u001b[0m\n",
       "\u001b[0;31m2            2  2019  ...  [-0.09117980680693145]   -37.499996\u001b[0m\n",
       "\u001b[0;31m3            1  2019  ...  [-0.04558990340346573]   -54.500004\u001b[0m\n",
       "\u001b[0;31m4            3  2019  ...    [0.7294384544554516]   -54.900002\u001b[0m\n",
       "\u001b[0;31m...        ...   ...  ...                     ...          ...\u001b[0m\n",
       "\u001b[0;31m74177        4  2019  ...   [0.38751417892945866]   -54.500000\u001b[0m\n",
       "\u001b[0;31m74178        2  2019  ...    [1.0713627299814446]   -48.299995\u001b[0m\n",
       "\u001b[0;31m74179        2  2019  ...  [-0.15956466191213003]   -51.199997\u001b[0m\n",
       "\u001b[0;31m74180        3  2019  ...    [0.5926687442450544]   -51.199997\u001b[0m\n",
       "\u001b[0;31m74181        1  2019  ...  [-0.09117980680693145]   -58.699997\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31m[74182 rows x 64 columns]\u001b[0m\n",
       "\u001b[0;31mMONTH                       int32\u001b[0m\n",
       "\u001b[0;31mHourlyVisibility          float64\u001b[0m\n",
       "\u001b[0;31mHourlyPressureTendency    float64\u001b[0m\n",
       "\u001b[0;31mHourlyStationPressure     float64\u001b[0m\n",
       "\u001b[0;31mHourlyWindDirection       float64\u001b[0m\n",
       "\u001b[0;31mHourlyWindSpeed           float64\u001b[0m\n",
       "\u001b[0;31mORIGIN_AIRPORT_ID         float64\u001b[0m\n",
       "\u001b[0;31mDEST_AIRPORT_ID           float64\u001b[0m\n",
       "\u001b[0;31msum_lat_lon               float32\u001b[0m\n",
       "\u001b[0;31mdtype: object\u001b[0m\n",
       "\u001b[0;31m       MONTH  HourlyVisibility  ...  DEST_AIRPORT_ID  sum_lat_lon\u001b[0m\n",
       "\u001b[0;31m0          5          3.988984  ...              9.0   -79.099998\u001b[0m\n",
       "\u001b[0;31m1          5          3.871201  ...             26.0   -54.500000\u001b[0m\n",
       "\u001b[0;31m2          6          4.077537  ...             20.0   -37.499996\u001b[0m\n",
       "\u001b[0;31m3          3          3.555348  ...             25.0   -54.500004\u001b[0m\n",
       "\u001b[0;31m4          7          4.304065  ...             62.0   -54.900002\u001b[0m\n",
       "\u001b[0;31m...      ...               ...  ...              ...          ...\u001b[0m\n",
       "\u001b[0;31m73857      9          4.025352  ...              7.0   -45.799999\u001b[0m\n",
       "\u001b[0;31m73858      6          4.077537  ...             87.0   -47.299995\u001b[0m\n",
       "\u001b[0;31m73859     12          3.295837  ...              4.0   -84.500000\u001b[0m\n",
       "\u001b[0;31m73860     12          4.158883  ...             50.0   -47.599998\u001b[0m\n",
       "\u001b[0;31m73861      9          4.127134  ...              1.0   -45.900002\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31m[73862 rows x 9 columns]\u001b[0m\n",
       "\u001b[0;31mDAY_OF_MONTH: 0 null values\u001b[0m\n",
       "\u001b[0;31mMONTH: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyVisibility: 237 null values\u001b[0m\n",
       "\u001b[0;31mHourlyPressureTendency: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyStationPressure: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyWindDirection: 153890 null values\u001b[0m\n",
       "\u001b[0;31mHourlyWindSpeed: 176258 null values\u001b[0m\n",
       "\u001b[0;31mORIGIN_AIRPORT_ID: 0 null values\u001b[0m\n",
       "\u001b[0;31mDEST_AIRPORT_ID: 0 null values\u001b[0m\n",
       "\u001b[0;31msum_lat_lon: 0 null values\u001b[0m\n",
       "\u001b[0;31mMONTH                      int32\u001b[0m\n",
       "\u001b[0;31mHourlyVisibility         float64\u001b[0m\n",
       "\u001b[0;31mHourlyStationPressure    float64\u001b[0m\n",
       "\u001b[0;31mHourlyWindGustSpeed      float64\u001b[0m\n",
       "\u001b[0;31mORIGIN_AIRPORT_ID        float64\u001b[0m\n",
       "\u001b[0;31mDEST_AIRPORT_ID          float64\u001b[0m\n",
       "\u001b[0;31mdtype: object\u001b[0m\n",
       "\u001b[0;31m       MONTH  HourlyVisibility  ...  ORIGIN_AIRPORT_ID  DEST_AIRPORT_ID\u001b[0m\n",
       "\u001b[0;31m0          5          3.988984  ...                6.0              9.0\u001b[0m\n",
       "\u001b[0;31m1          5          3.871201  ...               11.0             26.0\u001b[0m\n",
       "\u001b[0;31m2          6          4.077537  ...                5.0             20.0\u001b[0m\n",
       "\u001b[0;31m3          3          3.555348  ...                0.0             25.0\u001b[0m\n",
       "\u001b[0;31m4          7          4.304065  ...                1.0             62.0\u001b[0m\n",
       "\u001b[0;31m...      ...               ...  ...                ...              ...\u001b[0m\n",
       "\u001b[0;31m73853      2          3.526361  ...                9.0              8.0\u001b[0m\n",
       "\u001b[0;31m73854     10          3.951244  ...                2.0             16.0\u001b[0m\n",
       "\u001b[0;31m73855      4          4.248495  ...               45.0              9.0\u001b[0m\n",
       "\u001b[0;31m73856     11          3.610918  ...                3.0             24.0\u001b[0m\n",
       "\u001b[0;31m73857      3          4.204693  ...               52.0             38.0\u001b[0m\n",
       "\u001b[0;31m\u001b[0m\n",
       "\u001b[0;31m[73858 rows x 6 columns]\u001b[0m\n",
       "\u001b[0;31mDAY_OF_MONTH: 0 null values\u001b[0m\n",
       "\u001b[0;31mMONTH: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyVisibility: 24318 null values\u001b[0m\n",
       "\u001b[0;31mHourlyStationPressure: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyWindGustSpeed: 0 null values\u001b[0m\n",
       "\u001b[0;31mORIGIN_AIRPORT_ID: 0 null values\u001b[0m\n",
       "\u001b[0;31mDEST_AIRPORT_ID: 0 null values\u001b[0m\n",
       "\u001b[0;31mDAY_OF_MONTH: 0 null values\u001b[0m\n",
       "\u001b[0;31mMONTH: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyVisibility: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyStationPressure: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyWindGustSpeed: 0 null values\u001b[0m\n",
       "\u001b[0;31mORIGIN_AIRPORT_ID: 0 null values\u001b[0m\n",
       "\u001b[0;31mDEST_AIRPORT_ID: 0 null values\u001b[0m\n",
       "\u001b[0;31m176460\u001b[0m\n",
       "\u001b[0;31m176324\u001b[0m\n",
       "\u001b[0;31mDAY_OF_MONTH: 0 null values\u001b[0m\n",
       "\u001b[0;31mMONTH: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyVisibility: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyStationPressure: 0 null values\u001b[0m\n",
       "\u001b[0;31mHourlyWindGustSpeed: 0 null values\u001b[0m\n",
       "\u001b[0;31mORIGIN_AIRPORT_ID: 0 null values\u001b[0m\n",
       "\u001b[0;31mDEST_AIRPORT_ID: 0 null values\u001b[0m\n",
       "\u001b[0;31mTWO_HR_DELAY: 0 null values\u001b[0m\n",
       "\u001b[0;31mBefore Undersampling Label Ratio: 12.782856000921983\u001b[0m\n",
       "\u001b[0;31mAfter Undersampling Label Ratio: 8.998572743256723\u001b[0m\n",
       "\u001b[0;31mAccuracy: 0.9225314248327361\u001b[0m\n",
       "\u001b[0;31mPrecision: 0.3172043010752688\u001b[0m\n",
       "\u001b[0;31mRecall: 0.00020608257973474728\u001b[0m\n",
       "\u001b[0;31mDataFrame[prediction: double, TWO_HR_DELAY: int, binary_prediction: double]\u001b[0m"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001b[0m\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mLast messages on stderr:\u001b[0m\n\u001b[0;31m3396)\u001b[0m\n\u001b[0;31m\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\u001b[0m\n\u001b[0;31m\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\u001b[0m\n\u001b[0;31m\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3396)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1467)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1467)\u001b[0m\n\u001b[0;31m\tat scala.Option.foreach(Option.scala:407)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1467)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3709)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3633)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3621)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1213)\u001b[0m\n\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n\u001b[0;31m\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1201)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2841)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2824)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2936)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPythonInternal$4(Dataset.scala:4543)\u001b[0m\n\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1683)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPythonInternal$1(Dataset.scala:4547)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPythonInternal$1$adapted(Dataset.scala:4512)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4637)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:926)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4635)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:242)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:453)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:177)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:127)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:403)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4635)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.collectAsArrowToPythonInternal(Dataset.scala:4512)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4574)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4573)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:154)\u001b[0m\n\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1683)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:156)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:151)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:125)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:118)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:71)\u001b[0m\n\u001b[0;31m\tat scala.util.Try$.apply(Try.scala:213)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:71)\u001b[0m\n\u001b[0;31mCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$9246/1122250678: (struct<DAY_OF_MONTH_double_VectorAssembler_d7887a9ef877:double,MONTH_double_VectorAssembler_d7887a9ef877:double,ORIGIN_AIRPORT_ID:double,sum_lat_lon_double_VectorAssembler_d7887a9ef877:double,HourlyVisibility:double,HourlyStationPressure:double,HourlyWindGustSpeed:double,CARRIER_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,WEATHER_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,NAS_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SECURITY_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,LATE_AIRCRAFT_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,DEP_DELAY:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:248)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.$anonfun$next$1(ArrowConverters.scala:141)\u001b[0m\n\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1683)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.next(ArrowConverters.scala:154)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.execution.arrow.ArrowConverters$ArrowBatchIterator.next(ArrowConverters.scala:87)\u001b[0m\n\u001b[0;31m\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\u001b[0m\n\u001b[0;31m\tat scala.collection.Iterator.foreach(Iterator.scala:943)\u001b[0m\n\u001b[0;31m\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\u001b[0m\n\u001b[0;31m\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\u001b[0m\n\u001b[0;31m\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\u001b[0m\n\u001b[0;31m\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\u001b[0m\n\u001b[0;31m\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\u001b[0m\n\u001b[0;31m\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\u001b[0m\n\u001b[0;31m\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\u001b[0m\n\u001b[0;31m\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\u001b[0m\n\u001b[0;31m\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\u001b[0m\n\u001b[0;31m\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\u001b[0m\n\u001b[0;31m\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\u001b[0m\n\u001b[0;31m\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\u001b[0m\n\u001b[0;31m\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\u001b[0m\n\u001b[0;31m\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\u001b[0m\n\u001b[0;31m\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPythonInternal$5(Dataset.scala:4545)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.SparkContext.$anonfun$runJob$3(SparkContext.scala:2935)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\u001b[0m\n\u001b[0;31m\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\u001b[0m\n\u001b[0;31m\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:191)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:181)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:146)\u001b[0m\n\u001b[0;31m\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:146)\u001b[0m\n\u001b[0;31m\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:909)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1683)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:912)\u001b[0m\n\u001b[0;31m\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\u001b[0m\n\u001b[0;31m\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\u001b[0m\n\u001b[0;31m\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001b[0m\n\u001b[0;31m\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001b[0m\n\u001b[0;31m\tat java.lang.Thread.run(Thread.java:750)\u001b[0m\n\u001b[0;31mCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\u001b[0m\n\u001b[0;31mremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\u001b[0m\n\u001b[0;31m\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:293)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:262)\u001b[0m\n\u001b[0;31m\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\u001b[0m\n\u001b[0;31m\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\u001b[0m\n\u001b[0;31m\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:262)\u001b[0m\n\u001b[0;31m\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:145)\u001b[0m\n\u001b[0;31m\t... 48 more\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31m  warn(msg)\u001b[0m\n\u001b[0;31m/databricks/python/lib/python3.10/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\u001b[0m\n\u001b[0;31m  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\u001b[0m\n\u001b[0;31m/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\u001b[0m\n\u001b[0;31m  warnings.warn(\u001b[0m\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mLast messages on stdout:\u001b[0m\n\u001b[0;31mNOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31mTo exit, you will have to explicitly quit this process, by either sending\u001b[0m\n\u001b[0;31m\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31mTo read more about this, see https://github.com/ipython/ipython/issues/2049\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31mTo connect another client to this kernel, use:\u001b[0m\n\u001b[0;31m    --existing /databricks/kernel-connections/151c109e508beb94ab297631d523fd4cff01a46ba2e944e0b2caf4be3c718ef0.json\u001b[0m\n\u001b[0;31mBefore Undersampling Label Ratio: 12.782856000921983\u001b[0m\n\u001b[0;31mAfter Undersampling Label Ratio: 8.99837971728508\u001b[0m\n\u001b[0;31mroot\u001b[0m\n\u001b[0;31m-- QUARTER: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- YEAR: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- MONTH: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- DAY_OF_MONTH: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- DAY_OF_WEEK: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- FL_DATE: date (nullable = true)\u001b[0m\n\u001b[0;31m-- CRS_DEP_TIME: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- DEP_DELAY_GROUP: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- CRS_ARR_TIME: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- ARR_TIME: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- ARR_DELAY_GROUP: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- CANCELLATION_CODE: string (nullable = true)\u001b[0m\n\u001b[0;31m-- CRS_ELAPSED_TIME: float (nullable = true)\u001b[0m\n\u001b[0;31m-- ACTUAL_ELAPSED_TIME: float (nullable = true)\u001b[0m\n\u001b[0;31m-- AIR_TIME: float (nullable = true)\u001b[0m\n\u001b[0;31m-- DISTANCE: float (nullable = true)\u001b[0m\n\u001b[0;31m-- sched_depart_date_time: string (nullable = true)\u001b[0m\n\u001b[0;31m-- two_hours_prior_depart_UTC: string (nullable = true)\u001b[0m\n\u001b[0;31m-- dest_airport_lon: float (nullable = true)\u001b[0m\n\u001b[0;31m-- dest_airport_lat: float (nullable = true)\u001b[0m\n\u001b[0;31m-- STATION: string (nullable = true)\u001b[0m\n\u001b[0;31m-- DATE: string (nullable = true)\u001b[0m\n\u001b[0;31m-- LATITUDE: float (nullable = true)\u001b[0m\n\u001b[0;31m-- LONGITUDE: float (nullable = true)\u001b[0m\n\u001b[0;31m-- ELEVATION: float (nullable = true)\u001b[0m\n\u001b[0;31m-- DEP_TIME: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyPrecipitation: string (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyVisibility: double (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyPresentWeatherType: string (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlySkyConditions: string (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyPressureChange: double (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyPressureTendency: double (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyStationPressure: double (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyWindDirection: double (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyWindGustSpeed: double (nullable = true)\u001b[0m\n\u001b[0;31m-- HourlyWindSpeed: double (nullable = true)\u001b[0m\n\u001b[0;31m-- ARR_DELAY: double (nullable = true)\u001b[0m\n\u001b[0;31m-- ARR_DEL15: double (nullable = true)\u001b[0m\n\u001b[0;31m-- DEP_DEL15: double (nullable = true)\u001b[0m\n\u001b[0;31m-- DEP_TIME_BLK_DET: string (nullable = true)\u001b[0m\n\u001b[0;31m-- HOUR_BLK: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- HOUR_BLK_2: integer (nullable = true)\u001b[0m\n\u001b[0;31m-- TWO_HR_PROB: double (nullable = true)\u001b[0m\n\u001b[0;31m-- OP_UNIQUE_CARRIER: double (nullable = true)\u001b[0m\n\u001b[0;31m-- ORIGIN_AIRPORT_ID: double (nullable = true)\u001b[0m\n\u001b[0;31m-- ORIGIN_CITY_MARKET_ID: double (nullable = true)\u001b[0m\n\u001b[0;31m-- ORIGIN_STATE_FIPS: double (nullable = true)\u001b[0m\n\u001b[0;31m-- DEST_AIRPORT_ID: double (nullable = true)\u001b[0m\n\u001b[0;31m-- DEST_CITY_MARKET_ID: double (nullable = true)\u001b[0m\n\u001b[0;31m-- DEST_STATE_FIPS: double (nullable = true)\u001b[0m\n\u001b[0;31m-- DEP_TIME_BLK: double (nullable = true)\u001b[0m\n\u001b[0;31m-- ARR_TIME_BLK: double (nullable = true)\u001b[0m\n\u001b[0;31m-- CANCELLED: double (nullable = true)\u001b[0m\n\u001b[0;31m-- DIVERTED: double (nullable = true)\u001b[0m\n\u001b[0;31m-- DISTANCE_GROUP: double (nullable = true)\u001b[0m\n\u001b[0;31m-- i_HOUR_BLK: double (nullable = true)\u001b[0m\n\u001b[0;31m-- delay_features: vector (nullable = true)\u001b[0m\n\u001b[0;31m-- pca_delay_features: vector (nullable = true)\u001b[0m\n\u001b[0;31m-- CARRIER_DELAY: vector (nullable = true)\u001b[0m\n\u001b[0;31m-- WEATHER_DELAY: vector (nullable = true)\u001b[0m\n\u001b[0;31m-- NAS_DELAY: vector (nullable = true)\u001b[0m\n\u001b[0;31m-- SECURITY_DELAY: vector (nullable = true)\u001b[0m\n\u001b[0;31m-- LATE_AIRCRAFT_DELAY: vector (nullable = true)\u001b[0m\n\u001b[0;31m-- DEP_DELAY: vector (nullable = true)\u001b[0m\n\u001b[0;31m-- sum_lat_lon: float (nullable = true)\u001b[0m\n\u001b[0;31m-- TWO_HR_DELAY: integer (nullable = true)\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31mColumn DAY_OF_MONTH does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn MONTH does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn HourlyVisibility does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn HourlyPressureTendency does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn HourlyStationPressure does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn HourlyWindDirection does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn HourlyWindSpeed does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn ORIGIN_AIRPORT_ID does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn DEST_AIRPORT_ID does not contain date-like values.\u001b[0m\n\u001b[0;31mColumn sum_lat_lon does not contain date-like values.\u001b[0m\n\u001b[0;31mQUARTER                  int32\u001b[0m\n\u001b[0;31mYEAR                     int32\u001b[0m\n\u001b[0;31mMONTH                    int32\u001b[0m\n\u001b[0;31mDAY_OF_WEEK              int32\u001b[0m\n\u001b[0;31mFL_DATE                 object\u001b[0m\n\u001b[0;31m                        ...   \u001b[0m\n\u001b[0;31mNAS_DELAY               object\u001b[0m\n\u001b[0;31mSECURITY_DELAY          object\u001b[0m\n\u001b[0;31mLATE_AIRCRAFT_DELAY     object\u001b[0m\n\u001b[0;31mDEP_DELAY               object\u001b[0m\n\u001b[0;31msum_lat_lon            float32\u001b[0m\n\u001b[0;31mLength: 64, dtype: object\u001b[0m\n\u001b[0;31m       QUARTER  YEAR  ...                DEP_DELAY  sum_lat_lon\u001b[0m\n\u001b[0;31m0            2  2019  ...    [0.45589903403465726]   -79.099998\u001b[0m\n\u001b[0;31m1            2  2019  ...    [-0.0683848551051986]   -54.500000\u001b[0m\n\u001b[0;31m2            2  2019  ...   [-0.09117980680693145]   -37.499996\u001b[0m\n\u001b[0;31m3            1  2019  ...   [-0.04558990340346573]   -54.500004\u001b[0m\n\u001b[0;31m4            3  2019  ...     [0.7294384544554516]   -54.900002\u001b[0m\n\u001b[0;31m...        ...   ...  ...                      ...          ...\u001b[0m\n\u001b[0;31m74183        3  2019  ...    [-0.0683848551051986]   -62.199997\u001b[0m\n\u001b[0;31m74184        1  2019  ...    [-0.2735394204207944]   -65.400002\u001b[0m\n\u001b[0;31m74185        1  2019  ...    [0.22794951701732863]   -65.400002\u001b[0m\n\u001b[0;31m74186        4  2019  ...  [-0.022794951701732864]   -64.899994\u001b[0m\n\u001b[0;31m74187        3  2019  ...  [-0.022794951701732864]   -32.299999\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31m[74188 rows x 64 columns]\u001b[0m\n\u001b[0;31mQUARTER                  int32\u001b[0m\n\u001b[0;31mYEAR                     int32\u001b[0m\n\u001b[0;31mMONTH                    int32\u001b[0m\n\u001b[0;31mDAY_OF_WEEK              int32\u001b[0m\n\u001b[0;31mFL_DATE                 object\u001b[0m\n\u001b[0;31m                        ...   \u001b[0m\n\u001b[0;31mNAS_DELAY               object\u001b[0m\n\u001b[0;31mSECURITY_DELAY          object\u001b[0m\n\u001b[0;31mLATE_AIRCRAFT_DELAY     object\u001b[0m\n\u001b[0;31mDEP_DELAY               object\u001b[0m\n\u001b[0;31msum_lat_lon            float32\u001b[0m\n\u001b[0;31mLength: 64, dtype: object\u001b[0m\n\u001b[0;31m       QUARTER  YEAR  ...               DEP_DELAY  sum_lat_lon\u001b[0m\n\u001b[0;31m0            2  2019  ...   [0.45589903403465726]   -79.099998\u001b[0m\n\u001b[0;31m1            2  2019  ...   [-0.0683848551051986]   -54.500000\u001b[0m\n\u001b[0;31m2            2  2019  ...  [-0.09117980680693145]   -37.499996\u001b[0m\n\u001b[0;31m3            1  2019  ...  [-0.04558990340346573]   -54.500004\u001b[0m\n\u001b[0;31m4            3  2019  ...    [0.7294384544554516]   -54.900002\u001b[0m\n\u001b[0;31m...        ...   ...  ...                     ...          ...\u001b[0m\n\u001b[0;31m74177        4  2019  ...   [0.38751417892945866]   -54.500000\u001b[0m\n\u001b[0;31m74178        2  2019  ...    [1.0713627299814446]   -48.299995\u001b[0m\n\u001b[0;31m74179        2  2019  ...  [-0.15956466191213003]   -51.199997\u001b[0m\n\u001b[0;31m74180        3  2019  ...    [0.5926687442450544]   -51.199997\u001b[0m\n\u001b[0;31m74181        1  2019  ...  [-0.09117980680693145]   -58.699997\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31m[74182 rows x 64 columns]\u001b[0m\n\u001b[0;31mMONTH                       int32\u001b[0m\n\u001b[0;31mHourlyVisibility          float64\u001b[0m\n\u001b[0;31mHourlyPressureTendency    float64\u001b[0m\n\u001b[0;31mHourlyStationPressure     float64\u001b[0m\n\u001b[0;31mHourlyWindDirection       float64\u001b[0m\n\u001b[0;31mHourlyWindSpeed           float64\u001b[0m\n\u001b[0;31mORIGIN_AIRPORT_ID         float64\u001b[0m\n\u001b[0;31mDEST_AIRPORT_ID           float64\u001b[0m\n\u001b[0;31msum_lat_lon               float32\u001b[0m\n\u001b[0;31mdtype: object\u001b[0m\n\u001b[0;31m       MONTH  HourlyVisibility  ...  DEST_AIRPORT_ID  sum_lat_lon\u001b[0m\n\u001b[0;31m0          5          3.988984  ...              9.0   -79.099998\u001b[0m\n\u001b[0;31m1          5          3.871201  ...             26.0   -54.500000\u001b[0m\n\u001b[0;31m2          6          4.077537  ...             20.0   -37.499996\u001b[0m\n\u001b[0;31m3          3          3.555348  ...             25.0   -54.500004\u001b[0m\n\u001b[0;31m4          7          4.304065  ...             62.0   -54.900002\u001b[0m\n\u001b[0;31m...      ...               ...  ...              ...          ...\u001b[0m\n\u001b[0;31m73857      9          4.025352  ...              7.0   -45.799999\u001b[0m\n\u001b[0;31m73858      6          4.077537  ...             87.0   -47.299995\u001b[0m\n\u001b[0;31m73859     12          3.295837  ...              4.0   -84.500000\u001b[0m\n\u001b[0;31m73860     12          4.158883  ...             50.0   -47.599998\u001b[0m\n\u001b[0;31m73861      9          4.127134  ...              1.0   -45.900002\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31m[73862 rows x 9 columns]\u001b[0m\n\u001b[0;31mDAY_OF_MONTH: 0 null values\u001b[0m\n\u001b[0;31mMONTH: 0 null values\u001b[0m\n\u001b[0;31mHourlyVisibility: 237 null values\u001b[0m\n\u001b[0;31mHourlyPressureTendency: 0 null values\u001b[0m\n\u001b[0;31mHourlyStationPressure: 0 null values\u001b[0m\n\u001b[0;31mHourlyWindDirection: 153890 null values\u001b[0m\n\u001b[0;31mHourlyWindSpeed: 176258 null values\u001b[0m\n\u001b[0;31mORIGIN_AIRPORT_ID: 0 null values\u001b[0m\n\u001b[0;31mDEST_AIRPORT_ID: 0 null values\u001b[0m\n\u001b[0;31msum_lat_lon: 0 null values\u001b[0m\n\u001b[0;31mMONTH                      int32\u001b[0m\n\u001b[0;31mHourlyVisibility         float64\u001b[0m\n\u001b[0;31mHourlyStationPressure    float64\u001b[0m\n\u001b[0;31mHourlyWindGustSpeed      float64\u001b[0m\n\u001b[0;31mORIGIN_AIRPORT_ID        float64\u001b[0m\n\u001b[0;31mDEST_AIRPORT_ID          float64\u001b[0m\n\u001b[0;31mdtype: object\u001b[0m\n\u001b[0;31m       MONTH  HourlyVisibility  ...  ORIGIN_AIRPORT_ID  DEST_AIRPORT_ID\u001b[0m\n\u001b[0;31m0          5          3.988984  ...                6.0              9.0\u001b[0m\n\u001b[0;31m1          5          3.871201  ...               11.0             26.0\u001b[0m\n\u001b[0;31m2          6          4.077537  ...                5.0             20.0\u001b[0m\n\u001b[0;31m3          3          3.555348  ...                0.0             25.0\u001b[0m\n\u001b[0;31m4          7          4.304065  ...                1.0             62.0\u001b[0m\n\u001b[0;31m...      ...               ...  ...                ...              ...\u001b[0m\n\u001b[0;31m73853      2          3.526361  ...                9.0              8.0\u001b[0m\n\u001b[0;31m73854     10          3.951244  ...                2.0             16.0\u001b[0m\n\u001b[0;31m73855      4          4.248495  ...               45.0              9.0\u001b[0m\n\u001b[0;31m73856     11          3.610918  ...                3.0             24.0\u001b[0m\n\u001b[0;31m73857      3          4.204693  ...               52.0             38.0\u001b[0m\n\u001b[0;31m\u001b[0m\n\u001b[0;31m[73858 rows x 6 columns]\u001b[0m\n\u001b[0;31mDAY_OF_MONTH: 0 null values\u001b[0m\n\u001b[0;31mMONTH: 0 null values\u001b[0m\n\u001b[0;31mHourlyVisibility: 24318 null values\u001b[0m\n\u001b[0;31mHourlyStationPressure: 0 null values\u001b[0m\n\u001b[0;31mHourlyWindGustSpeed: 0 null values\u001b[0m\n\u001b[0;31mORIGIN_AIRPORT_ID: 0 null values\u001b[0m\n\u001b[0;31mDEST_AIRPORT_ID: 0 null values\u001b[0m\n\u001b[0;31mDAY_OF_MONTH: 0 null values\u001b[0m\n\u001b[0;31mMONTH: 0 null values\u001b[0m\n\u001b[0;31mHourlyVisibility: 0 null values\u001b[0m\n\u001b[0;31mHourlyStationPressure: 0 null values\u001b[0m\n\u001b[0;31mHourlyWindGustSpeed: 0 null values\u001b[0m\n\u001b[0;31mORIGIN_AIRPORT_ID: 0 null values\u001b[0m\n\u001b[0;31mDEST_AIRPORT_ID: 0 null values\u001b[0m\n\u001b[0;31m176460\u001b[0m\n\u001b[0;31m176324\u001b[0m\n\u001b[0;31mDAY_OF_MONTH: 0 null values\u001b[0m\n\u001b[0;31mMONTH: 0 null values\u001b[0m\n\u001b[0;31mHourlyVisibility: 0 null values\u001b[0m\n\u001b[0;31mHourlyStationPressure: 0 null values\u001b[0m\n\u001b[0;31mHourlyWindGustSpeed: 0 null values\u001b[0m\n\u001b[0;31mORIGIN_AIRPORT_ID: 0 null values\u001b[0m\n\u001b[0;31mDEST_AIRPORT_ID: 0 null values\u001b[0m\n\u001b[0;31mTWO_HR_DELAY: 0 null values\u001b[0m\n\u001b[0;31mBefore Undersampling Label Ratio: 12.782856000921983\u001b[0m\n\u001b[0;31mAfter Undersampling Label Ratio: 8.998572743256723\u001b[0m\n\u001b[0;31mAccuracy: 0.9225314248327361\u001b[0m\n\u001b[0;31mPrecision: 0.3172043010752688\u001b[0m\n\u001b[0;31mRecall: 0.00020608257973474728\u001b[0m\n\u001b[0;31mDataFrame[prediction: double, TWO_HR_DELAY: int, binary_prediction: double]\u001b[0m",
       "errorSummary": "<span class='ansi-red-fg'>Fatal error</span>: The Python kernel is unresponsive.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = otpw_train_transform.toPandas()\n",
    "test_data = otpw_test_transform.toPandas()\n",
    "\n",
    "X_train = train_data[feature_columns].values\n",
    "X_test = test_data[feature_columns].values\n",
    "y_train = train_data[target_label].values\n",
    "y_test = test_data[target_label].values\n",
    "\n",
    "# Reshape the input data for LSTM (samples, time steps, features)\n",
    "num_time_steps = 1  # For this example, we use each sample as a single time step\n",
    "num_features = len(feature_columns)\n",
    "X_train_lstm = X_train_scaled.reshape(-1, num_time_steps, num_features)\n",
    "X_test_lstm = X_test_scaled.reshape(-1, num_time_steps, num_features)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(units=50, activation='relu', input_shape=(num_time_steps, num_features)),\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train_lstm, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss = model.evaluate(X_test_lstm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6258aca-93c2-4104-afa8-af7485776628",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data_lstm = otpw_train_transform.select(\"features\").rdd.map(lambda x: np.array(x[0]))\n",
    "train_labels_lstm = otpw_train_transform.select(target_label).rdd.map(lambda x: x[0])\n",
    "\n",
    "test_data_lstm = otpw_test_transform.select(\"features\").rdd.map(lambda x: np.array(x[0]))\n",
    "test_labels_lstm = otpw_test_transform.select(target_label).rdd.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d754fb-0478-44c0-a98b-f4e1d77f7f58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.        ,   2.        , 211.        , -45.90000153,\n",
       "         2.39789527,   2.77258872])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data_lstm.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7ca651-3dcf-46e1-8705-b9dc3fa78a76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "num_features = 9\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, num_features)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train = np.array(train_data.collect())\n",
    "y_train = np.array(train_labels.collect())\n",
    "\n",
    "X_test = np.array(test_data.collect())\n",
    "y_test = np.array(test_labels.collect())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9139e1a8-2073-414b-aa9d-c64195ee89de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Model 8: 1D Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b136212a-300f-47aa-acc1-2a73a26e9809",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The 1D Convolutional Neural Network (1D CNN) is particularly suitable for predicting flight delays based on historical and environmental data because it excels at detecting local temporal patterns within sequences. This means that the 1D CNN can identify specific patterns, like short-term weather fluctuations or recurring congestion periods at airports, that lead up to flight delays. These networks use filters that traverse through the sequential input data, allowing them to recognize patterns regardless of their position in time, thereby providing a form of translational invariance. In essence, the 1D CNN's ability to capture and learn from these localized patterns within a large dataset can help yield accurate predictions for flight delays, harnessing information that might be missed by models not designed for sequential pattern detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1f8411-f78d-4e7f-a533-2d3489148bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 3000, 5)\n",
      "(31,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<command-3984215836275961>:50: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_array = np.array([x for x in X_test_array])\n"
     ]
    }
   ],
   "source": [
    "X_train_spark = spark.createDataFrame(X_train)\n",
    "X_test_spark = spark.createDataFrame(X_test)\n",
    "\n",
    "\n",
    "def sequence_data(data, feature_columns, N):\n",
    "    \n",
    "    # Add an index for sequencing and ordering\n",
    "    data = data.withColumn(\"index\", monotonically_increasing_id())\n",
    "    \n",
    "    # Truncate data to 3,000 entries per day\n",
    "    windowSpecTruncate = Window.partitionBy(\"DAY_OF_MONTH\").orderBy(\"index\")\n",
    "    data = data.withColumn(\"row_num\", row_number().over(windowSpecTruncate)).filter(col(\"row_num\") <= 3000).drop(\"row_num\")\n",
    "    \n",
    "    # Generate sequences for each feature\n",
    "    window_spec = Window.partitionBy(\"DAY_OF_MONTH\").orderBy(\"index\").rowsBetween(-N+1, 0)\n",
    "    for col_name in feature_columns:\n",
    "        data = data.withColumn(f\"{col_name}_seq\", collect_list(col(col_name)).over(window_spec))\n",
    "    \n",
    "    # Retaining only the sequenced columns\n",
    "    columns_to_retain = [f\"{col_name}_seq\" for col_name in feature_columns]\n",
    "    data = data.select(\"DAY_OF_MONTH\", *columns_to_retain).orderBy(\"DAY_OF_MONTH\", \"index\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "feature_col = ['HourlyVisibility', 'HourlyStationPressure', 'HourlyWindGustSpeed', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']\n",
    "target_label = 'TWO_HR_DELAY'\n",
    "\n",
    "# Define N\n",
    "N = 30  # Updated to 30 as per our discussion\n",
    "\n",
    "# Sequence data using the function\n",
    "X_train_sequenced = sequence_data(X_train_spark, feature_col, N)\n",
    "X_test_sequenced = sequence_data(X_test_spark, feature_col, N)\n",
    "\n",
    "# Convert back to pandas\n",
    "X_train_pandas = X_train_sequenced.toPandas()\n",
    "X_test_pandas = X_test_sequenced.toPandas()\n",
    "\n",
    "# Convert to numpy arrays and reshape\n",
    "X_train_array = X_train_pandas.groupby('DAY_OF_MONTH').apply(lambda group: group.drop(columns=['DAY_OF_MONTH']).to_numpy()).to_numpy()\n",
    "X_test_array = X_test_pandas.groupby('DAY_OF_MONTH').apply(lambda group: group.drop(columns=['DAY_OF_MONTH']).to_numpy()).to_numpy()\n",
    "\n",
    "X_train_array = np.array([x for x in X_train_array])\n",
    "X_test_array = np.array([x for x in X_test_array])\n",
    "\n",
    "print(X_train_array.shape)\n",
    "print(X_test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489ff08b-b505-4cd8-b6c5-c3735d8f38ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(N, len(feature_columns))))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2b35c1-a18e-4768-85af-eb61d192df1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93000, 6)\n",
      "(92719, 6)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pandas.shape)\n",
    "print(X_test_pandas.shape)\n",
    "\n",
    "print(X_train_pandas.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "598242a9-f5e8-4d04-9420-3be9729919ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "history = model.fit(X_train_array, y_train_adjusted, \n",
    "                    validation_data=(X_test_array, y_test_adjusted),\n",
    "                    epochs=10, batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test_array)\n",
    "y_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred] \n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_array, y_test_adjusted)\n",
    "train_accuracy = history.history['accuracy'][-1] \n",
    "test_recall = recall_score(y_test_adjusted, y_pred_binary)\n",
    "test_f1 = f1_score(y_test_adjusted, y_pred_binary)\n",
    "\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d696f3e-4301-4230-acfb-43a82aae08a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Loss Functions\n",
    "\n",
    "Listed below are the loss functions used in our modeling and experimentation.\n",
    "\n",
    "**Logistic Regression & Convolutional Neural Network (CNN):**\n",
    "$$\n",
    "\\text{Binary Cross Entropy} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right)\n",
    "$$\n",
    "\n",
    "**Decision Tree, Random Forest, XGBoost & Multilayer Perceptron (MLP):**\n",
    "$$\n",
    "\\text{Cross Entropy Loss} = -\\sum_{i=1}^{N} (y_i * \\log(p_i) + (1 - y_i) * \\log(1 - p_i))\n",
    "$$\n",
    "\n",
    "**Long Short-Term Memory (LSTM) Model:**\n",
    "$$\n",
    "\\text{Sparse Categorical Cross Entropy} = -\\sum_{i=1}^{N} y_i \\cdot \\log(p_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e8fc99-91d3-4e08-8975-87686542f510",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### End-to-End Modeling Pipeline Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ceb8dc-19cb-4598-8580-d7b389b14638",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Final Pipeline\n",
    "To summarize what was discussed above, our final modeling pipeline consists of the following main points after data sanitization/imputation and EDA:\n",
    "\n",
    "**1. Feature Engineering**\n",
    "| Technique | Summary |\n",
    "|------------|----------------|\n",
    "| Log Transformations | Transformations were applied to hourly weather features to adjust the skewness in the data, as many values in features such as HourlyPrecipitation were largely null. |\n",
    "| Dimensionality Reduction | Principal Component Analysis was used for dimensionality reduction on the Delay Details features. |\n",
    "| Scaling | Individual scaling (mean and standard deviation) was applied to each delay feature for standardization. |\n",
    "| Interaction Effects | Airport longitude and latitude were summed for their combined effect as a single feature. |\n",
    "| Min/Max Scaling | Min/max scaling was employed on each feature in the Hourly Weather Family to share the same scale. |\n",
    "| Graph-Based Features | For a graph-based feature, the number of flights was grouped and counted by origin airport. |\n",
    "| One-Hot Encoding | For features that are categorical and non-ordinal (i.e.; airport or city), one-hot encoding was used. |\n",
    "| Event-Based Features | An event-based feature was created to indicate whether a day was Christmas or New Year's. |\n",
    "\n",
    "**2. Data Splitting -** Traditional train/test splitting was performed and selected in favor over a cross validation approach. The splitting was time-based, with 2019 data becoming the test data.\n",
    "\n",
    "**3. Class Imbalance -** Both undersampling and oversampling techniques were explored. Undersampling was ultimately selected as the best strategy and a 3:1 ratio applied to non-delay/delay training data.\n",
    "\n",
    "**4. Experiments & Modeling -** The data was then pipelined into our model building and experiments phase. Our results are discussed in the 'Discussions' section below.\n",
    "\n",
    "* **Hyperparameter Tuning:** Hyperopt was the chosen library for hypertuning due to its parallelizable abilities and application of Tree-Structured Parzen Estimator (TPE).\n",
    "\n",
    "**5. Model Selection -** Based on the finalized model pipeline, the best-performing model was the **Multilayer Perceptron** (thirteen features, single hidden layer). More details are given in the below sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "635a1bd7-1b1d-4580-8d9d-2dc8a3557593",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Models (Phases II & III)\n",
    "\n",
    "For our Logistic Regression, Decision Tree, Random Forest and KNN models, we reference our experiments and results from phases II and III.\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "**1. Number of Input Features:**\n",
    "We used four input features for the logistic regression model, namely:\n",
    "- DAY_OF_MONTH\n",
    "- DISTANCE\n",
    "- LATITUDE\n",
    "- LONGITUDE\n",
    "\n",
    "**2. Number of Experiments Conducted:**\n",
    "During phase III, we conducted a total of one main experiment to build the logistic regression model for flight delay prediction. Additionally, we employed k-fold cross-validation to evaluate the model's performance and generalization.\n",
    "\n",
    "**3. Model Building Time:**\n",
    "The logistic regression model took approximately 1.5 minutes to train.\n",
    "\n",
    "**4. Metrics and Coefficients:**\n",
    "The coefficients (weights) learned by the logistic regression model for each input feature are as follows:\n",
    "\n",
    "- DAY_OF_MONTH: -0.0236\n",
    "- DISTANCE: 8.650e-05\n",
    "- LATITUDE: 0.0070\n",
    "- LONGITUDE: 0.0074\n",
    "\n",
    "**5. Discussion:**\n",
    "The baseline experiment involved using a logistic regression model with the four input features. During phase II, the primary focus was on understanding the importance of each input feature through their corresponding coefficients.\n",
    "\n",
    "From the coefficients, we observe that the DAY_OF_MONTH feature has a negative coefficient, suggesting a negative correlation with flight delays. In contrast, the DISTANCE, LATITUDE, and LONGITUDE features have positive coefficients, indicating a positive correlation with flight delays.\n",
    "\n",
    "To further improve the model's performance and overcome the challenge of predicting the majority class, phase III explored various families of models, such as random forest trees, and phas IV employed resampling techniques to address class imbalance.\n",
    "\n",
    "The results from phase III served as a solid foundation for refining the model and developing a more accurate and robust flight delay prediction system, enhancing the overall efficiency and reliability of airline operations for both airlines and passengers.\n",
    "\n",
    "\n",
    "#### Decision Tree\n",
    "\n",
    "**1. Number of Input Features:**\n",
    "We used eight input features for the decision tree model, namely:\n",
    "- DAY_OF_MONTH\n",
    "- MONTH\n",
    "- DISTANCE\n",
    "- sum_lat_lon\n",
    "- HourlyVisibility\n",
    "- HourlyPrecipitation\n",
    "- HourlyWindSpeed\n",
    "- ELEVATION\n",
    "\n",
    "**2. Number of Experiments Conducted:**\n",
    "During phase III, we conducted a total of three experiments to build the decision tree model for flight delay prediction. We also conducted hyperparameter tuning to pick the best number of bins and depth of the tree.\n",
    "\n",
    "**3. Model Building Time:**\n",
    "The decision tree model took approximate 1.5 minutes to train.\n",
    "\n",
    "**4. Metrics and Coefficients:**\n",
    "Below were the specifications of the Decision Tree:\n",
    "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_cc3b63234f74, depth=14, numNodes=5523, numClasses=2, numFeatures=9\n",
    "\n",
    "**5. Discussion:**\n",
    "In phase III, the model had a low accuracy but high precision and recall, and the model learned why flights were delayed.\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "**1. Number of Input Features:**\n",
    "We used four input features for the random forest model, namely:\n",
    "- DAY_OF_MONTH\n",
    "- MONTH\n",
    "- SUM_LAT_LON\n",
    "- HourlyVisibility\n",
    "- HourlyPrecipitation\n",
    "- ELEVATION\n",
    "\n",
    "**2. Number of Experiments Conducted:**\n",
    "During phase III, we conducted a total of three experiments to build the random forest model for flight delay prediction. Additionally, we employed k-fold cross-validation to evaluate the model's performance and generalization.\n",
    "\n",
    "**3. Model Building Time:**\n",
    "The random forest model took approximately 1 minute to train.\n",
    "\n",
    "**4. Metrics and Coefficients:**\n",
    "The feature importance learned by the random forest model for each input feature were as follows:\n",
    "\n",
    "- DAY_OF_MONTH: 0.5667\n",
    "- MONTH: 0.0634\n",
    "- SUM_LAT_LON: 0.0002\n",
    "- HourlyVisibility: 0.0127\n",
    "- HourlyPrecipitation: 0.2563\n",
    "- Elevation: 0.1008\n",
    "\n",
    "**5. Discussion:**\n",
    "The random forest model used six input features: DAY_OF_MONTH, MONTH, SUM_LAT_LON, HourlyVisibility, HourlyPrecipitation, Elevation. The model produced a test accuracy of 0.75, a precision of 0.72, and a recall of 0.75.\n",
    "\n",
    "From the feature importance, we observed that day of month and hourly precipitation were most relevant. We replaced sum of latitiude and longitude and hourly visibility with other features in phase IV.\n",
    "\n",
    "We had considered adding more features to see if there was improvement in the model's metrics, however in phase IV, we decided to explore other sophisticated algorithms. The random forest model remained a good candidate for the final model in predicting delayed flights as it performed about the same as the baseline logistic regression model.\n",
    "\n",
    "\n",
    "#### KNN\n",
    "\n",
    "**1. Number of Input Features:**\n",
    "We used twenty-five input features for the logistic regression model in phase III, namely:\n",
    "- MONTH\n",
    "- YEAR\n",
    "- MONTH\n",
    "- DAY_OF_MONTH\n",
    "- DAY_OF_WEEK\n",
    "- DISTANCE\n",
    "- dest_airport_lon\n",
    "- dest_airport_lat\n",
    "- LATITUDE\n",
    "- LONGITUDE\n",
    "- ELEVATION\n",
    "- HourlyPrecipitation\n",
    "- HourlyVisibility\n",
    "- HourlyPressureChange\n",
    "- HourlyPressureTendency\n",
    "- HourlyStationPressure\n",
    "- HourlyWindDirection\n",
    "- HourlyWindSpeed\n",
    "- ORIGIN_AIRPORT_ID\n",
    "- ORIGIN_CITY_MARKET_ID\n",
    "- ORIGIN_STATE_FIPS\n",
    "- DEST_AIRPORT_ID\n",
    "- DEP_TIME_BLK\n",
    "- STATION\n",
    "- sum_lat_lon\n",
    "\n",
    "**2. Number of Experiments Conducted:**\n",
    "During phase III, we conducted one experiment to build the KNN model for flight delay prediction. Additionally, we performed min-max scaling and employed k-fold cross-validation to evaluate the model's performance and generalization.\n",
    "\n",
    "**3. Model Building Time:**\n",
    "Using a subset of 50,000 samples, the KNN model took approximately 44.21 seconds to train in phase III.\n",
    "\n",
    "**4. Metrics and Coefficients:**\n",
    "In phase IV, the KNN was used for the identification of meaningful features. In the future, we propose to use the KNN as a sophisticated imputation technique. \n",
    "\n",
    "**5. Discussion:**\n",
    "k-Nearest Neighbors (KNN) is often unsuitable for large datasets due to its computational and memory intensiveness. For our purpose we expanded upon this option during phase IV by employing it for feature selection. This model assisted in identifying good clustering to help with selecting the following features:\n",
    "\n",
    "- DAY_OF_MONTH\n",
    "- HourlyVisibility\n",
    "- HourlyStationPressure\n",
    "- HourlyWindGustSpeed\n",
    "- ORIGIN_AIRPORT_ID\n",
    "- DEST_AIRPORT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fdf6a86-9c75-4d01-ad41-88d684639920",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Sophisticated Models (Phase IV)\n",
    "\n",
    "\n",
    "#### XGBoost\n",
    "**1. Number of Input Features:** We used six input features for the XGBoost model, namely:\n",
    "- DAY_OF_MONTH\n",
    "- MONTH\n",
    "- ORIGIN_AIRPORT_ID\n",
    "- sum_lat_lon\n",
    "- HourlyStationPressure\n",
    "- HourlyWindGustSpeed\n",
    "\n",
    "**2. Number of Experiments Conducted:** During phase IV, we conducted a total of one experiment to build the XGBoost model for flight delay prediction.\n",
    "\n",
    "**3. Model Building Time:** The XGBoost model took approximately 5.85 minutes to run.\n",
    "\n",
    "**4. Metrics and Coefficients:** The metrics for XGBoost were accuracy, weighted precision, weighted recall. The metrics are the following: accuracy: 0.92, weighted precision: 0.32, weighted recall: 0.0002. Further discussion can be found in the 'Results and Discussion of Results' section.\n",
    "\n",
    "**5. Discussion:** The XGBoost model was a further enhancement of the decision tree and random forest modeling performed in previous phases. The goal was to ultimately boost the results. Based on the metrics, the model did not perform as expected on predicting delayed flights. The XGBoost model had a high accuracy, but the precision and recall were significantly low. This indicated that out of the delayed flights, the XGBoost model did not predict delayed flights the majority of the time. \n",
    "\n",
    "\n",
    "#### Multilayer Perceptron (MLP)\n",
    "\n",
    "**1. Number of Input Features:** We used thirteen input features for the best-performing MLP model, namely:\n",
    "\n",
    "- DAY_OF_MONTH\n",
    "- MONTH\n",
    "- ORIGIN_AIRPORT_ID\n",
    "- sum_lat_lon\n",
    "- HourlyVisibility\n",
    "- HourlyStationPressure\n",
    "- HourlyWindGustSpeed\n",
    "- CARRIER_DELAY\n",
    "- WEATHER_DELAY \n",
    "- NAS_DELAY\n",
    "- SECURITY_DELAY\n",
    "- LATE_AIRCRAFT_DELAY\n",
    "- DEP_DELAY\n",
    "\n",
    "**2. Number of Experiments Conducted:** During phase IV, a total of three experiments were conducted to build the MLP model for flight delay prediction. The first two tested different features, while the third took the best-performing of the two models and applied tuned-hyperparameters.\n",
    "\n",
    "**3. Model Building Time:** The MLP model took approximately 9.66 minutes to train the best-performing version (Experiment 2).\n",
    "\n",
    "**4. Metrics and Coefficients:** The metrics for the MLP were accuracy, weighted precision, weighted recall, and F1 score. The model that performed the most well-rounded was selected with results of accuracy: 0.91, weighted precision: 0.93, weighted recall: 0.91, and f1-score: 0.92. Further discussion can be found in the 'Results and Discussion of Results' section.\n",
    "\n",
    "**5. Discussion:** Even with a single layer, the MLP was one of the most generalizable of the models experimented with. It gave consistently high performance across different features and experiments.\n",
    "\n",
    "\n",
    "#### Long Short-Term Memory Model (LSTM)\n",
    "\n",
    "**1. Number of Input Features:** We used thirteen input features for the LSTM model, namely:\n",
    "\n",
    "- DAY_OF_MONTH\n",
    "- MONTH\n",
    "- ORIGIN_AIRPORT_ID\n",
    "- sum_lat_lon\n",
    "- HourlyVisibility\n",
    "- HourlyStationPressure\n",
    "- HourlyWindGustSpeed\n",
    "- CARRIER_DELAY\n",
    "- WEATHER_DELAY \n",
    "- NAS_DELAY\n",
    "- SECURITY_DELAY\n",
    "- LATE_AIRCRAFT_DELAY\n",
    "- DEP_DELAY\n",
    "\n",
    "**2. Number of Experiments Conducted:** During phase IV, we attempted several experiments to build the LSTM model for flight delay prediction. Constraints such as time and computational resources prevented us from materializing a full implementation.\n",
    "\n",
    "**3. Model Building Time:** See above comments.\n",
    "\n",
    "**4. Metrics and Coefficients:** For metrics, we had planned to use the accuracy score for the LSTM model.\n",
    "\n",
    "**5. Discussion:** As a recurrent neural network, the LSTM model was one of the more challenging to implement. Although it had the advantage of being able to capture dependencies and patterns in sequential data, it also required careful tuning of hyperparameters and a sufficient amount of training data to perform effectively. The model's scalability on Spark was limited as it was built on Tensorflow and required Tensorflow-to-Databricks cluster configuration. This resulted in long training times and fewer possible experimentations we were interested in exploring.\n",
    "\n",
    "\n",
    "#### Convolutional Neural Network (CNN)\n",
    "\n",
    "**1. Number of Input Features:** We used five input features for the CNN model, namely:\n",
    "\n",
    "- HourlyVisibility\n",
    "- HourlyStationPressure\n",
    "- HourlyWindGustSpeed\n",
    "- ORIGIN_AIRPORT_ID\n",
    "- DEST_AIRPORT_ID\n",
    "\n",
    "**2. Number of Experiments Conducted:** During phase IV, we attempted several experiments to build the CNN model for flight delay prediction. Constraints such as time and computational resources prevented us from materializing a full implementation.\n",
    "\n",
    "**3. Model Building Time:** See above comments.\n",
    "\n",
    "**4. Metrics and Coefficients:** For metrics, we had planned to report the accuracy, recall, and F1 scores for the CNN.\n",
    "\n",
    "**5. Discussion:** The second of the two models that were more challenging to implement, the CNN used a series of sequential layers, including a single convolutional layer, to operate on the temporal flight delay data. The CNN would have learned to capture temporal patterns and relationships in the data, such as how certain weather conditions at certain times affect flight delays. The model was also built on Tensorflow and needed additional configuration with the cluster to train faster and more efficiently. Thus, long training times were also expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259b6f38-08de-45c5-a296-6f9ec8164ecb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Novel Approaches\n",
    "\n",
    "Throughout our modeling process, we employed a diverse set of techniques ranging from traditional machine learning models to more intricate deep learning architectures. Our approach aimed to identify the most suitable model that could best harness the intricacies of our dataset while ensuring reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9258f775-1dd9-4233-8f27-966b867dd2e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What Worked & What Didn't\n",
    "\n",
    "The Logistic Regression Model provided a solid baseline, achieving an accuracy of 0.75 on both training and testing datasets. The Decision Tree Classifier, surprisingly, underperformed with a test accuracy of 0.45. However, its high precision and recall indicate potential utility in certain scenarios. Random Forest mirrored our baseline's test accuracy and provided a modest boost in precision. The K-Nearest Neighbors, despite a higher training accuracy, did not outperform our baseline in the testing phase. XGBoost, LSTM, and 1D Convolutional Neural Network, unfortunately, were not operational in our environment. However, the standout was the Multilayer Perceptron (MLP), boasting the highest accuracy of 0.9076 along with impressive precision, recall, and F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f825303-a7d5-468f-bf4a-346e8443e737",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Model Experiment Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f037ab0-001f-4186-a610-59b59469a55b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Model Type</th><th>Input Features</th><th>Train Accuracy</th><th>Test Accuracy</th><th>Test Precision</th><th>Test Recall</th></tr></thead><tbody><tr><td>Baseline Logistic Regression</td><td>'DAY_OF_MONTH', 'DISTANCE', 'LATITUDE', 'LONGITUDE'</td><td>0.75</td><td>0.75</td><td>null</td><td>null</td></tr><tr><td>Decision Tree Classifier</td><td>'DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'HourlyWindSpeed', 'ELEVATION', 'DISTANCE_GROUP'</td><td>0.45</td><td>0.45</td><td>0.7</td><td>0.75</td></tr><tr><td>Random Forest Classifier</td><td>'DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'ELEVATION'</td><td>null</td><td>0.75</td><td>0.72</td><td>0.75</td></tr><tr><td>KNN</td><td>'MONTH', 'YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'dest_airport_lon', 'dest_airport_lat', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'HourlyPrecipitation', 'HourlyVisibility', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyStationPressure', 'HourlyWindDirection', 'HourlyWindSpeed', 'ORIGIN_AIRPORT_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_STATE_FIPS', 'DEST_AIRPORT_ID', 'DEP_TIME_BLK', 'STATION', 'sum_lat_lon'</td><td>0.8</td><td>0.73</td><td>0.42</td><td>0.21</td></tr><tr><td>XGBoost</td><td>'DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed'</td><td>null</td><td>0.92</td><td>0.32</td><td>2.0E-4</td></tr><tr><td>Multilayer Perceptron</td><td>'DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', 'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY', 'TWO_HR_DELAY'</td><td>null</td><td>0.91</td><td>0.93</td><td>0.91</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Baseline Logistic Regression",
         "'DAY_OF_MONTH', 'DISTANCE', 'LATITUDE', 'LONGITUDE'",
         0.75,
         0.75,
         null,
         null
        ],
        [
         "Decision Tree Classifier",
         "'DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'HourlyWindSpeed', 'ELEVATION', 'DISTANCE_GROUP'",
         0.45,
         0.45,
         0.7,
         0.75
        ],
        [
         "Random Forest Classifier",
         "'DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'ELEVATION'",
         null,
         0.75,
         0.72,
         0.75
        ],
        [
         "KNN",
         "'MONTH', 'YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'dest_airport_lon', 'dest_airport_lat', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'HourlyPrecipitation', 'HourlyVisibility', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyStationPressure', 'HourlyWindDirection', 'HourlyWindSpeed', 'ORIGIN_AIRPORT_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_STATE_FIPS', 'DEST_AIRPORT_ID', 'DEP_TIME_BLK', 'STATION', 'sum_lat_lon'",
         0.8,
         0.73,
         0.42,
         0.21
        ],
        [
         "XGBoost",
         "'DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed'",
         null,
         0.92,
         0.32,
         0.0002
        ],
        [
         "Multilayer Perceptron",
         "'DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', 'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY', 'TWO_HR_DELAY'",
         null,
         0.91,
         0.93,
         0.91
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Model Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Input Features",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Train Accuracy",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Test Accuracy",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Test Precision",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Test Recall",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results for LSMT and 1D CNN \n"
     ]
    }
   ],
   "source": [
    "model_performances = {\"Model Type\": [\"Baseline Logistic Regression\"], \"Input Features\": [\"'DAY_OF_MONTH', 'DISTANCE', 'LATITUDE', 'LONGITUDE'\"], \"Train Accuracy\": [0.75], \"Test Accuracy\": [0.75], \"Test Precision\": [None], \"Test Recall\": [None], }\n",
    "results_df = pd.DataFrame(model_performances)\n",
    "\n",
    "# Add the rest of the models here using the below line as an example\n",
    "results_df.loc[len(results_df)] = [\"Decision Tree Classifier\", \"'DAY_OF_MONTH', 'MONTH', 'DISTANCE', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'HourlyWindSpeed', 'ELEVATION', 'DISTANCE_GROUP'\", 0.45, 0.45, 0.70, 0.75]\n",
    "results_df.loc[len(results_df)] = [\"Random Forest Classifier\", \"'DAY_OF_MONTH', 'MONTH', 'sum_lat_lon', 'HourlyVisibility', 'HourlyPrecipitation', 'ELEVATION'\", None, 0.75, 0.72, 0.75]\n",
    "results_df.loc[len(results_df)] = [\"KNN\", \"'MONTH', 'YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'dest_airport_lon', 'dest_airport_lat', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'HourlyPrecipitation', 'HourlyVisibility', 'HourlyPressureChange', 'HourlyPressureTendency', 'HourlyStationPressure', 'HourlyWindDirection', 'HourlyWindSpeed', 'ORIGIN_AIRPORT_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_STATE_FIPS', 'DEST_AIRPORT_ID', 'DEP_TIME_BLK', 'STATION', 'sum_lat_lon'\", 0.80, 0.73, 0.42, 0.21]\n",
    "results_df.loc[len(results_df)] = [\"XGBoost\", \"'DAY_OF_MONTH', 'MONTH','ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyStationPressure','HourlyWindGustSpeed'\", None, 0.92, 0.32, 0.0002]\n",
    "results_df.loc[len(results_df)] = [\"Multilayer Perceptron\", \"'DAY_OF_MONTH', 'MONTH', 'ORIGIN_AIRPORT_ID', 'sum_lat_lon', 'HourlyVisibility', 'HourlyStationPressure', 'HourlyWindGustSpeed', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DEP_DELAY', 'TWO_HR_DELAY'\", None, 0.91, 0.93, 0.91]\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "print(\"No results for LSMT and 1D CNN \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6071c972-d69e-47e6-b3ff-ecb2b86289d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What We Learned\n",
    "\n",
    "Throughout the structured progression of our project, spanning four distinct phases, a multitude of insights were garnered:\n",
    "\n",
    "**Phase 1:** By implementing the baseline logistic regression model, we established a foundational understanding of our dataset's behavior and set a benchmark for subsequent models. This phase highlighted the importance of setting a comparative standard early on, anchoring our expectations and providing direction for future phases.\n",
    "\n",
    "**Phase 2:** As we delved into the Decision Tree, Random Forest, and KNN, it became evident that while some models, like the Decision Tree, might underperform in accuracy, they could offer valuable insights in other metrics such as precision and recall. The variances in model performance reiterated the idea that no single algorithm is universally optimal, and a nuanced approach to evaluation is essential.\n",
    "\n",
    "**Phase 3:** Our experimentation with advanced models, including MLP, XGBoost, LSTM, and 1D CNN, provided a deeper appreciation for the intricacies of model tuning and compatibility. Notably, while some models failed to execute efficiently, the MLP emerged as a stellar performer, reinforcing the notion that while some architectures might excel in certain datasets, they may not be suited for others.\n",
    "\n",
    "**Phase 4:** The final phase, which focused on fine-tuning feature selection for the MLP, was a testament to the significance of iterative refinement. Even with a high-performing model, the right features are paramount in unlocking its full potential. Through meticulous optimization, we were able to harness the maximum predictive power of the MLP.\n",
    "\n",
    "In retrospect, the phased approach not only allowed for methodical exploration and optimization but also provided us with a comprehensive understanding of the interplay between data, features, and models. It underscored the principle that effective machine learning isn't just about algorithms but the synergy between data preprocessing, model selection, and continuous evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3fde3f-7e3e-4024-95de-744c2358622d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Results and Discussion of Results\n",
    "\n",
    "**Gap Analysis of Our Team's Performance against Other Teams**\n",
    "\n",
    "Our flight delay prediction project has been an arduous journey of model exploration and refinement, culminating in the development of a high-performing Multi-Layer Perceptron (MLP) model. With a resounding test accuracy of 0.91, a precision of 0.93, and a recall of 0.91, our MLP model has proven its mettle as an exceptional tool for predicting flight delays accurately.\n",
    "\n",
    "In the context of the broader competition, we undertook a comprehensive gap analysis to contextualize our achievements among our peers based on the Project Leaderboard. This analysis highlights the following key insights:\n",
    "\n",
    "1. **Model Excellence**: Our MLP model's standout performance underscores its exceptional predictive capabilities. Achieving a remarkable test accuracy and maintaining high precision and recall rates signifies the model's robustness across various prediction scenarios.\n",
    "\n",
    "2. **Metric Consideration**: Our choice to focus on accuracy, precision, and recall as evaluation metrics allowed us to gain a holistic understanding of our model's performance. These metrics provide insights into the model's ability to make accurate predictions, identify true positive cases, and maintain precision under different conditions.\n",
    "\n",
    "3. **Diverse Approaches**: While our team prioritized the development and optimization of the MLP model, other teams explored a diverse array of machine learning algorithms, ranging from Random Forest and Logistic Regression to Gradient-Boosted Trees and beyond. This heterogeneity in approach reflects the multidimensionality of tackling the flight delay prediction challenge.\n",
    "\n",
    "4. **Feature Engineering Mastery**: A key contributor to our model's success was our rigorous feature engineering process. We strived to maintain the interpretability of selected features while maximizing their predictive power. This approach contrasts with other teams that opted for techniques like Principal Component Analysis (PCA) for dimensionality reduction.\n",
    "\n",
    "5. **Efficiency and Scalability**: The efficiency of our MLP model, both in training times and execution duration, positions it as a viable solution for real-world applications. This balance between accuracy and efficiency is crucial, particularly in domains with high data volumes and time-sensitive predictions.\n",
    "\n",
    "6. **Future Trajectory**: Our accomplishments provide a foundation for further refinement. Incorporating insights from the strategies employed by other teams, such as leveraging Gradient-Boosted Trees and Support Vector Machines, presents opportunities to bolster our model's robustness and predictive performance.\n",
    "\n",
    "7. **Holistic Impact**: Beyond the technical achievements, our successful model contributes to improved travel experiences by enhancing flight delay prediction accuracy. This not only benefits airlines but also empowers passengers with more informed decision-making during their journeys.\n",
    "\n",
    "In conclusion, our gap analysis underscores the collective commitment and expertise of our team in constructing a formidable flight delay prediction model. The supremacy of our MLP model, as demonstrated by its impressive accuracy, precision, and recall, positions us among the top contenders on the Project Leaderboard. Armed with these accomplishments and informed by the strategies of our peers, we are poised to elevate our model's capabilities even further, thus advancing the accuracy of flight delay prediction and elevating travel experiences for all stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c994cc43-cece-46f0-9088-8c38f9766ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Leakage\n",
    "\n",
    "Data leakage refers to a mistake made in the preprocessing of data where information from the test set unintentionally influences the training set. It often leads to overly optimistic and inaccurate model performance metrics. For instance, in the context of predicting flight delays, suppose we decide to use the average delay duration of an entire dataset, including the test set, to fill missing values in the training set. This would mean the training model is inadvertently informed by future data, resulting in leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd7fc26f-c632-478f-9388-ccef1a127dce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Potential Leakage in Our Pipeline\n",
    "\n",
    "Upon close inspection of our data processing pipeline - from high-level data cleansing, feature analysis and engineering, data splitting, addressing class imbalance, to hyperparameter tuning - there's no evident data leakage. Every step is carefully crafted to ensure separation between training and testing datasets. Especially notable is the time-based split used, ensuring that no future data (from 2019) informs the training models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b19df90d-df8e-4cde-941c-3ce0cc2e1678",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Potential Violations of ML Cardinal Rules\n",
    "\n",
    "We have been diligent to avoid common pitfalls in ML. For instance, our handling of time series data ensures temporal consistency, preventing any 'looking into the future'. Similarly, class imbalance is addressed by using undersampling, and hyperparameter tuning is automated using Hyperopt to avoid overfitting and to ensure generalization. Hence, we are not violating cardinal sins of ML in our approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f43b36-38e4-488e-ae5e-285f50d8014b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Why Our Pipeline is Sound\n",
    "\n",
    "Our pipeline is meticulously structured to prevent leakage. Features were engineered without any knowledge of the test set, and data splitting preserves the chronological order of the dataset. Additionally, undersampling was applied to tackle class imbalance without introducing biases. Hyperparameter tuning, managed by Hyperopt, guarantees a comprehensive yet efficient search without risking overfitting. With these precautions and strategies in place, our pipeline is safeguarded against both data leakage and cardinal sins of ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1277c3b-7d0d-4817-8326-46f582fde3b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Performance and Scalability\n",
    "\n",
    "In the dynamic realm of flight delay prediction, achieving optimal performance and scalability is paramount for effectively handling the vast and intricate datasets while ensuring accurate forecasts. PySpark, with its robust distributed computing capabilities, provides a solid foundation for tackling these challenges. However, the integration of external machine learning (ML) frameworks beyond Spark's native MLlib introduces certain complexities that require thoughtful consideration.\n",
    "\n",
    "PySpark's strengths lie in its inherent features:\n",
    "\n",
    "PySpark harnesses the power of distributed computing, making it adept at processing and analyzing extensive datasets, a critical aspect when dealing with historical flight data for delay prediction. Its parallel processing capabilities significantly expedite model training, a crucial advantage for adapting predictive models promptly as new data streams in. Furthermore, PySpark's built-in MLlib offers an array of machine learning algorithms optimized for Spark's distributed environment. This integration simplifies the model development process and ensures efficiency.\n",
    "\n",
    "Nonetheless, incorporating external ML frameworks can pose challenges:\n",
    "\n",
    "When introducing external ML frameworks into PySpark, complexities around compatibility and integration can arise. Data transformations may be necessary to align with the external framework's requirements, leading to additional overhead in terms of time and effort. Not all external frameworks seamlessly leverage Spark's distributed architecture, potentially limiting their efficiency with large-scale data. Ensuring consistent deployment between Spark's native MLlib and external frameworks can be intricate due to differences in training behavior and prediction outcomes. Additionally, the performance of external frameworks might vary based on factors like data distribution and cluster configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28613c0c-e136-4e5c-a958-d6ffd612602f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Limitations, Challenges, Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00502421-e803-4fcb-8ac7-348ae1e3b480",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Limitations\n",
    "\n",
    "**Data Scope:** Our dataset was primarily sourced from the US Department of Transportation and the National Oceanic and Atmospheric Administration, covering the period between 2015 and 2019. Data from 2020 onwards has been omitted, largely due to the unpredictable impacts of the COVID-19 pandemic on flight patterns.\n",
    "\n",
    "**Feature Engineering:** While we incorporated a broad array of feature engineering techniques, there's always the potential for other, unexplored features that could enhance our predictive power. For instance, we did not delve deeply into geospatial factors or airline-specific metrics. \n",
    "\n",
    "**Class Imbalance:** Although undersampling was applied, we maintained a 3:1 ratio of non-delayed to delayed flights. This means our models might still be biased towards predicting flights as non-delayed.\n",
    "\n",
    "**Computational Resources:** Our Databricks notebook environment was shared among four members, causing scheduling conflicts and resulting in time constraints. This limited the efficiency and the scope of tasks we could run simultaneously, hindering the timely progression of our analyses and experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "755d2a8e-bf67-4495-aefc-2bc4afd80fe7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Challenges\n",
    "\n",
    "**Complex Data Preprocessing:** The significant size and dimensionality of the datasets necessitated comprehensive preprocessing. Handling missing data, especially with respect to weather information, required sophisticated interpolation techniques. We also identified and and removed data that lacked utility.\n",
    "\n",
    "**Time-based Data Splitting:** Given the time-series nature of the data, ensuring that our train/test splits and cross-validation did not introduce any temporal leakage was a challenging endeavor.\n",
    "Hyperparameter Tuning: While Hyperopt streamlined the hyperparameter tuning process, determining the ideal search space and managing computational costs remained demanding tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea9bcfc0-1297-4d72-b03c-7476451cb042",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Future Work\n",
    "\n",
    "**Complete Current Pipeline:** Unfortunately we were not able to problem solve and get the LSTM or 1D CNN models working. Moving forward it would be interesting to see the result of these two models\n",
    "\n",
    "**Incorporation of Post-2019 Data:** Even though 2020-2022 are considered anomalous due to the pandemic, incorporating this data could provide insights into how major global events impact flight delays.\n",
    "\n",
    "**Expanded Feature Engineering:** Investigate other potential features, like geospatial data or intricate airport logistical data, to refine our predictive power.\n",
    "\n",
    "**Alternate Class Imbalance Strategies:** Beyond undersampling, techniques such as oversampling or using synthetic data with techniques like SMOTE could be explored to further mitigate the impact of class imbalance on model performance.\n",
    "\n",
    "**Enhanced Model Architectures:** Dive deeper into the realm of deep learning, possibly exploring Transformer-based architectures or more sophisticated recurrent neural network setups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb6ef2d-65d1-4f11-93a4-3ad87a471325",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Conclusion\n",
    "Based on the data provided, we have conducted exploratory visualizations to gain insights into the data values and structure. The data was thoroughly cleansed, removing inconsistencies, errors, and missing values to ensure a high-quality and reliable dataset. This clean dataset forms the foundation for building accurate predictive models.\n",
    "\n",
    "By performing feature engineering utilizing log transformations, scaling, and sum of columns, we are able to create better features for predicting delayed flights.\n",
    "\n",
    "For initial insights, we opted for a simpler logistic regression model. This model serves as a baseline to understand the data and analyze the performance of a low-effort approach. Evaluation metrics like accuracy, AUC, and ROC curve help assess the model's effectiveness in predicting flight delays.\n",
    "\n",
    "For our more advanced models, we opted for the decision tree, random forest, KNN model, XGBoost and MLP. Decision trees, random forests, XGBoost, and MLP work well on large scale datasets so it made sense to compare the models with the baseline logistic regression model. The KNN model gives us more understanding on which features are relevant in predicting weather delay.\n",
    "\n",
    "Hyperparameter tuning was a crucial part of the optimization process. Systematic experimentation with various hyperparameter combinations, such as learning rates, regularization parameters, and model complexity helped improve the model's performance. This iterative tuning process involved training and evaluating the model's performance.\n",
    "\n",
    "If we had more time, we would like to experiment more with the MLP model to achieve a higher accuracy."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "261-FP-Phase-4-Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
